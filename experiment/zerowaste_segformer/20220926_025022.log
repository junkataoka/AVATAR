2022-09-26 02:50:22,308 - mmseg - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.5 (default, Sep  4 2020, 07:30:14) [GCC 7.3.0]
CUDA available: True
GPU 0,1: Tesla P100-PCIE-12GB
CUDA_HOME: /cm/shared/apps/cuda11.1/toolkit/11.1.1
NVCC: Build cuda_11.1.TC455_06.29190527_0
GCC: gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)
PyTorch: 1.9.1
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.10.1
OpenCV: 4.6.0
MMCV: 1.4.0
MMCV Compiler: n/a
MMCV CUDA Compiler: n/a
MMSegmentation: 0.16.0+unknown
------------------------------------------------------------

2022-09-26 02:50:22,309 - mmseg - INFO - Distributed training: False
2022-09-26 02:50:22,618 - mmseg - INFO - Config:
log_config = dict(
    interval=50, hooks=[dict(type='TextLoggerHook', by_epoch=False)])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
cudnn_benchmark = True
norm_cfg = dict(type='BN', requires_grad=True)
find_unused_parameters = True
model = dict(
    type='EncoderDecoder',
    pretrained='pretrained/mit_b5.pth',
    backbone=dict(type='mit_b5', style='pytorch'),
    decode_head=dict(
        type='SegFormerHead',
        in_channels=[64, 128, 320, 512],
        in_index=[0, 1, 2, 3],
        channels=128,
        dropout_ratio=0.1,
        num_classes=19,
        norm_cfg=dict(type='BN', requires_grad=True),
        align_corners=False,
        decoder_params=dict(embed_dim=768, conv_kernel_size=1),
        loss_decode=dict(
            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0)),
    train_cfg=dict(
        work_dir='/data/home/jkataok1/AVATAR2022/experiment/zerowaste_segformer'
    ),
    test_cfg=dict(mode='whole'))
dataset_type = 'ZeroWasteDataset'
data_root = 'data/zerowaste-v2-splits/'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
crop_size = (512, 512)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(type='Resize', img_scale=(1280, 720)),
    dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.75),
    dict(type='RandomFlip', prob=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_semantic_seg'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1024, 512),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=4,
    train=dict(
        type='ZeroWasteDataset',
        data_root='data/zerowaste-f/train',
        img_dir='data',
        ann_dir='sem_seg',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations'),
            dict(type='Resize', img_scale=(1280, 720)),
            dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.75),
            dict(type='RandomFlip', prob=0.5),
            dict(type='PhotoMetricDistortion'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_semantic_seg'])
        ]),
    val=dict(
        type='ZeroWasteDataset',
        data_root='data/zerowaste-f/test',
        img_dir='data',
        ann_dir='sem_seg',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1024, 512),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='ZeroWasteV2Dataset',
        data_root='data/zerowaste-v2-splits/test',
        img_dir='data',
        ann_dir='sem_seg',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1024, 512),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
optimizer = dict(
    type='AdamW',
    lr=6e-05,
    betas=(0.9, 0.999),
    weight_decay=0.01,
    paramwise_cfg=dict(
        custom_keys=dict(
            head=dict(lr_mult=10.0),
            pos_block=dict(decay_mult=0.0),
            norm=dict(decay_mult=0.0))))
optimizer_config = dict()
lr_config = dict(
    policy='poly',
    warmup='linear',
    warmup_iters=1500,
    warmup_ratio=1e-06,
    power=1.0,
    min_lr=0.0,
    by_epoch=False)
n_gpus = 1
seed = 0
runner = dict(type='IterBasedRunner', max_iters=40000)
checkpoint_config = dict(by_epoch=False, interval=8000, max_keep_ckpts=1)
evaluation = dict(interval=5000, metric='mIoU')
name = 'zerowaste2zerowastev2_source-only_segformer_mitb5'
exp = 1
name_dataset = 'zerowaste2zerowastev2'
name_architecture = 'segformer_mitb5'
name_encoder = 'mitb5'
name_decoder = 'segformer'
name_uda = 'source-only'
name_opt = 'adamw_6e-05_pmTrue_poly10warm_1x2_40k'
work_dir = '/data/home/jkataok1/AVATAR2022/experiment/zerowaste_segformer'
git_rev = '8d6e710700ff5e6a053c77bfe384ba44d4672cbe'
gpu_ids = range(0, 1)

2022-09-26 02:50:22,618 - mmseg - INFO - Set random seed to 0, deterministic: False
2022-09-26 02:50:23,519 - mmseg - INFO - Load mit checkpoint.
2022-09-26 02:50:23,519 - mmseg - INFO - load checkpoint from local path: pretrained/mit_b5.pth
2022-09-26 02:50:24,234 - mmseg - INFO - initialize SegFormerHead with init_cfg {'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
Name of parameter - Initialization information

backbone.patch_embed1.proj.weight - torch.Size([64, 3, 7, 7]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.patch_embed1.proj.bias - torch.Size([64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.patch_embed1.norm.weight - torch.Size([64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.patch_embed1.norm.bias - torch.Size([64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.patch_embed2.proj.weight - torch.Size([128, 64, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.patch_embed2.proj.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.patch_embed2.norm.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.patch_embed2.norm.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.patch_embed3.proj.weight - torch.Size([320, 128, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.patch_embed3.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.patch_embed3.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.patch_embed3.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.patch_embed4.proj.weight - torch.Size([512, 320, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.patch_embed4.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.patch_embed4.norm.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.patch_embed4.norm.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.0.norm1.weight - torch.Size([64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.0.norm1.bias - torch.Size([64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.0.attn.q.weight - torch.Size([64, 64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.0.attn.q.bias - torch.Size([64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.0.attn.kv.weight - torch.Size([128, 64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.0.attn.kv.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.0.attn.proj.weight - torch.Size([64, 64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.0.attn.proj.bias - torch.Size([64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.0.attn.sr.weight - torch.Size([64, 64, 8, 8]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.0.attn.sr.bias - torch.Size([64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.0.attn.norm.weight - torch.Size([64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.0.attn.norm.bias - torch.Size([64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.0.norm2.weight - torch.Size([64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.0.norm2.bias - torch.Size([64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.0.mlp.fc1.weight - torch.Size([256, 64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.0.mlp.fc1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.0.mlp.dwconv.dwconv.weight - torch.Size([256, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.0.mlp.dwconv.dwconv.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.0.mlp.fc2.weight - torch.Size([64, 256]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.0.mlp.fc2.bias - torch.Size([64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.1.norm1.weight - torch.Size([64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.1.norm1.bias - torch.Size([64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.1.attn.q.weight - torch.Size([64, 64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.1.attn.q.bias - torch.Size([64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.1.attn.kv.weight - torch.Size([128, 64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.1.attn.kv.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.1.attn.proj.weight - torch.Size([64, 64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.1.attn.proj.bias - torch.Size([64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.1.attn.sr.weight - torch.Size([64, 64, 8, 8]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.1.attn.sr.bias - torch.Size([64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.1.attn.norm.weight - torch.Size([64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.1.attn.norm.bias - torch.Size([64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.1.norm2.weight - torch.Size([64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.1.norm2.bias - torch.Size([64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.1.mlp.fc1.weight - torch.Size([256, 64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.1.mlp.fc1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.1.mlp.dwconv.dwconv.weight - torch.Size([256, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.1.mlp.dwconv.dwconv.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.1.mlp.fc2.weight - torch.Size([64, 256]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.1.mlp.fc2.bias - torch.Size([64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.2.norm1.weight - torch.Size([64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.2.norm1.bias - torch.Size([64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.2.attn.q.weight - torch.Size([64, 64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.2.attn.q.bias - torch.Size([64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.2.attn.kv.weight - torch.Size([128, 64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.2.attn.kv.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.2.attn.proj.weight - torch.Size([64, 64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.2.attn.proj.bias - torch.Size([64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.2.attn.sr.weight - torch.Size([64, 64, 8, 8]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.2.attn.sr.bias - torch.Size([64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.2.attn.norm.weight - torch.Size([64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.2.attn.norm.bias - torch.Size([64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.2.norm2.weight - torch.Size([64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.2.norm2.bias - torch.Size([64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.2.mlp.fc1.weight - torch.Size([256, 64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.2.mlp.fc1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.2.mlp.dwconv.dwconv.weight - torch.Size([256, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.2.mlp.dwconv.dwconv.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.2.mlp.fc2.weight - torch.Size([64, 256]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block1.2.mlp.fc2.bias - torch.Size([64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.norm1.weight - torch.Size([64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.norm1.bias - torch.Size([64]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.0.norm1.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.0.norm1.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.0.attn.q.weight - torch.Size([128, 128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.0.attn.q.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.0.attn.kv.weight - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.0.attn.kv.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.0.attn.proj.weight - torch.Size([128, 128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.0.attn.proj.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.0.attn.sr.weight - torch.Size([128, 128, 4, 4]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.0.attn.sr.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.0.attn.norm.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.0.attn.norm.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.0.norm2.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.0.norm2.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.0.mlp.fc1.weight - torch.Size([512, 128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.0.mlp.fc1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.0.mlp.dwconv.dwconv.weight - torch.Size([512, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.0.mlp.dwconv.dwconv.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.0.mlp.fc2.weight - torch.Size([128, 512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.0.mlp.fc2.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.1.norm1.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.1.norm1.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.1.attn.q.weight - torch.Size([128, 128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.1.attn.q.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.1.attn.kv.weight - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.1.attn.kv.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.1.attn.proj.weight - torch.Size([128, 128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.1.attn.proj.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.1.attn.sr.weight - torch.Size([128, 128, 4, 4]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.1.attn.sr.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.1.attn.norm.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.1.attn.norm.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.1.norm2.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.1.norm2.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.1.mlp.fc1.weight - torch.Size([512, 128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.1.mlp.fc1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.1.mlp.dwconv.dwconv.weight - torch.Size([512, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.1.mlp.dwconv.dwconv.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.1.mlp.fc2.weight - torch.Size([128, 512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.1.mlp.fc2.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.2.norm1.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.2.norm1.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.2.attn.q.weight - torch.Size([128, 128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.2.attn.q.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.2.attn.kv.weight - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.2.attn.kv.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.2.attn.proj.weight - torch.Size([128, 128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.2.attn.proj.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.2.attn.sr.weight - torch.Size([128, 128, 4, 4]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.2.attn.sr.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.2.attn.norm.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.2.attn.norm.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.2.norm2.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.2.norm2.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.2.mlp.fc1.weight - torch.Size([512, 128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.2.mlp.fc1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.2.mlp.dwconv.dwconv.weight - torch.Size([512, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.2.mlp.dwconv.dwconv.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.2.mlp.fc2.weight - torch.Size([128, 512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.2.mlp.fc2.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.3.norm1.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.3.norm1.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.3.attn.q.weight - torch.Size([128, 128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.3.attn.q.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.3.attn.kv.weight - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.3.attn.kv.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.3.attn.proj.weight - torch.Size([128, 128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.3.attn.proj.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.3.attn.sr.weight - torch.Size([128, 128, 4, 4]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.3.attn.sr.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.3.attn.norm.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.3.attn.norm.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.3.norm2.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.3.norm2.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.3.mlp.fc1.weight - torch.Size([512, 128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.3.mlp.fc1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.3.mlp.dwconv.dwconv.weight - torch.Size([512, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.3.mlp.dwconv.dwconv.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.3.mlp.fc2.weight - torch.Size([128, 512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.3.mlp.fc2.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.4.norm1.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.4.norm1.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.4.attn.q.weight - torch.Size([128, 128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.4.attn.q.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.4.attn.kv.weight - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.4.attn.kv.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.4.attn.proj.weight - torch.Size([128, 128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.4.attn.proj.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.4.attn.sr.weight - torch.Size([128, 128, 4, 4]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.4.attn.sr.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.4.attn.norm.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.4.attn.norm.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.4.norm2.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.4.norm2.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.4.mlp.fc1.weight - torch.Size([512, 128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.4.mlp.fc1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.4.mlp.dwconv.dwconv.weight - torch.Size([512, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.4.mlp.dwconv.dwconv.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.4.mlp.fc2.weight - torch.Size([128, 512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.4.mlp.fc2.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.5.norm1.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.5.norm1.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.5.attn.q.weight - torch.Size([128, 128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.5.attn.q.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.5.attn.kv.weight - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.5.attn.kv.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.5.attn.proj.weight - torch.Size([128, 128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.5.attn.proj.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.5.attn.sr.weight - torch.Size([128, 128, 4, 4]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.5.attn.sr.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.5.attn.norm.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.5.attn.norm.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.5.norm2.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.5.norm2.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.5.mlp.fc1.weight - torch.Size([512, 128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.5.mlp.fc1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.5.mlp.dwconv.dwconv.weight - torch.Size([512, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.5.mlp.dwconv.dwconv.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.5.mlp.fc2.weight - torch.Size([128, 512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block2.5.mlp.fc2.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.norm2.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.norm2.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.0.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.0.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.0.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.0.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.0.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.0.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.0.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.0.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.0.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.0.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.0.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.0.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.0.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.0.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.0.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.0.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.0.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.0.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.0.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.0.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.1.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.1.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.1.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.1.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.1.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.1.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.1.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.1.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.1.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.1.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.1.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.1.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.1.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.1.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.1.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.1.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.1.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.1.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.1.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.1.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.2.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.2.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.2.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.2.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.2.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.2.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.2.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.2.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.2.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.2.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.2.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.2.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.2.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.2.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.2.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.2.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.2.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.2.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.2.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.2.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.3.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.3.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.3.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.3.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.3.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.3.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.3.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.3.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.3.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.3.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.3.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.3.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.3.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.3.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.3.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.3.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.3.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.3.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.3.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.3.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.4.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.4.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.4.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.4.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.4.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.4.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.4.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.4.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.4.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.4.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.4.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.4.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.4.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.4.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.4.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.4.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.4.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.4.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.4.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.4.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.5.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.5.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.5.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.5.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.5.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.5.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.5.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.5.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.5.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.5.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.5.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.5.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.5.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.5.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.5.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.5.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.5.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.5.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.5.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.5.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.6.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.6.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.6.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.6.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.6.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.6.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.6.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.6.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.6.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.6.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.6.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.6.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.6.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.6.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.6.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.6.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.6.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.6.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.6.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.6.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.7.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.7.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.7.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.7.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.7.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.7.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.7.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.7.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.7.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.7.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.7.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.7.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.7.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.7.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.7.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.7.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.7.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.7.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.7.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.7.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.8.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.8.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.8.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.8.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.8.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.8.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.8.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.8.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.8.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.8.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.8.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.8.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.8.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.8.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.8.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.8.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.8.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.8.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.8.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.8.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.9.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.9.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.9.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.9.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.9.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.9.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.9.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.9.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.9.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.9.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.9.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.9.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.9.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.9.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.9.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.9.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.9.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.9.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.9.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.9.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.10.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.10.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.10.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.10.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.10.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.10.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.10.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.10.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.10.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.10.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.10.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.10.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.10.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.10.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.10.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.10.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.10.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.10.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.10.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.10.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.11.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.11.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.11.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.11.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.11.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.11.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.11.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.11.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.11.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.11.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.11.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.11.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.11.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.11.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.11.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.11.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.11.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.11.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.11.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.11.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.12.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.12.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.12.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.12.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.12.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.12.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.12.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.12.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.12.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.12.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.12.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.12.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.12.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.12.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.12.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.12.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.12.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.12.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.12.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.12.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.13.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.13.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.13.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.13.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.13.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.13.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.13.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.13.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.13.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.13.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.13.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.13.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.13.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.13.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.13.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.13.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.13.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.13.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.13.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.13.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.14.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.14.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.14.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.14.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.14.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.14.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.14.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.14.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.14.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.14.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.14.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.14.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.14.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.14.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.14.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.14.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.14.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.14.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.14.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.14.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.15.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.15.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.15.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.15.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.15.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.15.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.15.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.15.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.15.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.15.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.15.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.15.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.15.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.15.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.15.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.15.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.15.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.15.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.15.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.15.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.16.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.16.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.16.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.16.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.16.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.16.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.16.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.16.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.16.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.16.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.16.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.16.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.16.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.16.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.16.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.16.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.16.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.16.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.16.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.16.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.17.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.17.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.17.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.17.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.17.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.17.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.17.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.17.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.17.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.17.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.17.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.17.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.17.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.17.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.17.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.17.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.17.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.17.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.17.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.17.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.18.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.18.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.18.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.18.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.18.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.18.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.18.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.18.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.18.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.18.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.18.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.18.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.18.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.18.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.18.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.18.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.18.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.18.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.18.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.18.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.19.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.19.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.19.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.19.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.19.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.19.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.19.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.19.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.19.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.19.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.19.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.19.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.19.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.19.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.19.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.19.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.19.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.19.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.19.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.19.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.20.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.20.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.20.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.20.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.20.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.20.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.20.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.20.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.20.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.20.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.20.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.20.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.20.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.20.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.20.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.20.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.20.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.20.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.20.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.20.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.21.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.21.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.21.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.21.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.21.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.21.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.21.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.21.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.21.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.21.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.21.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.21.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.21.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.21.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.21.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.21.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.21.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.21.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.21.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.21.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.22.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.22.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.22.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.22.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.22.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.22.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.22.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.22.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.22.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.22.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.22.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.22.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.22.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.22.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.22.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.22.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.22.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.22.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.22.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.22.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.23.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.23.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.23.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.23.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.23.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.23.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.23.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.23.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.23.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.23.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.23.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.23.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.23.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.23.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.23.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.23.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.23.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.23.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.23.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.23.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.24.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.24.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.24.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.24.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.24.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.24.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.24.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.24.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.24.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.24.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.24.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.24.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.24.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.24.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.24.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.24.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.24.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.24.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.24.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.24.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.25.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.25.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.25.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.25.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.25.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.25.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.25.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.25.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.25.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.25.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.25.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.25.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.25.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.25.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.25.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.25.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.25.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.25.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.25.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.25.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.26.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.26.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.26.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.26.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.26.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.26.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.26.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.26.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.26.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.26.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.26.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.26.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.26.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.26.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.26.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.26.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.26.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.26.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.26.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.26.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.27.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.27.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.27.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.27.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.27.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.27.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.27.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.27.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.27.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.27.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.27.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.27.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.27.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.27.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.27.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.27.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.27.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.27.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.27.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.27.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.28.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.28.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.28.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.28.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.28.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.28.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.28.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.28.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.28.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.28.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.28.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.28.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.28.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.28.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.28.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.28.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.28.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.28.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.28.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.28.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.29.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.29.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.29.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.29.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.29.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.29.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.29.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.29.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.29.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.29.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.29.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.29.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.29.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.29.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.29.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.29.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.29.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.29.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.29.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.29.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.30.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.30.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.30.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.30.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.30.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.30.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.30.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.30.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.30.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.30.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.30.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.30.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.30.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.30.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.30.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.30.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.30.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.30.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.30.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.30.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.31.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.31.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.31.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.31.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.31.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.31.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.31.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.31.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.31.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.31.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.31.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.31.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.31.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.31.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.31.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.31.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.31.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.31.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.31.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.31.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.32.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.32.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.32.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.32.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.32.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.32.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.32.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.32.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.32.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.32.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.32.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.32.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.32.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.32.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.32.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.32.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.32.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.32.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.32.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.32.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.33.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.33.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.33.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.33.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.33.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.33.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.33.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.33.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.33.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.33.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.33.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.33.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.33.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.33.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.33.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.33.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.33.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.33.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.33.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.33.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.34.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.34.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.34.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.34.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.34.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.34.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.34.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.34.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.34.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.34.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.34.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.34.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.34.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.34.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.34.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.34.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.34.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.34.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.34.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.34.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.35.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.35.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.35.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.35.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.35.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.35.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.35.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.35.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.35.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.35.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.35.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.35.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.35.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.35.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.35.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.35.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.35.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.35.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.35.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.35.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.36.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.36.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.36.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.36.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.36.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.36.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.36.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.36.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.36.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.36.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.36.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.36.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.36.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.36.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.36.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.36.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.36.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.36.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.36.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.36.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.37.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.37.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.37.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.37.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.37.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.37.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.37.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.37.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.37.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.37.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.37.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.37.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.37.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.37.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.37.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.37.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.37.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.37.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.37.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.37.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.38.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.38.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.38.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.38.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.38.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.38.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.38.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.38.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.38.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.38.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.38.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.38.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.38.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.38.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.38.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.38.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.38.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.38.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.38.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.38.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.39.norm1.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.39.norm1.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.39.attn.q.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.39.attn.q.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.39.attn.kv.weight - torch.Size([640, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.39.attn.kv.bias - torch.Size([640]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.39.attn.proj.weight - torch.Size([320, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.39.attn.proj.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.39.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.39.attn.sr.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.39.attn.norm.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.39.attn.norm.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.39.norm2.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.39.norm2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.39.mlp.fc1.weight - torch.Size([1280, 320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.39.mlp.fc1.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.39.mlp.dwconv.dwconv.weight - torch.Size([1280, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.39.mlp.dwconv.dwconv.bias - torch.Size([1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.39.mlp.fc2.weight - torch.Size([320, 1280]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block3.39.mlp.fc2.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.norm3.weight - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.norm3.bias - torch.Size([320]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.0.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.0.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.0.attn.q.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.0.attn.q.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.0.attn.kv.weight - torch.Size([1024, 512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.0.attn.kv.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.0.attn.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.0.attn.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.0.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.0.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.0.mlp.fc1.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.0.mlp.fc1.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.0.mlp.dwconv.dwconv.weight - torch.Size([2048, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.0.mlp.dwconv.dwconv.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.0.mlp.fc2.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.0.mlp.fc2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.1.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.1.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.1.attn.q.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.1.attn.q.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.1.attn.kv.weight - torch.Size([1024, 512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.1.attn.kv.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.1.attn.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.1.attn.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.1.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.1.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.1.mlp.fc1.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.1.mlp.fc1.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.1.mlp.dwconv.dwconv.weight - torch.Size([2048, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.1.mlp.dwconv.dwconv.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.1.mlp.fc2.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.1.mlp.fc2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.2.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.2.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.2.attn.q.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.2.attn.q.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.2.attn.kv.weight - torch.Size([1024, 512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.2.attn.kv.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.2.attn.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.2.attn.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.2.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.2.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.2.mlp.fc1.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.2.mlp.fc1.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.2.mlp.dwconv.dwconv.weight - torch.Size([2048, 1, 3, 3]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.2.mlp.dwconv.dwconv.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.2.mlp.fc2.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.block4.2.mlp.fc2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.norm4.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

backbone.norm4.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in mit_b5  

decode_head.conv_seg.weight - torch.Size([19, 128, 1, 1]): 
NormalInit: mean=0, std=0.01, bias=0 

decode_head.conv_seg.bias - torch.Size([19]): 
NormalInit: mean=0, std=0.01, bias=0 

decode_head.linear_c.0.proj.weight - torch.Size([768, 64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.linear_c.0.proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.linear_c.1.proj.weight - torch.Size([768, 128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.linear_c.1.proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.linear_c.2.proj.weight - torch.Size([768, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.linear_c.2.proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.linear_c.3.proj.weight - torch.Size([768, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.linear_c.3.proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.linear_fuse.conv.weight - torch.Size([768, 3072, 1, 1]): 
Initialized by user-defined `init_weights` in ConvModule  

decode_head.linear_fuse.bn.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.linear_fuse.bn.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.linear_pred.weight - torch.Size([19, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.linear_pred.bias - torch.Size([19]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  
2022-09-26 02:50:24,260 - mmseg - INFO - EncoderDecoder(
  (backbone): mit_b5(
    (patch_embed1): OverlapPatchEmbed(
      (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed2): OverlapPatchEmbed(
      (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed3): OverlapPatchEmbed(
      (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed4): OverlapPatchEmbed(
      (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (block1): ModuleList(
      (0): Block(
        (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (act): GELU()
          (fc2): Linear(in_features=256, out_features=64, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (act): GELU()
          (fc2): Linear(in_features=256, out_features=64, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (act): GELU()
          (fc2): Linear(in_features=256, out_features=64, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
    (block2): ModuleList(
      (0): Block(
        (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (act): GELU()
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (act): GELU()
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (act): GELU()
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (act): GELU()
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (act): GELU()
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (act): GELU()
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
    (block3): ModuleList(
      (0): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (12): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (13): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (14): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (15): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (16): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (17): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (18): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (19): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (20): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (21): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (22): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (23): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (24): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (25): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (26): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (27): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (28): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (29): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (30): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (31): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (32): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (33): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (34): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (35): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (36): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (37): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (38): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (39): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm3): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
    (block4): ModuleList(
      (0): Block(
        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (act): GELU()
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (act): GELU()
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (act): GELU()
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm4): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
  )
  (decode_head): SegFormerHead(
    input_transform=multiple_select, ignore_index=255, align_corners=False
    (loss_decode): CrossEntropyLoss()
    (conv_seg): Conv2d(128, 19, kernel_size=(1, 1), stride=(1, 1))
    (dropout): Dropout2d(p=0.1, inplace=False)
    (linear_c): ModuleDict(
      (0): MLP(
        (proj): Linear(in_features=64, out_features=768, bias=True)
      )
      (1): MLP(
        (proj): Linear(in_features=128, out_features=768, bias=True)
      )
      (2): MLP(
        (proj): Linear(in_features=320, out_features=768, bias=True)
      )
      (3): MLP(
        (proj): Linear(in_features=512, out_features=768, bias=True)
      )
    )
    (linear_fuse): ConvModule(
      (conv): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activate): ReLU(inplace=True)
    )
    (linear_pred): Conv2d(768, 19, kernel_size=(1, 1), stride=(1, 1))
  )
  init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
)
2022-09-26 02:50:24,426 - mmseg - INFO - Loaded 3002 images from data/zerowaste-f/train/data
2022-09-26 02:50:26,982 - mmseg - INFO - Loaded 929 images from data/zerowaste-f/test/data
2022-09-26 02:50:26,983 - mmseg - INFO - Start running, host: jkataok1@compute130, work_dir: /data/home/jkataok1/AVATAR2022/experiment/zerowaste_segformer
2022-09-26 02:50:26,983 - mmseg - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2022-09-26 02:50:26,983 - mmseg - INFO - workflow: [('train', 1)], max: 40000 iters
2022-09-26 02:50:26,984 - mmseg - INFO - Checkpoints will be saved to /data/home/jkataok1/AVATAR2022/experiment/zerowaste_segformer by HardDiskBackend.
2022-09-26 02:51:06,642 - mmseg - INFO - Iter [50/40000]	lr: 1.958e-06, eta: 8:37:30, time: 0.777, data_time: 0.026, memory: 7343, decode.loss_seg: 2.9256, decode.acc_seg: 7.6369, loss: 2.9256
2022-09-26 02:51:42,310 - mmseg - INFO - Iter [100/40000]	lr: 3.950e-06, eta: 8:15:47, time: 0.714, data_time: 0.007, memory: 7343, decode.loss_seg: 2.3107, decode.acc_seg: 48.3150, loss: 2.3107
2022-09-26 02:52:17,919 - mmseg - INFO - Iter [150/40000]	lr: 5.938e-06, eta: 8:07:46, time: 0.712, data_time: 0.007, memory: 7343, decode.loss_seg: 1.5335, decode.acc_seg: 73.2960, loss: 1.5335
2022-09-26 02:52:53,162 - mmseg - INFO - Iter [200/40000]	lr: 7.920e-06, eta: 8:02:15, time: 0.705, data_time: 0.006, memory: 7343, decode.loss_seg: 0.9856, decode.acc_seg: 79.9406, loss: 0.9856
2022-09-26 02:53:28,857 - mmseg - INFO - Iter [250/40000]	lr: 9.898e-06, eta: 7:59:55, time: 0.714, data_time: 0.007, memory: 7343, decode.loss_seg: 0.7042, decode.acc_seg: 83.0858, loss: 0.7042
2022-09-26 02:54:04,618 - mmseg - INFO - Iter [300/40000]	lr: 1.187e-05, eta: 7:58:17, time: 0.715, data_time: 0.007, memory: 7343, decode.loss_seg: 0.5840, decode.acc_seg: 82.9376, loss: 0.5840
2022-09-26 02:54:39,738 - mmseg - INFO - Iter [350/40000]	lr: 1.384e-05, eta: 7:55:45, time: 0.702, data_time: 0.007, memory: 7343, decode.loss_seg: 0.4967, decode.acc_seg: 84.3153, loss: 0.4967
2022-09-26 02:55:17,497 - mmseg - INFO - Iter [400/40000]	lr: 1.580e-05, eta: 7:58:04, time: 0.755, data_time: 0.058, memory: 7343, decode.loss_seg: 0.5152, decode.acc_seg: 82.6515, loss: 0.5152
2022-09-26 02:55:53,201 - mmseg - INFO - Iter [450/40000]	lr: 1.776e-05, eta: 7:56:42, time: 0.714, data_time: 0.007, memory: 7343, decode.loss_seg: 0.4453, decode.acc_seg: 85.3477, loss: 0.4453
2022-09-26 02:56:28,108 - mmseg - INFO - Iter [500/40000]	lr: 1.971e-05, eta: 7:54:27, time: 0.698, data_time: 0.006, memory: 7343, decode.loss_seg: 0.4615, decode.acc_seg: 83.1102, loss: 0.4615
2022-09-26 02:57:03,946 - mmseg - INFO - Iter [550/40000]	lr: 2.166e-05, eta: 7:53:37, time: 0.717, data_time: 0.008, memory: 7343, decode.loss_seg: 0.4588, decode.acc_seg: 84.8410, loss: 0.4588
2022-09-26 02:57:39,508 - mmseg - INFO - Iter [600/40000]	lr: 2.360e-05, eta: 7:52:31, time: 0.711, data_time: 0.008, memory: 7343, decode.loss_seg: 0.4387, decode.acc_seg: 84.4867, loss: 0.4387
2022-09-26 02:58:15,115 - mmseg - INFO - Iter [650/40000]	lr: 2.554e-05, eta: 7:51:32, time: 0.712, data_time: 0.006, memory: 7343, decode.loss_seg: 0.4183, decode.acc_seg: 85.2489, loss: 0.4183
2022-09-26 02:58:50,189 - mmseg - INFO - Iter [700/40000]	lr: 2.747e-05, eta: 7:50:07, time: 0.701, data_time: 0.006, memory: 7343, decode.loss_seg: 0.4652, decode.acc_seg: 83.8667, loss: 0.4652
2022-09-26 02:59:27,466 - mmseg - INFO - Iter [750/40000]	lr: 2.940e-05, eta: 7:50:44, time: 0.746, data_time: 0.047, memory: 7343, decode.loss_seg: 0.4355, decode.acc_seg: 84.5134, loss: 0.4355
2022-09-26 03:00:02,877 - mmseg - INFO - Iter [800/40000]	lr: 3.132e-05, eta: 7:49:40, time: 0.708, data_time: 0.006, memory: 7343, decode.loss_seg: 0.4088, decode.acc_seg: 85.0460, loss: 0.4088
2022-09-26 03:00:38,511 - mmseg - INFO - Iter [850/40000]	lr: 3.324e-05, eta: 7:48:50, time: 0.713, data_time: 0.006, memory: 7343, decode.loss_seg: 0.4532, decode.acc_seg: 83.7331, loss: 0.4532
2022-09-26 03:01:13,658 - mmseg - INFO - Iter [900/40000]	lr: 3.515e-05, eta: 7:47:40, time: 0.703, data_time: 0.007, memory: 7343, decode.loss_seg: 0.4072, decode.acc_seg: 85.3132, loss: 0.4072
2022-09-26 03:01:49,022 - mmseg - INFO - Iter [950/40000]	lr: 3.706e-05, eta: 7:46:43, time: 0.707, data_time: 0.007, memory: 7343, decode.loss_seg: 0.4173, decode.acc_seg: 85.0273, loss: 0.4173
2022-09-26 03:02:24,235 - mmseg - INFO - Exp name: zerowaste_to_zerowastev2_segformer
2022-09-26 03:02:24,235 - mmseg - INFO - Iter [1000/40000]	lr: 3.896e-05, eta: 7:45:42, time: 0.704, data_time: 0.007, memory: 7343, decode.loss_seg: 0.4308, decode.acc_seg: 83.7373, loss: 0.4308
2022-09-26 03:03:00,247 - mmseg - INFO - Iter [1050/40000]	lr: 4.086e-05, eta: 7:45:13, time: 0.720, data_time: 0.019, memory: 7343, decode.loss_seg: 0.3859, decode.acc_seg: 87.1161, loss: 0.3859
2022-09-26 03:03:35,843 - mmseg - INFO - Iter [1100/40000]	lr: 4.275e-05, eta: 7:44:29, time: 0.712, data_time: 0.006, memory: 7343, decode.loss_seg: 0.4087, decode.acc_seg: 86.0015, loss: 0.4087
2022-09-26 03:04:11,476 - mmseg - INFO - Iter [1150/40000]	lr: 4.464e-05, eta: 7:43:47, time: 0.713, data_time: 0.007, memory: 7343, decode.loss_seg: 0.3848, decode.acc_seg: 86.6533, loss: 0.3848
2022-09-26 03:04:46,487 - mmseg - INFO - Iter [1200/40000]	lr: 4.652e-05, eta: 7:42:45, time: 0.700, data_time: 0.007, memory: 7343, decode.loss_seg: 0.3966, decode.acc_seg: 85.9002, loss: 0.3966
2022-09-26 03:05:22,230 - mmseg - INFO - Iter [1250/40000]	lr: 4.840e-05, eta: 7:42:08, time: 0.715, data_time: 0.006, memory: 7343, decode.loss_seg: 0.4096, decode.acc_seg: 84.6476, loss: 0.4096
2022-09-26 03:05:57,701 - mmseg - INFO - Iter [1300/40000]	lr: 5.027e-05, eta: 7:41:23, time: 0.709, data_time: 0.006, memory: 7343, decode.loss_seg: 0.3622, decode.acc_seg: 86.8820, loss: 0.3622
2022-09-26 03:06:32,725 - mmseg - INFO - Iter [1350/40000]	lr: 5.214e-05, eta: 7:40:26, time: 0.700, data_time: 0.006, memory: 7343, decode.loss_seg: 0.4839, decode.acc_seg: 83.0169, loss: 0.4839
2022-09-26 03:07:08,474 - mmseg - INFO - Iter [1400/40000]	lr: 5.400e-05, eta: 7:39:50, time: 0.715, data_time: 0.012, memory: 7343, decode.loss_seg: 0.4589, decode.acc_seg: 83.7309, loss: 0.4589
2022-09-26 03:07:44,251 - mmseg - INFO - Iter [1450/40000]	lr: 5.586e-05, eta: 7:39:16, time: 0.716, data_time: 0.006, memory: 7343, decode.loss_seg: 0.4041, decode.acc_seg: 84.9429, loss: 0.4041
2022-09-26 03:08:19,700 - mmseg - INFO - Iter [1500/40000]	lr: 5.771e-05, eta: 7:38:32, time: 0.709, data_time: 0.007, memory: 7343, decode.loss_seg: 0.4060, decode.acc_seg: 84.4224, loss: 0.4060
2022-09-26 03:08:59,552 - mmseg - INFO - Iter [1550/40000]	lr: 5.768e-05, eta: 7:39:39, time: 0.797, data_time: 0.078, memory: 7343, decode.loss_seg: 0.4130, decode.acc_seg: 84.5585, loss: 0.4130
2022-09-26 03:09:35,804 - mmseg - INFO - Iter [1600/40000]	lr: 5.760e-05, eta: 7:39:12, time: 0.725, data_time: 0.017, memory: 7343, decode.loss_seg: 0.3564, decode.acc_seg: 88.0386, loss: 0.3564
2022-09-26 03:10:11,965 - mmseg - INFO - Iter [1650/40000]	lr: 5.753e-05, eta: 7:38:43, time: 0.723, data_time: 0.019, memory: 7343, decode.loss_seg: 0.4001, decode.acc_seg: 85.8672, loss: 0.4001
2022-09-26 03:10:48,242 - mmseg - INFO - Iter [1700/40000]	lr: 5.745e-05, eta: 7:38:16, time: 0.726, data_time: 0.017, memory: 7343, decode.loss_seg: 0.4310, decode.acc_seg: 84.6717, loss: 0.4310
2022-09-26 03:11:24,850 - mmseg - INFO - Iter [1750/40000]	lr: 5.738e-05, eta: 7:37:56, time: 0.732, data_time: 0.018, memory: 7343, decode.loss_seg: 0.4446, decode.acc_seg: 84.0799, loss: 0.4446
2022-09-26 03:12:01,286 - mmseg - INFO - Iter [1800/40000]	lr: 5.730e-05, eta: 7:37:31, time: 0.729, data_time: 0.018, memory: 7343, decode.loss_seg: 0.3796, decode.acc_seg: 86.6606, loss: 0.3796
2022-09-26 03:12:37,361 - mmseg - INFO - Iter [1850/40000]	lr: 5.723e-05, eta: 7:36:58, time: 0.721, data_time: 0.018, memory: 7343, decode.loss_seg: 0.3849, decode.acc_seg: 86.1059, loss: 0.3849
2022-09-26 03:13:13,698 - mmseg - INFO - Iter [1900/40000]	lr: 5.715e-05, eta: 7:36:30, time: 0.727, data_time: 0.018, memory: 7343, decode.loss_seg: 0.3780, decode.acc_seg: 86.5160, loss: 0.3780
2022-09-26 03:13:50,149 - mmseg - INFO - Iter [1950/40000]	lr: 5.708e-05, eta: 7:36:04, time: 0.729, data_time: 0.019, memory: 7343, decode.loss_seg: 0.3675, decode.acc_seg: 86.3696, loss: 0.3675
2022-09-26 03:14:26,560 - mmseg - INFO - Exp name: zerowaste_to_zerowastev2_segformer
2022-09-26 03:14:26,561 - mmseg - INFO - Iter [2000/40000]	lr: 5.700e-05, eta: 7:35:36, time: 0.728, data_time: 0.017, memory: 7343, decode.loss_seg: 0.3063, decode.acc_seg: 89.4050, loss: 0.3063
2022-09-26 03:15:02,861 - mmseg - INFO - Iter [2050/40000]	lr: 5.693e-05, eta: 7:35:07, time: 0.726, data_time: 0.017, memory: 7343, decode.loss_seg: 0.3996, decode.acc_seg: 86.1673, loss: 0.3996
2022-09-26 03:15:39,085 - mmseg - INFO - Iter [2100/40000]	lr: 5.685e-05, eta: 7:34:35, time: 0.724, data_time: 0.019, memory: 7343, decode.loss_seg: 0.3859, decode.acc_seg: 85.6759, loss: 0.3859
2022-09-26 03:16:17,700 - mmseg - INFO - Iter [2150/40000]	lr: 5.678e-05, eta: 7:34:45, time: 0.772, data_time: 0.068, memory: 7343, decode.loss_seg: 0.2948, decode.acc_seg: 89.9968, loss: 0.2948
2022-09-26 03:16:54,215 - mmseg - INFO - Iter [2200/40000]	lr: 5.670e-05, eta: 7:34:17, time: 0.730, data_time: 0.018, memory: 7343, decode.loss_seg: 0.3473, decode.acc_seg: 88.2868, loss: 0.3473
2022-09-26 03:17:30,819 - mmseg - INFO - Iter [2250/40000]	lr: 5.663e-05, eta: 7:33:51, time: 0.732, data_time: 0.018, memory: 7343, decode.loss_seg: 0.3392, decode.acc_seg: 88.6037, loss: 0.3392
2022-09-26 03:18:06,995 - mmseg - INFO - Iter [2300/40000]	lr: 5.655e-05, eta: 7:33:16, time: 0.724, data_time: 0.017, memory: 7343, decode.loss_seg: 0.3225, decode.acc_seg: 88.6542, loss: 0.3225
2022-09-26 03:18:43,255 - mmseg - INFO - Iter [2350/40000]	lr: 5.648e-05, eta: 7:32:43, time: 0.725, data_time: 0.019, memory: 7343, decode.loss_seg: 0.3625, decode.acc_seg: 86.6903, loss: 0.3625
2022-09-26 03:19:19,766 - mmseg - INFO - Iter [2400/40000]	lr: 5.640e-05, eta: 7:32:14, time: 0.730, data_time: 0.017, memory: 7343, decode.loss_seg: 0.3145, decode.acc_seg: 88.6799, loss: 0.3145
2022-09-26 03:19:56,057 - mmseg - INFO - Iter [2450/40000]	lr: 5.633e-05, eta: 7:31:41, time: 0.726, data_time: 0.017, memory: 7343, decode.loss_seg: 0.3397, decode.acc_seg: 87.2924, loss: 0.3397
2022-09-26 03:20:32,635 - mmseg - INFO - Iter [2500/40000]	lr: 5.625e-05, eta: 7:31:12, time: 0.732, data_time: 0.018, memory: 7343, decode.loss_seg: 0.3049, decode.acc_seg: 89.1392, loss: 0.3049
2022-09-26 03:21:09,026 - mmseg - INFO - Iter [2550/40000]	lr: 5.618e-05, eta: 7:30:41, time: 0.728, data_time: 0.017, memory: 7343, decode.loss_seg: 0.3296, decode.acc_seg: 87.6589, loss: 0.3296
2022-09-26 03:21:45,026 - mmseg - INFO - Iter [2600/40000]	lr: 5.610e-05, eta: 7:30:03, time: 0.720, data_time: 0.018, memory: 7343, decode.loss_seg: 0.2612, decode.acc_seg: 90.7381, loss: 0.2612
2022-09-26 03:22:22,155 - mmseg - INFO - Iter [2650/40000]	lr: 5.603e-05, eta: 7:29:41, time: 0.743, data_time: 0.034, memory: 7343, decode.loss_seg: 0.3369, decode.acc_seg: 88.5729, loss: 0.3369
2022-09-26 03:22:58,607 - mmseg - INFO - Iter [2700/40000]	lr: 5.595e-05, eta: 7:29:10, time: 0.729, data_time: 0.017, memory: 7343, decode.loss_seg: 0.3442, decode.acc_seg: 87.9391, loss: 0.3442
2022-09-26 03:23:35,061 - mmseg - INFO - Iter [2750/40000]	lr: 5.588e-05, eta: 7:28:38, time: 0.729, data_time: 0.017, memory: 7343, decode.loss_seg: 0.2941, decode.acc_seg: 89.4068, loss: 0.2941
2022-09-26 03:24:11,564 - mmseg - INFO - Iter [2800/40000]	lr: 5.580e-05, eta: 7:28:07, time: 0.730, data_time: 0.018, memory: 7343, decode.loss_seg: 0.3197, decode.acc_seg: 88.8902, loss: 0.3197
2022-09-26 03:24:47,751 - mmseg - INFO - Iter [2850/40000]	lr: 5.573e-05, eta: 7:27:31, time: 0.724, data_time: 0.019, memory: 7343, decode.loss_seg: 0.3260, decode.acc_seg: 87.9280, loss: 0.3260
2022-09-26 03:25:24,186 - mmseg - INFO - Iter [2900/40000]	lr: 5.565e-05, eta: 7:26:59, time: 0.729, data_time: 0.018, memory: 7343, decode.loss_seg: 0.3752, decode.acc_seg: 86.5838, loss: 0.3752
2022-09-26 03:26:00,553 - mmseg - INFO - Iter [2950/40000]	lr: 5.558e-05, eta: 7:26:26, time: 0.727, data_time: 0.018, memory: 7343, decode.loss_seg: 0.3848, decode.acc_seg: 86.7136, loss: 0.3848
2022-09-26 03:26:36,636 - mmseg - INFO - Exp name: zerowaste_to_zerowastev2_segformer
2022-09-26 03:26:36,636 - mmseg - INFO - Iter [3000/40000]	lr: 5.550e-05, eta: 7:25:49, time: 0.722, data_time: 0.016, memory: 7343, decode.loss_seg: 0.3364, decode.acc_seg: 88.3986, loss: 0.3364
2022-09-26 03:27:16,279 - mmseg - INFO - Iter [3050/40000]	lr: 5.543e-05, eta: 7:25:55, time: 0.793, data_time: 0.075, memory: 7343, decode.loss_seg: 0.2969, decode.acc_seg: 89.6566, loss: 0.2969
2022-09-26 03:27:52,621 - mmseg - INFO - Iter [3100/40000]	lr: 5.535e-05, eta: 7:25:20, time: 0.727, data_time: 0.018, memory: 7343, decode.loss_seg: 0.3615, decode.acc_seg: 86.4755, loss: 0.3615
2022-09-26 03:28:28,689 - mmseg - INFO - Iter [3150/40000]	lr: 5.528e-05, eta: 7:24:43, time: 0.721, data_time: 0.018, memory: 7343, decode.loss_seg: 0.3070, decode.acc_seg: 89.3209, loss: 0.3070
2022-09-26 03:29:06,079 - mmseg - INFO - Iter [3200/40000]	lr: 5.520e-05, eta: 7:24:20, time: 0.748, data_time: 0.042, memory: 7343, decode.loss_seg: 0.2951, decode.acc_seg: 89.9508, loss: 0.2951
2022-09-26 03:29:44,945 - mmseg - INFO - Iter [3250/40000]	lr: 5.513e-05, eta: 7:24:14, time: 0.777, data_time: 0.074, memory: 7343, decode.loss_seg: 0.3041, decode.acc_seg: 88.7252, loss: 0.3041
2022-09-26 03:30:23,751 - mmseg - INFO - Iter [3300/40000]	lr: 5.505e-05, eta: 7:24:06, time: 0.776, data_time: 0.072, memory: 7343, decode.loss_seg: 0.3073, decode.acc_seg: 89.6234, loss: 0.3073
2022-09-26 03:30:59,851 - mmseg - INFO - Iter [3350/40000]	lr: 5.498e-05, eta: 7:23:27, time: 0.722, data_time: 0.017, memory: 7343, decode.loss_seg: 0.2852, decode.acc_seg: 90.0806, loss: 0.2852
2022-09-26 03:31:36,282 - mmseg - INFO - Iter [3400/40000]	lr: 5.490e-05, eta: 7:22:52, time: 0.729, data_time: 0.018, memory: 7343, decode.loss_seg: 0.2816, decode.acc_seg: 90.5096, loss: 0.2816
2022-09-26 03:32:12,927 - mmseg - INFO - Iter [3450/40000]	lr: 5.483e-05, eta: 7:22:20, time: 0.733, data_time: 0.019, memory: 7343, decode.loss_seg: 0.3425, decode.acc_seg: 87.2469, loss: 0.3425
2022-09-26 03:32:49,242 - mmseg - INFO - Iter [3500/40000]	lr: 5.475e-05, eta: 7:21:43, time: 0.726, data_time: 0.018, memory: 7343, decode.loss_seg: 0.3247, decode.acc_seg: 88.2602, loss: 0.3247
2022-09-26 03:33:25,201 - mmseg - INFO - Iter [3550/40000]	lr: 5.468e-05, eta: 7:21:03, time: 0.719, data_time: 0.017, memory: 7343, decode.loss_seg: 0.3636, decode.acc_seg: 87.2999, loss: 0.3636
2022-09-26 03:34:03,453 - mmseg - INFO - Iter [3600/40000]	lr: 5.460e-05, eta: 7:20:47, time: 0.765, data_time: 0.057, memory: 7343, decode.loss_seg: 0.3157, decode.acc_seg: 89.0344, loss: 0.3157
2022-09-26 03:34:41,814 - mmseg - INFO - Iter [3650/40000]	lr: 5.453e-05, eta: 7:20:31, time: 0.767, data_time: 0.062, memory: 7343, decode.loss_seg: 0.3433, decode.acc_seg: 88.2687, loss: 0.3433
2022-09-26 03:35:18,351 - mmseg - INFO - Iter [3700/40000]	lr: 5.445e-05, eta: 7:19:56, time: 0.731, data_time: 0.017, memory: 7343, decode.loss_seg: 0.3026, decode.acc_seg: 89.1215, loss: 0.3026
2022-09-26 03:35:54,845 - mmseg - INFO - Iter [3750/40000]	lr: 5.438e-05, eta: 7:19:21, time: 0.730, data_time: 0.017, memory: 7343, decode.loss_seg: 0.3629, decode.acc_seg: 87.5533, loss: 0.3629
2022-09-26 03:36:30,887 - mmseg - INFO - Iter [3800/40000]	lr: 5.430e-05, eta: 7:18:42, time: 0.721, data_time: 0.018, memory: 7343, decode.loss_seg: 0.2697, decode.acc_seg: 90.9987, loss: 0.2697
2022-09-26 03:37:07,155 - mmseg - INFO - Iter [3850/40000]	lr: 5.423e-05, eta: 7:18:05, time: 0.725, data_time: 0.018, memory: 7343, decode.loss_seg: 0.3006, decode.acc_seg: 90.1703, loss: 0.3006
2022-09-26 03:37:43,571 - mmseg - INFO - Iter [3900/40000]	lr: 5.415e-05, eta: 7:17:29, time: 0.728, data_time: 0.017, memory: 7343, decode.loss_seg: 0.3038, decode.acc_seg: 88.8675, loss: 0.3038
2022-09-26 03:38:20,025 - mmseg - INFO - Iter [3950/40000]	lr: 5.408e-05, eta: 7:16:53, time: 0.729, data_time: 0.018, memory: 7343, decode.loss_seg: 0.3158, decode.acc_seg: 88.9197, loss: 0.3158
2022-09-26 03:38:56,157 - mmseg - INFO - Exp name: zerowaste_to_zerowastev2_segformer
2022-09-26 03:38:56,158 - mmseg - INFO - Iter [4000/40000]	lr: 5.400e-05, eta: 7:16:15, time: 0.722, data_time: 0.017, memory: 7343, decode.loss_seg: 0.3542, decode.acc_seg: 88.0908, loss: 0.3542
2022-09-26 03:39:32,415 - mmseg - INFO - Iter [4050/40000]	lr: 5.393e-05, eta: 7:15:38, time: 0.726, data_time: 0.019, memory: 7343, decode.loss_seg: 0.2817, decode.acc_seg: 90.2383, loss: 0.2817
2022-09-26 03:40:10,842 - mmseg - INFO - Iter [4100/40000]	lr: 5.385e-05, eta: 7:15:19, time: 0.769, data_time: 0.061, memory: 7343, decode.loss_seg: 0.2891, decode.acc_seg: 89.8882, loss: 0.2891
2022-09-26 03:40:47,116 - mmseg - INFO - Iter [4150/40000]	lr: 5.378e-05, eta: 7:14:42, time: 0.725, data_time: 0.018, memory: 7343, decode.loss_seg: 0.2656, decode.acc_seg: 90.9691, loss: 0.2656
2022-09-26 03:41:23,563 - mmseg - INFO - Iter [4200/40000]	lr: 5.370e-05, eta: 7:14:06, time: 0.729, data_time: 0.018, memory: 7343, decode.loss_seg: 0.2668, decode.acc_seg: 89.8551, loss: 0.2668
2022-09-26 03:41:59,760 - mmseg - INFO - Iter [4250/40000]	lr: 5.363e-05, eta: 7:13:28, time: 0.724, data_time: 0.017, memory: 7343, decode.loss_seg: 0.2867, decode.acc_seg: 89.8075, loss: 0.2867
2022-09-26 03:42:35,875 - mmseg - INFO - Iter [4300/40000]	lr: 5.355e-05, eta: 7:12:50, time: 0.723, data_time: 0.019, memory: 7343, decode.loss_seg: 0.3444, decode.acc_seg: 87.2368, loss: 0.3444
2022-09-26 03:43:13,939 - mmseg - INFO - Iter [4350/40000]	lr: 5.348e-05, eta: 7:12:27, time: 0.761, data_time: 0.056, memory: 7343, decode.loss_seg: 0.2707, decode.acc_seg: 90.6976, loss: 0.2707
2022-09-26 03:43:52,490 - mmseg - INFO - Iter [4400/40000]	lr: 5.340e-05, eta: 7:12:08, time: 0.771, data_time: 0.067, memory: 7343, decode.loss_seg: 0.2940, decode.acc_seg: 89.1843, loss: 0.2940
2022-09-26 03:44:28,772 - mmseg - INFO - Iter [4450/40000]	lr: 5.333e-05, eta: 7:11:31, time: 0.726, data_time: 0.018, memory: 7343, decode.loss_seg: 0.3568, decode.acc_seg: 87.5518, loss: 0.3568
2022-09-26 03:45:04,750 - mmseg - INFO - Iter [4500/40000]	lr: 5.325e-05, eta: 7:10:51, time: 0.720, data_time: 0.015, memory: 7343, decode.loss_seg: 0.3109, decode.acc_seg: 89.3085, loss: 0.3109
2022-09-26 03:45:44,511 - mmseg - INFO - Iter [4550/40000]	lr: 5.318e-05, eta: 7:10:41, time: 0.795, data_time: 0.076, memory: 7343, decode.loss_seg: 0.2597, decode.acc_seg: 90.9274, loss: 0.2597
2022-09-26 03:46:20,601 - mmseg - INFO - Iter [4600/40000]	lr: 5.310e-05, eta: 7:10:02, time: 0.722, data_time: 0.017, memory: 7343, decode.loss_seg: 0.2645, decode.acc_seg: 90.7673, loss: 0.2645
2022-09-26 03:46:56,729 - mmseg - INFO - Iter [4650/40000]	lr: 5.303e-05, eta: 7:09:23, time: 0.723, data_time: 0.019, memory: 7343, decode.loss_seg: 0.2648, decode.acc_seg: 90.1766, loss: 0.2648
2022-09-26 03:47:33,162 - mmseg - INFO - Iter [4700/40000]	lr: 5.295e-05, eta: 7:08:46, time: 0.729, data_time: 0.017, memory: 7343, decode.loss_seg: 0.2529, decode.acc_seg: 90.4558, loss: 0.2529
2022-09-26 03:48:09,492 - mmseg - INFO - Iter [4750/40000]	lr: 5.288e-05, eta: 7:08:09, time: 0.727, data_time: 0.018, memory: 7343, decode.loss_seg: 0.2739, decode.acc_seg: 90.7010, loss: 0.2739
2022-09-26 03:48:45,927 - mmseg - INFO - Iter [4800/40000]	lr: 5.280e-05, eta: 7:07:33, time: 0.729, data_time: 0.018, memory: 7343, decode.loss_seg: 0.2726, decode.acc_seg: 90.5986, loss: 0.2726
2022-09-26 03:49:22,017 - mmseg - INFO - Iter [4850/40000]	lr: 5.273e-05, eta: 7:06:54, time: 0.722, data_time: 0.019, memory: 7343, decode.loss_seg: 0.2763, decode.acc_seg: 89.8486, loss: 0.2763
2022-09-26 03:49:59,984 - mmseg - INFO - Iter [4900/40000]	lr: 5.265e-05, eta: 7:06:28, time: 0.759, data_time: 0.054, memory: 7343, decode.loss_seg: 0.2837, decode.acc_seg: 89.7222, loss: 0.2837
2022-09-26 03:50:38,788 - mmseg - INFO - Iter [4950/40000]	lr: 5.258e-05, eta: 7:06:08, time: 0.776, data_time: 0.069, memory: 7343, decode.loss_seg: 0.3316, decode.acc_seg: 88.2572, loss: 0.3316
2022-09-26 03:51:14,928 - mmseg - INFO - Exp name: zerowaste_to_zerowastev2_segformer
2022-09-26 03:51:14,928 - mmseg - INFO - Iter [5000/40000]	lr: 5.250e-05, eta: 7:05:29, time: 0.723, data_time: 0.017, memory: 7343, decode.loss_seg: 0.2957, decode.acc_seg: 89.2557, loss: 0.2957
2022-09-26 03:55:50,016 - mmseg - INFO - per class results:
2022-09-26 03:55:50,017 - mmseg - INFO - 
+---------------+-------+-------+
|     Class     |  IoU  |  Acc  |
+---------------+-------+-------+
|   background  | 89.69 | 95.58 |
| rigid_plastic | 21.18 | 48.63 |
|   cardboard   |  53.6 | 73.49 |
|     metal     |  25.8 | 35.41 |
|  soft_plastic | 34.12 | 35.02 |
+---------------+-------+-------+
2022-09-26 03:55:50,017 - mmseg - INFO - Summary:
2022-09-26 03:55:50,018 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 89.19 | 44.88 | 57.63 |
+-------+-------+-------+
2022-09-26 03:55:50,103 - mmseg - INFO - Exp name: zerowaste_to_zerowastev2_segformer
