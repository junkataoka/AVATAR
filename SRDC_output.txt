==> creating model 'resnet50' 
/data/home/jkataok1/alexnet_resnet_finetune/checkpoints/amazon_to_webcam_resnet50.pkl
Source pre-trained model has been loaded!
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=7, stride=7, padding=0)
  (fc1): Sequential(
    (0): Linear(in_features=2048, out_features=512, bias=True)
    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (fc2): Linear(in_features=512, out_features=31, bias=True)
  (random_layer): RandomLayer()
  (domain_classifier): Sequential(
    (d_fc1): Linear(in_features=1024, out_features=512, bias=True)
    (d_bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (d_relu1): ReLU(inplace=True)
    (d_fc2): Linear(in_features=512, out_features=32, bias=True)
  )
)
https://app.neptune.ai/junkataoka/SRDC/e/SRDC-253
Remember to stop your run once youâ€™ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.
begin training
Test on T training set - [0][0/13]	T 0.674 (0.674)	D 0.548 (0.548)	T@1 100.000 (100.000)	T@5 100.000 (100.000)	L 0.1956 (0.1956)
Test on T training set - [0][10/13]	T 0.459 (0.479)	D 0.347 (0.359)	T@1 76.190 (81.530)	T@5 100.000 (94.517)	L 0.8891 (0.7919)
 * Test on T training set - Prec@1 81.132, Prec@5 94.843
Test on T test set - [0][0/13]	Time 0.659 (0.659)	Loss 0.1956 (0.1956)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Test on T test set - [0][10/13]	Time 0.474 (0.477)	Loss 0.8891 (0.7919)	Prec@1 76.190 (81.530)	Prec@5 100.000 (94.517)
 * Test on T test set - Prec@1 81.132, Prec@5 94.843
Epoch 0, K-means clustering 0, Average clustering time 0.012, Prec@1 74.465
Epoch 0, K-means clustering 1, Average clustering time 0.054, Prec@1 81.258
Epoch 0, K-means clustering 2, Average clustering time 0.061, Prec@1 82.013
Epoch 0, K-means clustering 3, Average clustering time 0.062, Prec@1 82.138
Epoch 0, K-means clustering 4, Average clustering time 0.062, Prec@1 82.642
Epoch 0, K-means clustering 0, Average clustering time 0.001, Prec@1 77.610
Epoch 0, K-means clustering 1, Average clustering time 0.030, Prec@1 83.648
Epoch 0, K-means clustering 2, Average clustering time 0.040, Prec@1 85.283
Epoch 0, K-means clustering 3, Average clustering time 0.045, Prec@1 86.038
Epoch 0, K-means clustering 4, Average clustering time 0.048, Prec@1 86.289
Train - epoch [0/200]	BT 1.992 (1.992)	DT 0.000 (0.000)	S@1 95.238 (95.238)	Loss 0.0000 (0.0000)
The penalty weight is 0.000000
Train - epoch [0/200]	BT 0.583 (0.583)	DT 0.000 (0.000)	S@1 90.476 (90.476)	Loss 0.0000 (0.0000)
Train - epoch [0/200]	BT 0.571 (0.571)	DT 0.000 (0.000)	S@1 97.619 (97.619)	Loss 0.0000 (0.0000)
Train - epoch [0/200]	BT 0.602 (0.602)	DT 0.000 (0.000)	S@1 95.238 (95.238)	Loss 0.0000 (0.0000)
Train - epoch [0/200]	BT 0.601 (0.601)	DT 0.000 (0.000)	S@1 92.857 (92.857)	Loss 0.0000 (0.0000)
Train - epoch [0/200]	BT 0.617 (0.617)	DT 0.000 (0.000)	S@1 92.857 (92.857)	Loss 0.0000 (0.0000)
Train - epoch [0/200]	BT 0.590 (0.590)	DT 0.000 (0.000)	S@1 95.238 (95.238)	Loss 0.0000 (0.0000)
Test on T training set - [0][0/13]	T 0.703 (0.703)	D 0.577 (0.577)	T@1 100.000 (100.000)	T@5 100.000 (100.000)	L 0.1043 (0.1043)
Test on T training set - [0][10/13]	T 0.456 (0.478)	D 0.344 (0.358)	T@1 65.079 (80.952)	T@5 98.413 (95.094)	L 0.9167 (0.7286)
 * Test on T training set - Prec@1 80.000, Prec@5 95.094
Test on T test set - [0][0/13]	Time 0.668 (0.668)	Loss 0.1043 (0.1043)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Test on T test set - [0][10/13]	Time 0.475 (0.475)	Loss 0.9167 (0.7286)	Prec@1 65.079 (80.952)	Prec@5 98.413 (95.094)
 * Test on T test set - Prec@1 80.000, Prec@5 95.094
Epoch 0, K-means clustering 0, Average clustering time 0.130, Prec@1 89.308
Epoch 0, K-means clustering 1, Average clustering time 0.107, Prec@1 90.063
Epoch 0, K-means clustering 2, Average clustering time 0.092, Prec@1 89.308
Epoch 0, K-means clustering 3, Average clustering time 0.085, Prec@1 88.931
Epoch 0, K-means clustering 4, Average clustering time 0.081, Prec@1 88.931
Epoch 0, K-means clustering 0, Average clustering time 0.001, Prec@1 87.799
Epoch 0, K-means clustering 1, Average clustering time 0.031, Prec@1 89.057
Epoch 0, K-means clustering 2, Average clustering time 0.041, Prec@1 89.182
Epoch 0, K-means clustering 3, Average clustering time 0.046, Prec@1 89.182
Epoch 0, K-means clustering 4, Average clustering time 0.049, Prec@1 88.931
The penalty weight is 0.024995
Train - epoch [1/200]	BT 0.548 (0.548)	DT 0.000 (0.000)	S@1 88.095 (88.095)	Loss nan (nan)
Train - epoch [1/200]	BT 0.617 (0.617)	DT 0.000 (0.000)	S@1 0.000 (0.000)	Loss nan (nan)
Train - epoch [1/200]	BT 0.601 (0.601)	DT 0.000 (0.000)	S@1 2.381 (2.381)	Loss nan (nan)
Train - epoch [1/200]	BT 0.592 (0.592)	DT 0.000 (0.000)	S@1 0.000 (0.000)	Loss nan (nan)
Train - epoch [1/200]	BT 0.589 (0.589)	DT 0.000 (0.000)	S@1 4.762 (4.762)	Loss nan (nan)
Train - epoch [1/200]	BT 0.572 (0.572)	DT 0.000 (0.000)	S@1 0.000 (0.000)	Loss nan (nan)
Train - epoch [1/200]	BT 0.605 (0.605)	DT 0.000 (0.000)	S@1 7.143 (7.143)	Loss nan (nan)
Test on T training set - [1][0/13]	T 0.862 (0.862)	D 0.743 (0.743)	T@1 33.333 (33.333)	T@5 100.000 (100.000)	L nan (nan)
Test on T training set - [1][10/13]	T 0.467 (0.491)	D 0.343 (0.370)	T@1 0.000 (3.030)	T@5 0.000 (15.296)	L nan (nan)
Shutting down background jobs, please wait a moment...
Done!
All 31 operations synced, thanks for waiting!
