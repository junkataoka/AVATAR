==> creating model 'resnet50' 
/data/home/jkataok1/CycleGAN-PyTorch/checkpoints/dslr_to_amazon_resnet50.pkl
Source pre-trained model has been loaded!
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=7, stride=7, padding=0)
  (fc1): Sequential(
    (0): Linear(in_features=2048, out_features=512, bias=True)
    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (fc2): Linear(in_features=512, out_features=31, bias=True)
  (domain_classifier): Sequential(
    (d_fc1): Linear(in_features=2560, out_features=512, bias=True)
    (d_bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (d_relu1): ReLU(inplace=True)
    (d_fc2): Linear(in_features=512, out_features=2, bias=True)
    (d_softmax): LogSoftmax(dim=1)
  )
)
https://app.neptune.ai/junkataoka/SRDC/e/SRDC-97
Remember to stop your run once youâ€™ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.
begin training
Train - epoch [0/200]	BT 1.346 (1.346)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.0000 (0.0000)
Test on T training set - [0][0/45]	T 0.452 (0.452)	D 0.328 (0.328)	T@1 85.714 (85.714)	T@5 98.413 (98.413)	L 2.2211 (2.2211)
Test on T training set - [0][10/45]	T 0.321 (0.332)	D 0.195 (0.212)	T@1 34.921 (73.882)	T@5 71.429 (91.053)	L 2.7946 (2.3495)
Test on T training set - [0][20/45]	T 0.331 (0.351)	D 0.218 (0.232)	T@1 66.667 (69.992)	T@5 98.413 (90.174)	L 2.5281 (2.4297)
Test on T training set - [0][30/45]	T 0.311 (0.339)	D 0.186 (0.220)	T@1 76.190 (68.203)	T@5 95.238 (88.479)	L 2.5034 (2.4583)
Test on T training set - [0][40/45]	T 0.303 (0.331)	D 0.179 (0.212)	T@1 49.206 (65.196)	T@5 73.016 (84.204)	L 2.7474 (2.5131)
 * Test on T training set - Prec@1 61.803, Prec@5 82.144
Test on T training set - [0][0/45]	T 0.453 (0.453)	D 0.333 (0.333)	T@1 85.714 (85.714)	T@5 96.825 (96.825)	L 0.6771 (0.6771)
Test on T training set - [0][10/45]	T 0.474 (0.370)	D 0.354 (0.252)	T@1 31.746 (78.211)	T@5 65.079 (90.765)	L 2.8106 (0.9255)
Test on T training set - [0][20/45]	T 0.310 (0.339)	D 0.190 (0.221)	T@1 88.889 (74.225)	T@5 100.000 (89.116)	L 0.4705 (1.0593)
Test on T training set - [0][30/45]	T 0.306 (0.330)	D 0.186 (0.211)	T@1 73.016 (72.555)	T@5 95.238 (88.479)	L 1.1558 (1.1206)
Test on T training set - [0][40/45]	T 0.300 (0.323)	D 0.186 (0.204)	T@1 39.683 (67.983)	T@5 76.190 (84.475)	L 2.2229 (1.3272)
 * Test on T training set - Prec@1 65.673, Prec@5 84.061
Test on T test set - [0][0/45]	Time 0.455 (0.455)	Loss 2.2211 (2.2211)	Prec@1 85.714 (85.714)	Prec@5 98.413 (98.413)
Test on T test set - [0][10/45]	Time 0.317 (0.324)	Loss 2.7946 (2.3495)	Prec@1 34.921 (73.882)	Prec@5 71.429 (91.053)
Test on T test set - [0][20/45]	Time 0.329 (0.323)	Loss 2.5281 (2.4297)	Prec@1 66.667 (69.992)	Prec@5 98.413 (90.174)
Test on T test set - [0][30/45]	Time 0.308 (0.317)	Loss 2.5034 (2.4583)	Prec@1 76.190 (68.203)	Prec@5 95.238 (88.479)
Test on T test set - [0][40/45]	Time 0.311 (0.314)	Loss 2.7474 (2.5131)	Prec@1 49.206 (65.196)	Prec@5 73.016 (84.204)
 * Test on T test set - Prec@1 61.803, Prec@5 82.144
Epoch 0, K-means clustering 0, Average clustering time 0.028, Prec@1 66.454
Epoch 0, K-means clustering 1, Average clustering time 0.076, Prec@1 72.524
Epoch 0, K-means clustering 2, Average clustering time 0.085, Prec@1 73.873
Epoch 0, K-means clustering 3, Average clustering time 0.086, Prec@1 73.837
Epoch 0, K-means clustering 4, Average clustering time 0.088, Prec@1 74.015
Epoch 0, K-means clustering 0, Average clustering time 0.003, Prec@1 65.495
Epoch 0, K-means clustering 1, Average clustering time 0.031, Prec@1 69.720
Epoch 0, K-means clustering 2, Average clustering time 0.041, Prec@1 70.891
Epoch 0, K-means clustering 3, Average clustering time 0.046, Prec@1 71.672
Epoch 0, K-means clustering 4, Average clustering time 0.050, Prec@1 71.565
Test on T test set - [0][0/45]	Time 0.441 (0.441)	Loss 0.6771 (0.6771)	Prec@1 85.714 (85.714)	Prec@5 96.825 (96.825)
Test on T test set - [0][10/45]	Time 0.311 (0.330)	Loss 2.8106 (0.9255)	Prec@1 31.746 (78.211)	Prec@5 65.079 (90.765)
Test on T test set - [0][20/45]	Time 0.306 (0.320)	Loss 0.4705 (1.0593)	Prec@1 88.889 (74.225)	Prec@5 100.000 (89.116)
Test on T test set - [0][30/45]	Time 0.309 (0.314)	Loss 1.1558 (1.1206)	Prec@1 73.016 (72.555)	Prec@5 95.238 (88.479)
Test on T test set - [0][40/45]	Time 0.307 (0.313)	Loss 2.2229 (1.3272)	Prec@1 39.683 (67.983)	Prec@5 76.190 (84.475)
 * Test on T test set - Prec@1 65.673, Prec@5 84.061
Epoch 0, K-means clustering 0, Average clustering time 0.323, Prec@1 72.453
Epoch 0, K-means clustering 1, Average clustering time 0.388, Prec@1 74.263
Epoch 0, K-means clustering 2, Average clustering time 0.287, Prec@1 74.618
Epoch 0, K-means clustering 3, Average clustering time 0.516, Prec@1 74.441
Epoch 0, K-means clustering 4, Average clustering time 0.432, Prec@1 74.050
Epoch 0, K-means clustering 0, Average clustering time 0.003, Prec@1 70.323
Epoch 0, K-means clustering 1, Average clustering time 0.039, Prec@1 70.749
Epoch 0, K-means clustering 2, Average clustering time 0.050, Prec@1 71.211
Epoch 0, K-means clustering 3, Average clustering time 0.056, Prec@1 71.317
Epoch 0, K-means clustering 4, Average clustering time 0.059, Prec@1 71.317
Train - epoch [0/200]	BT 2.384 (2.384)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.0000 (0.0000)
The penalty weight is 0.000000
Train - epoch [0/200]	BT 1.247 (1.247)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.0000 (0.0000)
The penalty weight is 0.024995
Train - epoch [1/200]	BT 1.278 (1.278)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.3916 (0.3916)
Train - epoch [0/200]	BT 1.297 (1.297)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.0000 (0.0000)
Train - epoch [1/200]	BT 1.311 (1.311)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.4406 (0.4406)
Train - epoch [0/200]	BT 1.265 (1.265)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.0000 (0.0000)
Train - epoch [1/200]	BT 1.614 (1.614)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.4380 (0.4380)
Train - epoch [0/200]	BT 1.277 (1.277)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.0000 (0.0000)
Train - epoch [1/200]	BT 1.778 (1.778)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.4218 (0.4218)
Train - epoch [1/200]	BT 1.236 (1.236)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.3960 (0.3960)
Train - epoch [0/200]	BT 1.276 (1.276)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.0000 (0.0000)
Train - epoch [1/200]	BT 1.311 (1.311)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.4555 (0.4555)
Train - epoch [0/200]	BT 1.272 (1.272)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.0000 (0.0000)
Train - epoch [1/200]	BT 1.281 (1.281)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.4044 (0.4044)
Test on T training set - [0][0/45]	T 0.449 (0.449)	D 0.329 (0.329)	T@1 85.714 (85.714)	T@5 96.825 (96.825)	L 0.6771 (0.6771)
Test on T training set - [0][10/45]	T 0.295 (0.324)	D 0.182 (0.202)	T@1 31.746 (78.211)	T@5 65.079 (90.765)	L 2.8106 (0.9255)
Test on T training set - [0][20/45]	T 0.314 (0.329)	D 0.194 (0.208)	T@1 88.889 (74.225)	T@5 100.000 (89.116)	L 0.4705 (1.0593)
Test on T training set - [0][30/45]	T 0.300 (0.322)	D 0.184 (0.201)	T@1 73.016 (72.555)	T@5 95.238 (88.479)	L 1.1558 (1.1206)
Test on T training set - [0][40/45]	T 0.296 (0.317)	D 0.183 (0.197)	T@1 39.683 (67.983)	T@5 76.190 (84.475)	L 2.2229 (1.3272)
 * Test on T training set - Prec@1 65.673, Prec@5 84.061
Test on T training set - [1][0/45]	T 0.459 (0.459)	D 0.337 (0.337)	T@1 85.714 (85.714)	T@5 96.825 (96.825)	L 0.6050 (0.6050)
Test on T training set - [1][10/45]	T 0.312 (0.323)	D 0.189 (0.203)	T@1 34.921 (78.788)	T@5 66.667 (91.198)	L 2.7823 (0.8919)
Test on T training set - [1][20/45]	T 0.291 (0.326)	D 0.178 (0.206)	T@1 87.302 (74.452)	T@5 100.000 (89.418)	L 0.5187 (1.0515)
Test on T training set - [1][30/45]	T 0.314 (0.320)	D 0.189 (0.200)	T@1 74.603 (72.862)	T@5 95.238 (88.530)	L 1.0738 (1.1130)
Test on T training set - [1][40/45]	T 0.298 (0.321)	D 0.186 (0.201)	T@1 39.683 (68.099)	T@5 76.190 (84.630)	L 2.2162 (1.3196)
 * Test on T training set - Prec@1 65.815, Prec@5 84.097
Test on T test set - [0][0/45]	Time 0.442 (0.442)	Loss 0.6771 (0.6771)	Prec@1 85.714 (85.714)	Prec@5 96.825 (96.825)
Test on T test set - [0][10/45]	Time 0.294 (0.334)	Loss 2.8106 (0.9255)	Prec@1 31.746 (78.211)	Prec@5 65.079 (90.765)
Test on T test set - [0][20/45]	Time 0.304 (0.335)	Loss 0.4705 (1.0593)	Prec@1 88.889 (74.225)	Prec@5 100.000 (89.116)
Test on T test set - [0][30/45]	Time 0.297 (0.329)	Loss 1.1558 (1.1206)	Prec@1 73.016 (72.555)	Prec@5 95.238 (88.479)
Test on T test set - [0][40/45]	Time 0.299 (0.323)	Loss 2.2229 (1.3272)	Prec@1 39.683 (67.983)	Prec@5 76.190 (84.475)
 * Test on T test set - Prec@1 65.673, Prec@5 84.061
Epoch 0, K-means clustering 0, Average clustering time 0.286, Prec@1 72.453
Epoch 0, K-means clustering 1, Average clustering time 0.208, Prec@1 74.263
Epoch 0, K-means clustering 2, Average clustering time 0.170, Prec@1 74.618
Epoch 0, K-means clustering 3, Average clustering time 0.149, Prec@1 74.441
Epoch 0, K-means clustering 4, Average clustering time 0.136, Prec@1 74.050
Epoch 0, K-means clustering 0, Average clustering time 0.008, Prec@1 70.323
Epoch 0, K-means clustering 1, Average clustering time 0.035, Prec@1 70.749
Epoch 0, K-means clustering 2, Average clustering time 0.045, Prec@1 71.211
Epoch 0, K-means clustering 3, Average clustering time 0.050, Prec@1 71.317
Epoch 0, K-means clustering 4, Average clustering time 0.052, Prec@1 71.317
Test on T test set - [1][0/45]	Time 0.701 (0.701)	Loss 0.6050 (0.6050)	Prec@1 85.714 (85.714)	Prec@5 96.825 (96.825)
Test on T test set - [1][10/45]	Time 0.288 (0.334)	Loss 2.7823 (0.8919)	Prec@1 34.921 (78.788)	Prec@5 66.667 (91.198)
Test on T test set - [1][20/45]	Time 0.315 (0.340)	Loss 0.5187 (1.0515)	Prec@1 87.302 (74.452)	Prec@5 100.000 (89.418)
Test on T test set - [1][30/45]	Time 0.312 (0.329)	Loss 1.0738 (1.1130)	Prec@1 74.603 (72.862)	Prec@5 95.238 (88.530)
Test on T test set - [1][40/45]	Time 0.299 (0.323)	Loss 2.2162 (1.3196)	Prec@1 39.683 (68.099)	Prec@5 76.190 (84.630)
 * Test on T test set - Prec@1 65.815, Prec@5 84.097
Epoch 1, K-means clustering 0, Average clustering time 0.476, Prec@1 72.311
Epoch 1, K-means clustering 1, Average clustering time 0.298, Prec@1 73.837
Epoch 1, K-means clustering 2, Average clustering time 0.228, Prec@1 74.618
Epoch 1, K-means clustering 3, Average clustering time 0.193, Prec@1 74.618
Epoch 1, K-means clustering 4, Average clustering time 0.173, Prec@1 74.476
Epoch 1, K-means clustering 0, Average clustering time 0.003, Prec@1 70.288
Epoch 1, K-means clustering 1, Average clustering time 0.107, Prec@1 71.033
Epoch 1, K-means clustering 2, Average clustering time 0.262, Prec@1 71.530
Epoch 1, K-means clustering 3, Average clustering time 0.214, Prec@1 71.459
Epoch 1, K-means clustering 4, Average clustering time 0.185, Prec@1 71.175
The penalty weight is 0.024995
Train - epoch [1/200]	BT 1.309 (1.309)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.4287 (0.4287)
The penalty weight is 0.049958
Train - epoch [2/200]	BT 1.243 (1.243)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.8851 (0.8851)
Train - epoch [1/200]	BT 1.330 (1.330)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.4777 (0.4777)
Train - epoch [2/200]	BT 1.253 (1.253)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.8705 (0.8705)
Train - epoch [1/200]	BT 1.331 (1.331)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.4751 (0.4751)
Train - epoch [2/200]	BT 1.274 (1.274)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.8798 (0.8798)
Train - epoch [1/200]	BT 1.726 (1.726)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.4588 (0.4588)
Train - epoch [1/200]	BT 1.226 (1.226)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.4328 (0.4328)
Train - epoch [2/200]	BT 1.297 (1.297)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.8848 (0.8848)
Train - epoch [1/200]	BT 1.304 (1.304)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.4926 (0.4926)
Train - epoch [2/200]	BT 1.267 (1.267)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.7853 (0.7853)
Train - epoch [1/200]	BT 1.281 (1.281)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.4411 (0.4411)
Train - epoch [2/200]	BT 1.317 (1.317)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.8974 (0.8974)
Train - epoch [2/200]	BT 1.702 (1.702)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.8655 (0.8655)
Test on T training set - [1][0/45]	T 0.439 (0.439)	D 0.322 (0.322)	T@1 85.714 (85.714)	T@5 96.825 (96.825)	L 0.6050 (0.6050)
Test on T training set - [1][10/45]	T 0.416 (0.323)	D 0.293 (0.204)	T@1 34.921 (78.788)	T@5 66.667 (91.198)	L 2.7823 (0.8919)
Test on T training set - [1][20/45]	T 0.303 (0.325)	D 0.185 (0.207)	T@1 87.302 (74.452)	T@5 100.000 (89.418)	L 0.5187 (1.0515)
Test on T training set - [1][30/45]	T 0.304 (0.327)	D 0.182 (0.209)	T@1 74.603 (72.862)	T@5 95.238 (88.530)	L 1.0738 (1.1130)
Test on T training set - [1][40/45]	T 0.308 (0.327)	D 0.183 (0.209)	T@1 39.683 (68.099)	T@5 76.190 (84.630)	L 2.2162 (1.3196)
 * Test on T training set - Prec@1 65.815, Prec@5 84.097
Test on T training set - [2][0/45]	T 0.446 (0.446)	D 0.330 (0.330)	T@1 85.714 (85.714)	T@5 96.825 (96.825)	L 0.6369 (0.6369)
Test on T training set - [2][10/45]	T 0.540 (0.334)	D 0.414 (0.215)	T@1 34.921 (78.644)	T@5 68.254 (91.198)	L 2.7909 (0.8965)
Test on T training set - [2][20/45]	T 0.313 (0.333)	D 0.191 (0.214)	T@1 85.714 (74.301)	T@5 100.000 (89.720)	L 0.5829 (1.0554)
Test on T training set - [2][30/45]	T 0.300 (0.331)	D 0.182 (0.213)	T@1 73.016 (72.709)	T@5 93.651 (88.991)	L 0.9995 (1.1074)
Test on T training set - [2][40/45]	T 0.300 (0.331)	D 0.185 (0.212)	T@1 41.270 (68.525)	T@5 77.778 (85.288)	L 2.0899 (1.3096)
 * Test on T training set - Prec@1 66.312, Prec@5 84.807
Test on T test set - [1][0/45]	Time 0.455 (0.455)	Loss 0.6050 (0.6050)	Prec@1 85.714 (85.714)	Prec@5 96.825 (96.825)
Test on T test set - [1][10/45]	Time 0.310 (0.343)	Loss 2.7823 (0.8919)	Prec@1 34.921 (78.788)	Prec@5 66.667 (91.198)
Test on T test set - [1][20/45]	Time 0.319 (0.325)	Loss 0.5187 (1.0515)	Prec@1 87.302 (74.452)	Prec@5 100.000 (89.418)
Test on T test set - [1][30/45]	Time 0.305 (0.328)	Loss 1.0738 (1.1130)	Prec@1 74.603 (72.862)	Prec@5 95.238 (88.530)
Test on T test set - [1][40/45]	Time 0.298 (0.321)	Loss 2.2162 (1.3196)	Prec@1 39.683 (68.099)	Prec@5 76.190 (84.630)
 * Test on T test set - Prec@1 65.815, Prec@5 84.097
Epoch 1, K-means clustering 0, Average clustering time 0.028, Prec@1 72.311
Epoch 1, K-means clustering 1, Average clustering time 0.074, Prec@1 73.837
Epoch 1, K-means clustering 2, Average clustering time 0.074, Prec@1 74.618
Epoch 1, K-means clustering 3, Average clustering time 0.074, Prec@1 74.618
Epoch 1, K-means clustering 4, Average clustering time 0.074, Prec@1 74.476
Epoch 1, K-means clustering 0, Average clustering time 0.003, Prec@1 70.288
Epoch 1, K-means clustering 1, Average clustering time 0.043, Prec@1 71.033
Epoch 1, K-means clustering 2, Average clustering time 0.055, Prec@1 71.530
Epoch 1, K-means clustering 3, Average clustering time 0.062, Prec@1 71.459
Epoch 1, K-means clustering 4, Average clustering time 0.063, Prec@1 71.175
Test on T test set - [2][0/45]	Time 0.450 (0.450)	Loss 0.6369 (0.6369)	Prec@1 85.714 (85.714)	Prec@5 96.825 (96.825)
Test on T test set - [2][10/45]	Time 0.303 (0.341)	Loss 2.7909 (0.8965)	Prec@1 34.921 (78.644)	Prec@5 68.254 (91.198)
Test on T test set - [2][20/45]	Time 0.320 (0.324)	Loss 0.5829 (1.0554)	Prec@1 85.714 (74.301)	Prec@5 100.000 (89.720)
Test on T test set - [2][30/45]	Time 0.301 (0.327)	Loss 0.9995 (1.1074)	Prec@1 73.016 (72.709)	Prec@5 93.651 (88.991)
Test on T test set - [2][40/45]	Time 0.302 (0.320)	Loss 2.0899 (1.3096)	Prec@1 41.270 (68.525)	Prec@5 77.778 (85.288)
 * Test on T test set - Prec@1 66.312, Prec@5 84.807
Epoch 2, K-means clustering 0, Average clustering time 0.483, Prec@1 73.056
Epoch 2, K-means clustering 1, Average clustering time 0.297, Prec@1 74.476
Epoch 2, K-means clustering 2, Average clustering time 0.223, Prec@1 74.796
Epoch 2, K-means clustering 3, Average clustering time 0.186, Prec@1 75.009
Epoch 2, K-means clustering 4, Average clustering time 0.164, Prec@1 74.902
Epoch 2, K-means clustering 0, Average clustering time 0.003, Prec@1 70.749
Epoch 2, K-means clustering 1, Average clustering time 0.035, Prec@1 71.494
Epoch 2, K-means clustering 2, Average clustering time 0.045, Prec@1 71.849
Epoch 2, K-means clustering 3, Average clustering time 0.050, Prec@1 72.027
Epoch 2, K-means clustering 4, Average clustering time 0.053, Prec@1 71.956
The penalty weight is 0.049958
Train - epoch [2/200]	BT 1.270 (1.270)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.9581 (0.9581)
The penalty weight is 0.074860
Train - epoch [3/200]	BT 1.253 (1.253)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.3159 (1.3159)
Train - epoch [2/200]	BT 1.270 (1.270)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.9452 (0.9452)
Train - epoch [3/200]	BT 1.263 (1.263)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.3045 (1.3045)
Train - epoch [3/200]	BT 1.304 (1.304)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.1611 (1.1611)
Train - epoch [2/200]	BT 1.268 (1.268)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.9519 (0.9519)
Train - epoch [3/200]	BT 1.283 (1.283)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.3353 (1.3353)
Train - epoch [2/200]	BT 1.286 (1.286)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.9572 (0.9572)
Train - epoch [3/200]	BT 1.293 (1.293)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.2061 (1.2061)
Train - epoch [2/200]	BT 1.299 (1.299)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.8588 (0.8588)
Train - epoch [3/200]	BT 1.328 (1.328)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.2357 (1.2357)
Train - epoch [2/200]	BT 1.259 (1.259)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.9704 (0.9704)
Train - epoch [2/200]	BT 1.686 (1.686)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.9384 (0.9384)
Test on T training set - [3][0/45]	T 0.448 (0.448)	D 0.334 (0.334)	T@1 84.127 (84.127)	T@5 96.825 (96.825)	L 0.6996 (0.6996)
Test on T training set - [3][10/45]	T 0.323 (0.319)	D 0.201 (0.199)	T@1 33.333 (78.499)	T@5 66.667 (90.909)	L 2.7628 (0.9022)
Test on T training set - [3][20/45]	T 0.303 (0.313)	D 0.190 (0.194)	T@1 84.127 (74.150)	T@5 100.000 (89.191)	L 0.5350 (1.0590)
Test on T training set - [3][30/45]	T 0.314 (0.312)	D 0.192 (0.192)	T@1 73.016 (72.401)	T@5 92.063 (88.428)	L 1.0708 (1.1190)
Test on T training set - [3][40/45]	T 0.310 (0.310)	D 0.188 (0.191)	T@1 39.683 (68.060)	T@5 77.778 (84.669)	L 2.0697 (1.3177)
 * Test on T training set - Prec@1 65.921, Prec@5 84.203
Test on T training set - [2][0/45]	T 0.444 (0.444)	D 0.321 (0.321)	T@1 85.714 (85.714)	T@5 96.825 (96.825)	L 0.6370 (0.6370)
Test on T training set - [2][10/45]	T 0.308 (0.315)	D 0.187 (0.193)	T@1 34.921 (78.644)	T@5 68.254 (91.198)	L 2.7909 (0.8965)
Test on T training set - [2][20/45]	T 0.303 (0.308)	D 0.190 (0.189)	T@1 85.714 (74.301)	T@5 100.000 (89.720)	L 0.5829 (1.0554)
Test on T training set - [2][30/45]	T 0.303 (0.308)	D 0.178 (0.189)	T@1 73.016 (72.709)	T@5 93.651 (88.991)	L 0.9995 (1.1074)
Test on T training set - [2][40/45]	T 0.301 (0.307)	D 0.187 (0.187)	T@1 41.270 (68.525)	T@5 77.778 (85.288)	L 2.0899 (1.3096)
 * Test on T training set - Prec@1 66.312, Prec@5 84.807
Test on T test set - [2][0/45]	Time 0.450 (0.450)	Loss 0.6370 (0.6370)	Prec@1 85.714 (85.714)	Prec@5 96.825 (96.825)
Test on T test set - [2][10/45]	Time 0.304 (0.315)	Loss 2.7909 (0.8965)	Prec@1 34.921 (78.644)	Prec@5 68.254 (91.198)
Test on T test set - [2][20/45]	Time 0.301 (0.310)	Loss 0.5829 (1.0554)	Prec@1 85.714 (74.301)	Prec@5 100.000 (89.720)
Test on T test set - [2][30/45]	Time 0.300 (0.313)	Loss 0.9995 (1.1074)	Prec@1 73.016 (72.709)	Prec@5 93.651 (88.991)
Test on T test set - [2][40/45]	Time 0.293 (0.314)	Loss 2.0899 (1.3096)	Prec@1 41.270 (68.525)	Prec@5 77.778 (85.288)
 * Test on T test set - Prec@1 66.312, Prec@5 84.807
Epoch 2, K-means clustering 0, Average clustering time 0.028, Prec@1 73.056
Epoch 2, K-means clustering 1, Average clustering time 0.073, Prec@1 74.476
Epoch 2, K-means clustering 2, Average clustering time 0.073, Prec@1 74.796
Epoch 2, K-means clustering 3, Average clustering time 0.078, Prec@1 75.009
Epoch 2, K-means clustering 4, Average clustering time 0.080, Prec@1 74.902
Epoch 2, K-means clustering 0, Average clustering time 0.008, Prec@1 70.749
Epoch 2, K-means clustering 1, Average clustering time 0.042, Prec@1 71.494
Epoch 2, K-means clustering 2, Average clustering time 0.051, Prec@1 71.849
Epoch 2, K-means clustering 3, Average clustering time 0.056, Prec@1 72.027
Epoch 2, K-means clustering 4, Average clustering time 0.059, Prec@1 71.956
Test on T test set - [3][0/45]	Time 0.476 (0.476)	Loss 0.6996 (0.6996)	Prec@1 84.127 (84.127)	Prec@5 96.825 (96.825)
Test on T test set - [3][10/45]	Time 0.302 (0.319)	Loss 2.7628 (0.9022)	Prec@1 33.333 (78.499)	Prec@5 66.667 (90.909)
Test on T test set - [3][20/45]	Time 0.302 (0.313)	Loss 0.5350 (1.0590)	Prec@1 84.127 (74.150)	Prec@5 100.000 (89.191)
Test on T test set - [3][30/45]	Time 0.300 (0.313)	Loss 1.0708 (1.1190)	Prec@1 73.016 (72.401)	Prec@5 92.063 (88.428)
Test on T test set - [3][40/45]	Time 0.305 (0.313)	Loss 2.0697 (1.3177)	Prec@1 39.683 (68.060)	Prec@5 77.778 (84.669)
 * Test on T test set - Prec@1 65.921, Prec@5 84.203
Epoch 3, K-means clustering 0, Average clustering time 0.474, Prec@1 72.950
Epoch 3, K-means clustering 1, Average clustering time 0.294, Prec@1 74.405
Epoch 3, K-means clustering 2, Average clustering time 0.223, Prec@1 74.725
Epoch 3, K-means clustering 3, Average clustering time 0.184, Prec@1 74.512
Epoch 3, K-means clustering 4, Average clustering time 0.162, Prec@1 74.370
Epoch 3, K-means clustering 0, Average clustering time 0.003, Prec@1 70.856
Epoch 3, K-means clustering 1, Average clustering time 0.038, Prec@1 71.672
Epoch 3, K-means clustering 2, Average clustering time 0.050, Prec@1 72.240
Epoch 3, K-means clustering 3, Average clustering time 0.056, Prec@1 72.240
Epoch 3, K-means clustering 4, Average clustering time 0.059, Prec@1 72.169
The penalty weight is 0.099668
Train - epoch [4/200]	BT 1.292 (1.292)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.7234 (1.7234)
The penalty weight is 0.074860
Train - epoch [3/200]	BT 1.302 (1.302)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.4255 (1.4255)
Train - epoch [3/200]	BT 1.285 (1.285)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.4132 (1.4132)
Train - epoch [4/200]	BT 1.281 (1.281)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.7234 (1.7234)
Train - epoch [4/200]	BT 1.754 (1.754)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.7769 (1.7769)
Train - epoch [4/200]	BT 1.216 (1.216)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.6019 (1.6019)
Train - epoch [3/200]	BT 1.292 (1.292)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.2687 (1.2687)
Train - epoch [3/200]	BT 1.292 (1.292)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.4433 (1.4433)
Train - epoch [4/200]	BT 1.261 (1.261)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.7627 (1.7627)
Train - epoch [4/200]	BT 1.292 (1.292)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.5727 (1.5727)
Train - epoch [3/200]	BT 1.279 (1.279)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.3147 (1.3147)
Train - epoch [4/200]	BT 1.240 (1.240)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.6238 (1.6238)
Train - epoch [3/200]	BT 1.301 (1.301)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.3432 (1.3432)
Test on T training set - [3][0/45]	T 0.443 (0.443)	D 0.318 (0.318)	T@1 84.127 (84.127)	T@5 96.825 (96.825)	L 0.6996 (0.6996)
Test on T training set - [3][10/45]	T 0.299 (0.359)	D 0.177 (0.240)	T@1 33.333 (78.499)	T@5 66.667 (90.909)	L 2.7626 (0.9023)
Test on T training set - [3][20/45]	T 0.293 (0.360)	D 0.180 (0.242)	T@1 84.127 (74.150)	T@5 100.000 (89.191)	L 0.5351 (1.0590)
Test on T training set - [3][30/45]	T 0.297 (0.341)	D 0.178 (0.222)	T@1 73.016 (72.401)	T@5 92.063 (88.428)	L 1.0711 (1.1189)
Test on T training set - [3][40/45]	T 0.303 (0.340)	D 0.177 (0.220)	T@1 39.683 (68.060)	T@5 77.778 (84.669)	L 2.0696 (1.3176)
 * Test on T training set - Prec@1 65.921, Prec@5 84.203
Test on T training set - [4][0/45]	T 0.451 (0.451)	D 0.333 (0.333)	T@1 85.714 (85.714)	T@5 98.413 (98.413)	L 0.6130 (0.6130)
Test on T training set - [4][10/45]	T 0.302 (0.373)	D 0.177 (0.253)	T@1 31.746 (78.788)	T@5 68.254 (91.919)	L 2.7906 (0.8698)
Test on T training set - [4][20/45]	T 0.298 (0.375)	D 0.178 (0.255)	T@1 87.302 (74.679)	T@5 100.000 (90.023)	L 0.4504 (1.0351)
Test on T training set - [4][30/45]	T 0.306 (0.353)	D 0.185 (0.234)	T@1 71.429 (72.350)	T@5 92.063 (88.940)	L 1.1810 (1.1120)
Test on T training set - [4][40/45]	T 0.297 (0.350)	D 0.184 (0.231)	T@1 41.270 (67.751)	T@5 77.778 (85.017)	L 2.1048 (1.3258)
 * Test on T training set - Prec@1 65.566, Prec@5 84.629
Test on T test set - [3][0/45]	Time 0.430 (0.430)	Loss 0.6996 (0.6996)	Prec@1 84.127 (84.127)	Prec@5 96.825 (96.825)
Test on T test set - [3][10/45]	Time 0.303 (0.319)	Loss 2.7626 (0.9023)	Prec@1 33.333 (78.499)	Prec@5 66.667 (90.909)
Test on T test set - [3][20/45]	Time 0.298 (0.312)	Loss 0.5351 (1.0590)	Prec@1 84.127 (74.150)	Prec@5 100.000 (89.191)
Test on T test set - [3][30/45]	Time 0.297 (0.308)	Loss 1.0711 (1.1189)	Prec@1 73.016 (72.401)	Prec@5 92.063 (88.428)
Test on T test set - [3][40/45]	Time 0.302 (0.308)	Loss 2.0696 (1.3176)	Prec@1 39.683 (68.060)	Prec@5 77.778 (84.669)
 * Test on T test set - Prec@1 65.921, Prec@5 84.203
Epoch 3, K-means clustering 0, Average clustering time 0.028, Prec@1 72.950
Epoch 3, K-means clustering 1, Average clustering time 0.073, Prec@1 74.476
Epoch 3, K-means clustering 2, Average clustering time 0.077, Prec@1 74.760
Epoch 3, K-means clustering 3, Average clustering time 0.076, Prec@1 74.476
Epoch 3, K-means clustering 4, Average clustering time 0.075, Prec@1 74.405
Epoch 3, K-means clustering 0, Average clustering time 0.009, Prec@1 70.820
Epoch 3, K-means clustering 1, Average clustering time 0.033, Prec@1 71.672
Epoch 3, K-means clustering 2, Average clustering time 0.105, Prec@1 72.240
Epoch 3, K-means clustering 3, Average clustering time 0.099, Prec@1 72.240
Epoch 3, K-means clustering 4, Average clustering time 0.095, Prec@1 72.169
Test on T test set - [4][0/45]	Time 0.659 (0.659)	Loss 0.6130 (0.6130)	Prec@1 85.714 (85.714)	Prec@5 98.413 (98.413)
Test on T test set - [4][10/45]	Time 0.296 (0.335)	Loss 2.7906 (0.8698)	Prec@1 31.746 (78.788)	Prec@5 68.254 (91.919)
Test on T test set - [4][20/45]	Time 0.302 (0.323)	Loss 0.4504 (1.0351)	Prec@1 87.302 (74.679)	Prec@5 100.000 (90.023)
Test on T test set - [4][30/45]	Time 0.402 (0.318)	Loss 1.1810 (1.1120)	Prec@1 71.429 (72.350)	Prec@5 92.063 (88.940)
Test on T test set - [4][40/45]	Time 0.296 (0.313)	Loss 2.1048 (1.3258)	Prec@1 41.270 (67.751)	Prec@5 77.778 (85.017)
 * Test on T test set - Prec@1 65.566, Prec@5 84.629
Epoch 4, K-means clustering 0, Average clustering time 0.484, Prec@1 72.843
Epoch 4, K-means clustering 1, Average clustering time 0.319, Prec@1 74.441
Epoch 4, K-means clustering 2, Average clustering time 0.237, Prec@1 74.689
Epoch 4, K-means clustering 3, Average clustering time 0.197, Prec@1 74.370
Epoch 4, K-means clustering 4, Average clustering time 0.173, Prec@1 74.157
Epoch 4, K-means clustering 0, Average clustering time 0.003, Prec@1 70.607
Epoch 4, K-means clustering 1, Average clustering time 0.035, Prec@1 71.530
Epoch 4, K-means clustering 2, Average clustering time 0.046, Prec@1 71.707
Epoch 4, K-means clustering 3, Average clustering time 0.051, Prec@1 72.062
Epoch 4, K-means clustering 4, Average clustering time 0.055, Prec@1 72.133
The penalty weight is 0.099668
Train - epoch [4/200]	BT 1.299 (1.299)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.8656 (1.8656)
The penalty weight is 0.124353
Train - epoch [5/200]	BT 1.253 (1.253)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.1033 (2.1033)
Train - epoch [4/200]	BT 1.264 (1.264)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.8634 (1.8634)
Train - epoch [5/200]	BT 1.329 (1.329)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.0583 (2.0583)
Train - epoch [4/200]	BT 1.788 (1.788)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.9201 (1.9201)
Train - epoch [4/200]	BT 1.219 (1.219)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.7446 (1.7446)
Train - epoch [5/200]	BT 1.504 (1.504)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.9736 (1.9736)
Train - epoch [4/200]	BT 1.382 (1.382)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.9034 (1.9034)
Train - epoch [5/200]	BT 1.266 (1.266)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.1292 (2.1292)
Train - epoch [4/200]	BT 1.301 (1.301)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.7148 (1.7148)
Train - epoch [5/200]	BT 1.248 (1.248)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.1263 (2.1263)
Train - epoch [4/200]	BT 1.245 (1.245)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.7636 (1.7636)
Train - epoch [5/200]	BT 1.751 (1.751)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.2484 (2.2484)
Train - epoch [5/200]	BT 1.105 (1.105)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.1864 (2.1864)
Test on T training set - [4][0/45]	T 0.425 (0.425)	D 0.310 (0.310)	T@1 85.714 (85.714)	T@5 98.413 (98.413)	L 0.6129 (0.6129)
Test on T training set - [4][10/45]	T 0.300 (0.316)	D 0.187 (0.199)	T@1 31.746 (78.788)	T@5 68.254 (91.919)	L 2.7902 (0.8698)
Test on T training set - [4][20/45]	T 0.302 (0.311)	D 0.182 (0.192)	T@1 87.302 (74.679)	T@5 100.000 (90.023)	L 0.4506 (1.0351)
Test on T training set - [4][30/45]	T 0.295 (0.308)	D 0.182 (0.190)	T@1 71.429 (72.350)	T@5 92.063 (88.940)	L 1.1812 (1.1118)
Test on T training set - [4][40/45]	T 0.304 (0.307)	D 0.187 (0.189)	T@1 41.270 (67.789)	T@5 77.778 (85.017)	L 2.1045 (1.3257)
 * Test on T training set - Prec@1 65.602, Prec@5 84.629
Test on T training set - [5][0/45]	T 0.453 (0.453)	D 0.339 (0.339)	T@1 85.714 (85.714)	T@5 96.825 (96.825)	L 0.6257 (0.6257)
Test on T training set - [5][10/45]	T 0.304 (0.325)	D 0.191 (0.206)	T@1 36.508 (78.788)	T@5 68.254 (91.486)	L 2.6785 (0.8907)
Test on T training set - [5][20/45]	T 0.307 (0.315)	D 0.183 (0.196)	T@1 84.127 (74.528)	T@5 100.000 (89.796)	L 0.6522 (1.0524)
Test on T training set - [5][30/45]	T 0.304 (0.312)	D 0.179 (0.192)	T@1 73.016 (72.657)	T@5 95.238 (88.684)	L 1.0301 (1.1116)
Test on T training set - [5][40/45]	T 0.304 (0.309)	D 0.191 (0.190)	T@1 36.508 (68.060)	T@5 77.778 (84.901)	L 2.1946 (1.3128)
 * Test on T training set - Prec@1 65.886, Prec@5 84.487
Test on T test set - [4][0/45]	Time 0.451 (0.451)	Loss 0.6129 (0.6129)	Prec@1 85.714 (85.714)	Prec@5 98.413 (98.413)
Test on T test set - [4][10/45]	Time 0.302 (0.316)	Loss 2.7902 (0.8698)	Prec@1 31.746 (78.788)	Prec@5 68.254 (91.919)
Test on T test set - [4][20/45]	Time 0.315 (0.334)	Loss 0.4506 (1.0351)	Prec@1 87.302 (74.679)	Prec@5 100.000 (90.023)
Test on T test set - [4][30/45]	Time 0.305 (0.323)	Loss 1.1812 (1.1118)	Prec@1 71.429 (72.350)	Prec@5 92.063 (88.940)
Test on T test set - [4][40/45]	Time 0.306 (0.318)	Loss 2.1045 (1.3257)	Prec@1 41.270 (67.789)	Prec@5 77.778 (85.017)
 * Test on T test set - Prec@1 65.602, Prec@5 84.629
Epoch 4, K-means clustering 0, Average clustering time 0.028, Prec@1 72.914
Epoch 4, K-means clustering 1, Average clustering time 0.072, Prec@1 74.405
Epoch 4, K-means clustering 2, Average clustering time 0.072, Prec@1 74.654
Epoch 4, K-means clustering 3, Average clustering time 0.071, Prec@1 74.334
Epoch 4, K-means clustering 4, Average clustering time 0.071, Prec@1 74.121
Epoch 4, K-means clustering 0, Average clustering time 0.008, Prec@1 70.572
Epoch 4, K-means clustering 1, Average clustering time 0.040, Prec@1 71.530
Epoch 4, K-means clustering 2, Average clustering time 0.050, Prec@1 71.743
Epoch 4, K-means clustering 3, Average clustering time 0.056, Prec@1 72.062
Epoch 4, K-means clustering 4, Average clustering time 0.060, Prec@1 72.169
Test on T test set - [5][0/45]	Time 0.461 (0.461)	Loss 0.6257 (0.6257)	Prec@1 85.714 (85.714)	Prec@5 96.825 (96.825)
Test on T test set - [5][10/45]	Time 0.300 (0.318)	Loss 2.6785 (0.8907)	Prec@1 36.508 (78.788)	Prec@5 68.254 (91.486)
Test on T test set - [5][20/45]	Time 0.298 (0.333)	Loss 0.6522 (1.0524)	Prec@1 84.127 (74.528)	Prec@5 100.000 (89.796)
Test on T test set - [5][30/45]	Time 0.294 (0.323)	Loss 1.0301 (1.1116)	Prec@1 73.016 (72.657)	Prec@5 95.238 (88.684)
Test on T test set - [5][40/45]	Time 0.306 (0.318)	Loss 2.1946 (1.3128)	Prec@1 36.508 (68.060)	Prec@5 77.778 (84.901)
 * Test on T test set - Prec@1 65.886, Prec@5 84.487
Epoch 5, K-means clustering 0, Average clustering time 0.483, Prec@1 72.701
Epoch 5, K-means clustering 1, Average clustering time 0.303, Prec@1 73.979
Epoch 5, K-means clustering 2, Average clustering time 0.231, Prec@1 74.050
Epoch 5, K-means clustering 3, Average clustering time 0.193, Prec@1 73.837
Epoch 5, K-means clustering 4, Average clustering time 0.171, Prec@1 73.624
Epoch 5, K-means clustering 0, Average clustering time 0.003, Prec@1 70.146
Epoch 5, K-means clustering 1, Average clustering time 0.034, Prec@1 70.785
Epoch 5, K-means clustering 2, Average clustering time 0.044, Prec@1 71.175
Epoch 5, K-means clustering 3, Average clustering time 0.050, Prec@1 71.033
Epoch 5, K-means clustering 4, Average clustering time 0.054, Prec@1 70.891
The penalty weight is 0.124353
Train - epoch [5/200]	BT 1.397 (1.397)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.2789 (2.2789)
The penalty weight is 0.148885
Train - epoch [6/200]	BT 1.189 (1.189)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.4164 (2.4164)
Train - epoch [5/200]	BT 1.302 (1.302)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.2346 (2.2346)
Train - epoch [6/200]	BT 1.261 (1.261)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.4262 (2.4262)
Train - epoch [5/200]	BT 1.300 (1.300)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.1501 (2.1501)
Train - epoch [6/200]	BT 1.295 (1.295)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.4938 (2.4938)
Train - epoch [6/200]	BT 1.300 (1.300)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.5058 (2.5058)
Train - epoch [5/200]	BT 1.244 (1.244)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.3053 (2.3053)
Train - epoch [5/200]	BT 1.263 (1.263)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.3002 (2.3002)
Train - epoch [6/200]	BT 1.285 (1.285)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.4468 (2.4468)
Train - epoch [6/200]	BT 1.316 (1.316)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6339 (2.6339)
Train - epoch [5/200]	BT 1.726 (1.726)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.4230 (2.4230)
Train - epoch [5/200]	BT 1.124 (1.124)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.3635 (2.3635)
Test on T training set - [6][0/45]	T 0.460 (0.460)	D 0.337 (0.337)	T@1 84.127 (84.127)	T@5 96.825 (96.825)	L 0.6922 (0.6922)
Test on T training set - [6][10/45]	T 0.314 (0.319)	D 0.191 (0.197)	T@1 31.746 (77.489)	T@5 66.667 (91.053)	L 2.8701 (0.9248)
Test on T training set - [6][20/45]	T 0.301 (0.315)	D 0.188 (0.195)	T@1 87.302 (73.847)	T@5 100.000 (88.813)	L 0.4218 (1.0792)
Test on T training set - [6][30/45]	T 0.313 (0.313)	D 0.188 (0.193)	T@1 69.841 (72.094)	T@5 92.063 (88.172)	L 1.1904 (1.1387)
Test on T training set - [6][40/45]	T 0.304 (0.311)	D 0.186 (0.191)	T@1 39.683 (67.364)	T@5 77.778 (84.669)	L 2.0976 (1.3374)
 * Test on T training set - Prec@1 65.495, Prec@5 84.381
Test on T training set - [5][0/45]	T 0.445 (0.445)	D 0.328 (0.328)	T@1 85.714 (85.714)	T@5 96.825 (96.825)	L 0.6255 (0.6255)
Test on T training set - [5][10/45]	T 0.308 (0.316)	D 0.191 (0.198)	T@1 36.508 (78.788)	T@5 68.254 (91.486)	L 2.6777 (0.8908)
Test on T training set - [5][20/45]	T 0.318 (0.313)	D 0.189 (0.193)	T@1 84.127 (74.528)	T@5 100.000 (89.796)	L 0.6524 (1.0524)
Test on T training set - [5][30/45]	T 0.300 (0.309)	D 0.186 (0.190)	T@1 73.016 (72.657)	T@5 95.238 (88.684)	L 1.0308 (1.1115)
Test on T training set - [5][40/45]	T 0.289 (0.308)	D 0.175 (0.188)	T@1 36.508 (68.060)	T@5 77.778 (84.901)	L 2.1943 (1.3127)
 * Test on T training set - Prec@1 65.886, Prec@5 84.487
Test on T test set - [5][0/45]	Time 0.444 (0.444)	Loss 0.6255 (0.6255)	Prec@1 85.714 (85.714)	Prec@5 96.825 (96.825)
Test on T test set - [5][10/45]	Time 0.315 (0.323)	Loss 2.6777 (0.8908)	Prec@1 36.508 (78.788)	Prec@5 68.254 (91.486)
Test on T test set - [5][20/45]	Time 0.291 (0.313)	Loss 0.6524 (1.0524)	Prec@1 84.127 (74.528)	Prec@5 100.000 (89.796)
Test on T test set - [5][30/45]	Time 0.299 (0.312)	Loss 1.0308 (1.1115)	Prec@1 73.016 (72.657)	Prec@5 95.238 (88.684)
Test on T test set - [5][40/45]	Time 0.367 (0.312)	Loss 2.1943 (1.3127)	Prec@1 36.508 (68.060)	Prec@5 77.778 (84.901)
 * Test on T test set - Prec@1 65.886, Prec@5 84.487
Epoch 5, K-means clustering 0, Average clustering time 0.028, Prec@1 72.666
Epoch 5, K-means clustering 1, Average clustering time 0.074, Prec@1 73.944
Epoch 5, K-means clustering 2, Average clustering time 0.074, Prec@1 74.050
Epoch 5, K-means clustering 3, Average clustering time 0.078, Prec@1 73.837
Epoch 5, K-means clustering 4, Average clustering time 0.082, Prec@1 73.695
Epoch 5, K-means clustering 0, Average clustering time 0.009, Prec@1 70.110
Epoch 5, K-means clustering 1, Average clustering time 0.042, Prec@1 70.891
Epoch 5, K-means clustering 2, Average clustering time 0.052, Prec@1 71.175
Epoch 5, K-means clustering 3, Average clustering time 0.057, Prec@1 71.175
Epoch 5, K-means clustering 4, Average clustering time 0.061, Prec@1 70.962
Test on T test set - [6][0/45]	Time 0.472 (0.472)	Loss 0.6922 (0.6922)	Prec@1 84.127 (84.127)	Prec@5 96.825 (96.825)
Test on T test set - [6][10/45]	Time 0.294 (0.324)	Loss 2.8701 (0.9248)	Prec@1 31.746 (77.489)	Prec@5 66.667 (91.053)
Test on T test set - [6][20/45]	Time 0.304 (0.314)	Loss 0.4218 (1.0792)	Prec@1 87.302 (73.847)	Prec@5 100.000 (88.813)
Test on T test set - [6][30/45]	Time 0.305 (0.312)	Loss 1.1904 (1.1387)	Prec@1 69.841 (72.094)	Prec@5 92.063 (88.172)
Test on T test set - [6][40/45]	Time 0.308 (0.311)	Loss 2.0976 (1.3374)	Prec@1 39.683 (67.364)	Prec@5 77.778 (84.669)
 * Test on T test set - Prec@1 65.495, Prec@5 84.381
Epoch 6, K-means clustering 0, Average clustering time 0.475, Prec@1 72.772
Epoch 6, K-means clustering 1, Average clustering time 0.292, Prec@1 74.512
Epoch 6, K-means clustering 2, Average clustering time 0.219, Prec@1 75.009
Epoch 6, K-means clustering 3, Average clustering time 0.180, Prec@1 74.760
Epoch 6, K-means clustering 4, Average clustering time 0.159, Prec@1 74.760
Epoch 6, K-means clustering 0, Average clustering time 0.003, Prec@1 70.927
Epoch 6, K-means clustering 1, Average clustering time 0.038, Prec@1 71.494
Epoch 6, K-means clustering 2, Average clustering time 0.051, Prec@1 71.991
Epoch 6, K-means clustering 3, Average clustering time 0.056, Prec@1 72.098
Epoch 6, K-means clustering 4, Average clustering time 0.087, Prec@1 72.133
The penalty weight is 0.148885
Train - epoch [6/200]	BT 1.307 (1.307)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6264 (2.6264)
The penalty weight is 0.173235
Train - epoch [7/200]	BT 1.170 (1.170)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.9651 (2.9651)
Train - epoch [7/200]	BT 1.759 (1.759)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.0165 (3.0165)
Train - epoch [7/200]	BT 1.331 (1.331)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.8442 (2.8442)
Train - epoch [6/200]	BT 1.283 (1.283)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6363 (2.6363)
Train - epoch [6/200]	BT 1.355 (1.355)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.7074 (2.7074)
Train - epoch [7/200]	BT 1.298 (1.298)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.9017 (2.9017)
Train - epoch [7/200]	BT 1.273 (1.273)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.9332 (2.9332)
Train - epoch [6/200]	BT 1.290 (1.290)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.7133 (2.7133)
Train - epoch [7/200]	BT 1.282 (1.282)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.9115 (2.9115)
Train - epoch [6/200]	BT 1.281 (1.281)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6568 (2.6568)
Train - epoch [7/200]	BT 1.271 (1.271)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.1141 (3.1141)
Train - epoch [6/200]	BT 1.301 (1.301)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.8453 (2.8453)
Test on T training set - [7][0/45]	T 0.453 (0.453)	D 0.329 (0.329)	T@1 85.714 (85.714)	T@5 96.825 (96.825)	L 0.6473 (0.6473)
Test on T training set - [7][10/45]	T 0.304 (0.320)	D 0.188 (0.199)	T@1 34.921 (78.932)	T@5 68.254 (91.342)	L 2.6023 (0.8792)
Test on T training set - [7][20/45]	T 0.301 (0.313)	D 0.179 (0.193)	T@1 87.302 (75.057)	T@5 100.000 (89.872)	L 0.5607 (1.0264)
Test on T training set - [7][30/45]	T 0.301 (0.310)	D 0.188 (0.190)	T@1 74.603 (72.862)	T@5 95.238 (88.684)	L 0.9933 (1.0947)
Test on T training set - [7][40/45]	T 0.299 (0.309)	D 0.182 (0.189)	T@1 41.270 (68.448)	T@5 79.365 (84.901)	L 2.1035 (1.3027)
 * Test on T training set - Prec@1 66.347, Prec@5 84.452
Test on T training set - [6][0/45]	T 0.442 (0.442)	D 0.317 (0.317)	T@1 84.127 (84.127)	T@5 96.825 (96.825)	L 0.6920 (0.6920)
Test on T training set - [6][10/45]	T 0.301 (0.315)	D 0.185 (0.194)	T@1 31.746 (77.489)	T@5 66.667 (90.909)	L 2.8692 (0.9248)
Test on T training set - [6][20/45]	T 0.292 (0.309)	D 0.177 (0.188)	T@1 87.302 (73.772)	T@5 100.000 (88.813)	L 0.4220 (1.0792)
Test on T training set - [6][30/45]	T 0.314 (0.307)	D 0.188 (0.187)	T@1 69.841 (72.094)	T@5 92.063 (88.172)	L 1.1906 (1.1386)
Test on T training set - [6][40/45]	T 0.311 (0.306)	D 0.198 (0.186)	T@1 39.683 (67.364)	T@5 77.778 (84.669)	L 2.0973 (1.3372)
 * Test on T training set - Prec@1 65.495, Prec@5 84.416
Test on T test set - [6][0/45]	Time 0.463 (0.463)	Loss 0.6920 (0.6920)	Prec@1 84.127 (84.127)	Prec@5 96.825 (96.825)
Test on T test set - [6][10/45]	Time 0.338 (0.320)	Loss 2.8692 (0.9248)	Prec@1 31.746 (77.489)	Prec@5 66.667 (90.909)
Test on T test set - [6][20/45]	Time 0.305 (0.314)	Loss 0.4220 (1.0792)	Prec@1 87.302 (73.772)	Prec@5 100.000 (88.813)
Test on T test set - [6][30/45]	Time 0.302 (0.311)	Loss 1.1906 (1.1386)	Prec@1 69.841 (72.094)	Prec@5 92.063 (88.172)
Test on T test set - [6][40/45]	Time 0.304 (0.309)	Loss 2.0973 (1.3372)	Prec@1 39.683 (67.364)	Prec@5 77.778 (84.669)
 * Test on T test set - Prec@1 65.495, Prec@5 84.416
Epoch 6, K-means clustering 0, Average clustering time 0.028, Prec@1 72.808
Epoch 6, K-means clustering 1, Average clustering time 0.072, Prec@1 74.512
Epoch 6, K-means clustering 2, Average clustering time 0.071, Prec@1 75.009
Epoch 6, K-means clustering 3, Average clustering time 0.071, Prec@1 74.760
Epoch 6, K-means clustering 4, Average clustering time 0.071, Prec@1 74.760
Epoch 6, K-means clustering 0, Average clustering time 0.009, Prec@1 70.927
Epoch 6, K-means clustering 1, Average clustering time 0.038, Prec@1 71.601
Epoch 6, K-means clustering 2, Average clustering time 0.049, Prec@1 72.027
Epoch 6, K-means clustering 3, Average clustering time 0.056, Prec@1 72.133
Epoch 6, K-means clustering 4, Average clustering time 0.060, Prec@1 72.169
Test on T test set - [7][0/45]	Time 0.473 (0.473)	Loss 0.6473 (0.6473)	Prec@1 85.714 (85.714)	Prec@5 96.825 (96.825)
Test on T test set - [7][10/45]	Time 0.317 (0.321)	Loss 2.6023 (0.8792)	Prec@1 34.921 (78.932)	Prec@5 68.254 (91.342)
Test on T test set - [7][20/45]	Time 0.307 (0.317)	Loss 0.5607 (1.0264)	Prec@1 87.302 (75.057)	Prec@5 100.000 (89.872)
Test on T test set - [7][30/45]	Time 0.306 (0.314)	Loss 0.9933 (1.0947)	Prec@1 74.603 (72.862)	Prec@5 95.238 (88.684)
Test on T test set - [7][40/45]	Time 0.318 (0.312)	Loss 2.1035 (1.3027)	Prec@1 41.270 (68.448)	Prec@5 79.365 (84.901)
 * Test on T test set - Prec@1 66.347, Prec@5 84.452
Epoch 7, K-means clustering 0, Average clustering time 0.479, Prec@1 73.056
Epoch 7, K-means clustering 1, Average clustering time 0.301, Prec@1 74.512
Epoch 7, K-means clustering 2, Average clustering time 0.228, Prec@1 74.938
Epoch 7, K-means clustering 3, Average clustering time 0.191, Prec@1 75.080
Epoch 7, K-means clustering 4, Average clustering time 0.169, Prec@1 74.902
Epoch 7, K-means clustering 0, Average clustering time 0.003, Prec@1 70.856
Epoch 7, K-means clustering 1, Average clustering time 0.035, Prec@1 71.530
Epoch 7, K-means clustering 2, Average clustering time 0.046, Prec@1 71.956
Epoch 7, K-means clustering 3, Average clustering time 0.051, Prec@1 71.956
Epoch 7, K-means clustering 4, Average clustering time 0.055, Prec@1 71.991
The penalty weight is 0.173235
Train - epoch [7/200]	BT 1.199 (1.199)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.2096 (3.2096)
The penalty weight is 0.197375
Train - epoch [8/200]	BT 1.340 (1.340)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.3557 (3.3557)
Train - epoch [7/200]	BT 1.740 (1.740)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.2618 (3.2618)
Train - epoch [7/200]	BT 1.227 (1.227)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.0884 (3.0884)
Train - epoch [8/200]	BT 1.274 (1.274)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.5439 (3.5439)
Train - epoch [7/200]	BT 1.269 (1.269)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.1430 (3.1430)
Train - epoch [8/200]	BT 1.267 (1.267)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.3993 (3.3993)
Train - epoch [7/200]	BT 1.284 (1.284)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.1791 (3.1791)
Train - epoch [8/200]	BT 1.282 (1.282)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.0796 (3.0796)
Train - epoch [7/200]	BT 1.290 (1.290)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.1541 (3.1541)
Train - epoch [8/200]	BT 1.772 (1.772)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.4004 (3.4004)
Train - epoch [8/200]	BT 1.224 (1.224)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.5487 (3.5487)
Train - epoch [7/200]	BT 1.290 (1.290)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.3558 (3.3558)
Train - epoch [8/200]	BT 1.263 (1.263)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.5708 (3.5708)
Test on T training set - [7][0/45]	T 0.454 (0.454)	D 0.330 (0.330)	T@1 85.714 (85.714)	T@5 96.825 (96.825)	L 0.6472 (0.6472)
Test on T training set - [7][10/45]	T 0.305 (0.320)	D 0.192 (0.201)	T@1 34.921 (78.932)	T@5 68.254 (91.342)	L 2.6010 (0.8794)
Test on T training set - [7][20/45]	T 0.324 (0.325)	D 0.202 (0.205)	T@1 87.302 (75.057)	T@5 100.000 (89.872)	L 0.5610 (1.0266)
Test on T training set - [7][30/45]	T 0.296 (0.318)	D 0.174 (0.198)	T@1 74.603 (72.862)	T@5 95.238 (88.684)	L 0.9935 (1.0946)
Test on T training set - [7][40/45]	T 0.304 (0.316)	D 0.184 (0.196)	T@1 41.270 (68.448)	T@5 79.365 (84.901)	L 2.1032 (1.3026)
 * Test on T training set - Prec@1 66.347, Prec@5 84.452
Test on T training set - [8][0/45]	T 0.460 (0.460)	D 0.337 (0.337)	T@1 85.714 (85.714)	T@5 96.825 (96.825)	L 0.6752 (0.6752)
Test on T training set - [8][10/45]	T 0.307 (0.318)	D 0.186 (0.200)	T@1 34.921 (77.922)	T@5 68.254 (91.342)	L 2.6752 (0.8972)
Test on T training set - [8][20/45]	T 0.303 (0.322)	D 0.182 (0.202)	T@1 84.127 (74.225)	T@5 100.000 (89.872)	L 0.6685 (1.0358)
Test on T training set - [8][30/45]	T 0.296 (0.316)	D 0.179 (0.196)	T@1 74.603 (72.555)	T@5 95.238 (88.479)	L 0.9819 (1.0987)
Test on T training set - [8][40/45]	T 0.310 (0.314)	D 0.186 (0.195)	T@1 41.270 (68.099)	T@5 80.952 (84.592)	L 2.0666 (1.3030)
 * Test on T training set - Prec@1 65.921, Prec@5 84.097
Test on T test set - [7][0/45]	Time 0.444 (0.444)	Loss 0.6472 (0.6472)	Prec@1 85.714 (85.714)	Prec@5 96.825 (96.825)
Test on T test set - [7][10/45]	Time 0.306 (0.318)	Loss 2.6010 (0.8794)	Prec@1 34.921 (78.932)	Prec@5 68.254 (91.342)
Test on T test set - [7][20/45]	Time 0.300 (0.334)	Loss 0.5610 (1.0266)	Prec@1 87.302 (75.057)	Prec@5 100.000 (89.872)
Test on T test set - [7][30/45]	Time 0.297 (0.324)	Loss 0.9935 (1.0946)	Prec@1 74.603 (72.862)	Prec@5 95.238 (88.684)
Test on T test set - [7][40/45]	Time 0.303 (0.321)	Loss 2.1032 (1.3026)	Prec@1 41.270 (68.448)	Prec@5 79.365 (84.901)
 * Test on T test set - Prec@1 66.347, Prec@5 84.452
Epoch 7, K-means clustering 0, Average clustering time 0.028, Prec@1 73.092
Epoch 7, K-means clustering 1, Average clustering time 0.076, Prec@1 74.654
Epoch 7, K-means clustering 2, Average clustering time 0.083, Prec@1 75.115
Epoch 7, K-means clustering 3, Average clustering time 0.083, Prec@1 75.186
Epoch 7, K-means clustering 4, Average clustering time 0.083, Prec@1 75.009
Epoch 7, K-means clustering 0, Average clustering time 0.008, Prec@1 70.856
Epoch 7, K-means clustering 1, Average clustering time 0.036, Prec@1 71.459
Epoch 7, K-means clustering 2, Average clustering time 0.043, Prec@1 71.956
Epoch 7, K-means clustering 3, Average clustering time 0.047, Prec@1 71.814
Epoch 7, K-means clustering 4, Average clustering time 0.049, Prec@1 71.778
Test on T test set - [8][0/45]	Time 0.463 (0.463)	Loss 0.6752 (0.6752)	Prec@1 85.714 (85.714)	Prec@5 96.825 (96.825)
Test on T test set - [8][10/45]	Time 0.301 (0.316)	Loss 2.6752 (0.8972)	Prec@1 34.921 (77.922)	Prec@5 68.254 (91.342)
Test on T test set - [8][20/45]	Time 0.317 (0.333)	Loss 0.6685 (1.0358)	Prec@1 84.127 (74.225)	Prec@5 100.000 (89.872)
Test on T test set - [8][30/45]	Time 0.305 (0.323)	Loss 0.9819 (1.0987)	Prec@1 74.603 (72.555)	Prec@5 95.238 (88.479)
Test on T test set - [8][40/45]	Time 0.300 (0.320)	Loss 2.0666 (1.3030)	Prec@1 41.270 (68.099)	Prec@5 80.952 (84.592)
 * Test on T test set - Prec@1 65.921, Prec@5 84.097
Epoch 8, K-means clustering 0, Average clustering time 0.479, Prec@1 73.873
Epoch 8, K-means clustering 1, Average clustering time 0.300, Prec@1 74.725
Epoch 8, K-means clustering 2, Average clustering time 0.226, Prec@1 74.973
Epoch 8, K-means clustering 3, Average clustering time 0.189, Prec@1 74.654
Epoch 8, K-means clustering 4, Average clustering time 0.166, Prec@1 74.370
Epoch 8, K-means clustering 0, Average clustering time 0.003, Prec@1 70.359
Epoch 8, K-means clustering 1, Average clustering time 0.034, Prec@1 71.388
Epoch 8, K-means clustering 2, Average clustering time 0.045, Prec@1 71.530
Epoch 8, K-means clustering 3, Average clustering time 0.051, Prec@1 71.636
Epoch 8, K-means clustering 4, Average clustering time 0.055, Prec@1 71.778
The penalty weight is 0.197375
Train - epoch [8/200]	BT 1.379 (1.379)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.6318 (3.6318)
The penalty weight is 0.221278
Train - epoch [9/200]	BT 1.268 (1.268)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.8217 (3.8217)
Train - epoch [8/200]	BT 1.278 (1.278)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.8238 (3.8238)
Train - epoch [9/200]	BT 1.275 (1.275)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.8860 (3.8860)
Train - epoch [8/200]	BT 1.295 (1.295)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.6804 (3.6804)
Train - epoch [9/200]	BT 1.302 (1.302)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.5774 (3.5774)
Train - epoch [8/200]	BT 1.254 (1.254)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.3579 (3.3579)
Train - epoch [9/200]	BT 1.259 (1.259)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.8978 (3.8978)
Train - epoch [8/200]	BT 1.758 (1.758)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.6800 (3.6800)
Train - epoch [8/200]	BT 1.248 (1.248)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.8216 (3.8216)
Train - epoch [9/200]	BT 1.279 (1.279)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.8116 (3.8116)
Train - epoch [9/200]	BT 1.292 (1.292)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.5563 (3.5563)
Train - epoch [8/200]	BT 1.298 (1.298)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.8513 (3.8513)
Test on T training set - [9][0/45]	T 0.462 (0.462)	D 0.335 (0.335)	T@1 84.127 (84.127)	T@5 96.825 (96.825)	L 0.7193 (0.7193)
Test on T training set - [9][10/45]	T 0.297 (0.319)	D 0.184 (0.200)	T@1 33.333 (77.922)	T@5 68.254 (91.198)	L 2.7627 (0.9255)
Test on T training set - [9][20/45]	T 0.307 (0.326)	D 0.185 (0.206)	T@1 87.302 (74.376)	T@5 100.000 (89.267)	L 0.4698 (1.0626)
Test on T training set - [9][30/45]	T 0.307 (0.317)	D 0.184 (0.199)	T@1 71.429 (72.657)	T@5 95.238 (88.530)	L 1.1734 (1.1180)
Test on T training set - [9][40/45]	T 0.296 (0.313)	D 0.177 (0.195)	T@1 41.270 (68.138)	T@5 77.778 (84.901)	L 2.1573 (1.3208)
 * Test on T training set - Prec@1 65.921, Prec@5 84.274
Test on T training set - [8][0/45]	T 0.442 (0.442)	D 0.322 (0.322)	T@1 85.714 (85.714)	T@5 96.825 (96.825)	L 0.6752 (0.6752)
Test on T training set - [8][10/45]	T 0.301 (0.317)	D 0.176 (0.197)	T@1 34.921 (77.922)	T@5 68.254 (91.342)	L 2.6732 (0.8975)
Test on T training set - [8][20/45]	T 0.302 (0.323)	D 0.182 (0.202)	T@1 84.127 (74.225)	T@5 100.000 (89.872)	L 0.6690 (1.0361)
Test on T training set - [8][30/45]	T 0.287 (0.315)	D 0.174 (0.195)	T@1 74.603 (72.555)	T@5 95.238 (88.479)	L 0.9821 (1.0987)
Test on T training set - [8][40/45]	T 0.295 (0.311)	D 0.182 (0.191)	T@1 41.270 (68.060)	T@5 80.952 (84.592)	L 2.0663 (1.3031)
 * Test on T training set - Prec@1 65.886, Prec@5 84.097
Test on T test set - [8][0/45]	Time 0.447 (0.447)	Loss 0.6752 (0.6752)	Prec@1 85.714 (85.714)	Prec@5 96.825 (96.825)
Test on T test set - [8][10/45]	Time 0.297 (0.315)	Loss 2.6732 (0.8975)	Prec@1 34.921 (77.922)	Prec@5 68.254 (91.342)
Test on T test set - [8][20/45]	Time 0.300 (0.309)	Loss 0.6690 (1.0361)	Prec@1 84.127 (74.225)	Prec@5 100.000 (89.872)
Test on T test set - [8][30/45]	Time 0.296 (0.306)	Loss 0.9821 (1.0987)	Prec@1 74.603 (72.555)	Prec@5 95.238 (88.479)
Test on T test set - [8][40/45]	Time 0.303 (0.305)	Loss 2.0663 (1.3031)	Prec@1 41.270 (68.060)	Prec@5 80.952 (84.592)
 * Test on T test set - Prec@1 65.886, Prec@5 84.097
Epoch 8, K-means clustering 0, Average clustering time 0.028, Prec@1 73.731
Epoch 8, K-means clustering 1, Average clustering time 0.073, Prec@1 74.405
Epoch 8, K-means clustering 2, Average clustering time 0.072, Prec@1 74.831
Epoch 8, K-means clustering 3, Average clustering time 0.077, Prec@1 74.441
Epoch 8, K-means clustering 4, Average clustering time 0.080, Prec@1 74.263
Epoch 8, K-means clustering 0, Average clustering time 0.008, Prec@1 70.430
Epoch 8, K-means clustering 1, Average clustering time 0.043, Prec@1 71.388
Epoch 8, K-means clustering 2, Average clustering time 0.052, Prec@1 71.601
Epoch 8, K-means clustering 3, Average clustering time 0.057, Prec@1 71.565
Epoch 8, K-means clustering 4, Average clustering time 0.060, Prec@1 71.707
Test on T test set - [9][0/45]	Time 0.451 (0.451)	Loss 0.7193 (0.7193)	Prec@1 84.127 (84.127)	Prec@5 96.825 (96.825)
Test on T test set - [9][10/45]	Time 0.296 (0.315)	Loss 2.7627 (0.9255)	Prec@1 33.333 (77.922)	Prec@5 68.254 (91.198)
Test on T test set - [9][20/45]	Time 0.308 (0.309)	Loss 0.4698 (1.0626)	Prec@1 87.302 (74.376)	Prec@5 100.000 (89.267)
Test on T test set - [9][30/45]	Time 0.298 (0.306)	Loss 1.1734 (1.1180)	Prec@1 71.429 (72.657)	Prec@5 95.238 (88.530)
Test on T test set - [9][40/45]	Time 0.298 (0.305)	Loss 2.1573 (1.3208)	Prec@1 41.270 (68.138)	Prec@5 77.778 (84.901)
 * Test on T test set - Prec@1 65.921, Prec@5 84.274
Epoch 9, K-means clustering 0, Average clustering time 0.478, Prec@1 72.808
Epoch 9, K-means clustering 1, Average clustering time 0.295, Prec@1 74.370
Epoch 9, K-means clustering 2, Average clustering time 0.223, Prec@1 74.370
Epoch 9, K-means clustering 3, Average clustering time 0.184, Prec@1 74.192
Epoch 9, K-means clustering 4, Average clustering time 0.162, Prec@1 73.731
Epoch 9, K-means clustering 0, Average clustering time 0.003, Prec@1 70.572
Epoch 9, K-means clustering 1, Average clustering time 0.039, Prec@1 70.962
Epoch 9, K-means clustering 2, Average clustering time 0.051, Prec@1 71.707
Epoch 9, K-means clustering 3, Average clustering time 0.056, Prec@1 71.743
Epoch 9, K-means clustering 4, Average clustering time 0.059, Prec@1 71.778
The penalty weight is 0.221278
Train - epoch [9/200]	BT 1.270 (1.270)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 4.1310 (4.1310)
Train - epoch [10/200]	BT 1.348 (1.348)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.8661 (3.8661)
The penalty weight is 0.244919
Train - epoch [10/200]	BT 1.225 (1.225)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.8897 (3.8897)
Train - epoch [9/200]	BT 1.302 (1.302)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 4.1950 (4.1950)
Train - epoch [10/200]	BT 1.278 (1.278)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 4.1068 (4.1068)
Train - epoch [9/200]	BT 1.288 (1.288)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.8842 (3.8842)
Train - epoch [10/200]	BT 1.369 (1.369)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.9062 (3.9062)
Train - epoch [10/200]	BT 1.300 (1.300)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 4.0008 (4.0008)
Train - epoch [9/200]	BT 1.276 (1.276)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 4.2057 (4.2057)
Train - epoch [10/200]	BT 1.328 (1.328)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 4.2072 (4.2072)
Train - epoch [9/200]	BT 1.885 (1.885)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 4.1248 (4.1248)
Train - epoch [10/200]	BT 1.528 (1.528)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 4.3685 (4.3685)
Train - epoch [9/200]	BT 1.285 (1.285)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.8682 (3.8682)
Test on T training set - [10][0/45]	T 0.460 (0.460)	D 0.334 (0.334)	T@1 84.127 (84.127)	T@5 96.825 (96.825)	L 0.6841 (0.6841)
Test on T training set - [10][10/45]	T 0.303 (0.317)	D 0.181 (0.197)	T@1 31.746 (78.355)	T@5 66.667 (91.631)	L 2.7580 (0.9024)
Test on T training set - [10][20/45]	T 0.293 (0.311)	D 0.171 (0.190)	T@1 84.127 (74.225)	T@5 100.000 (89.494)	L 0.5289 (1.0561)
Test on T training set - [10][30/45]	T 0.309 (0.308)	D 0.192 (0.189)	T@1 76.190 (72.504)	T@5 93.651 (88.582)	L 1.0787 (1.1101)
Test on T training set - [10][40/45]	T 0.304 (0.308)	D 0.183 (0.189)	T@1 41.270 (68.138)	T@5 76.190 (84.708)	L 2.1015 (1.3142)
 * Test on T training set - Prec@1 65.992, Prec@5 84.274
Test on T training set - [9][0/45]	T 0.443 (0.443)	D 0.327 (0.327)	T@1 84.127 (84.127)	T@5 96.825 (96.825)	L 0.7190 (0.7190)
Test on T training set - [9][10/45]	T 0.297 (0.314)	D 0.181 (0.197)	T@1 33.333 (77.922)	T@5 68.254 (91.198)	L 2.7608 (0.9257)
Test on T training set - [9][20/45]	T 0.301 (0.309)	D 0.187 (0.192)	T@1 87.302 (74.376)	T@5 100.000 (89.267)	L 0.4704 (1.0628)
Test on T training set - [9][30/45]	T 0.294 (0.307)	D 0.181 (0.189)	T@1 71.429 (72.657)	T@5 95.238 (88.530)	L 1.1730 (1.1180)
Test on T training set - [9][40/45]	T 0.291 (0.307)	D 0.178 (0.188)	T@1 41.270 (68.138)	T@5 77.778 (84.863)	L 2.1569 (1.3207)
 * Test on T training set - Prec@1 65.921, Prec@5 84.239
Test on T test set - [10][0/45]	Time 0.455 (0.455)	Loss 0.6841 (0.6841)	Prec@1 84.127 (84.127)	Prec@5 96.825 (96.825)
Test on T test set - [10][10/45]	Time 0.305 (0.320)	Loss 2.7580 (0.9024)	Prec@1 31.746 (78.355)	Prec@5 66.667 (91.631)
Test on T test set - [10][20/45]	Time 0.575 (0.326)	Loss 0.5289 (1.0561)	Prec@1 84.127 (74.225)	Prec@5 100.000 (89.494)
Test on T test set - [10][30/45]	Time 0.384 (0.321)	Loss 1.0787 (1.1101)	Prec@1 76.190 (72.504)	Prec@5 93.651 (88.582)
Test on T test set - [10][40/45]	Time 0.298 (0.318)	Loss 2.1015 (1.3142)	Prec@1 41.270 (68.138)	Prec@5 76.190 (84.708)
 * Test on T test set - Prec@1 65.992, Prec@5 84.274
Epoch 10, K-means clustering 0, Average clustering time 0.478, Prec@1 72.737
Epoch 10, K-means clustering 1, Average clustering time 0.295, Prec@1 74.263
Epoch 10, K-means clustering 2, Average clustering time 0.222, Prec@1 74.618
Epoch 10, K-means clustering 3, Average clustering time 0.183, Prec@1 74.263
Epoch 10, K-means clustering 4, Average clustering time 0.160, Prec@1 74.299
Epoch 10, K-means clustering 0, Average clustering time 0.003, Prec@1 70.927
Epoch 10, K-means clustering 1, Average clustering time 0.032, Prec@1 72.098
Epoch 10, K-means clustering 2, Average clustering time 0.043, Prec@1 72.240
Epoch 10, K-means clustering 3, Average clustering time 0.050, Prec@1 72.311
Epoch 10, K-means clustering 4, Average clustering time 0.054, Prec@1 72.453
Test on T test set - [9][0/45]	Time 0.475 (0.475)	Loss 0.7190 (0.7190)	Prec@1 84.127 (84.127)	Prec@5 96.825 (96.825)
Test on T test set - [9][10/45]	Time 0.299 (0.321)	Loss 2.7608 (0.9257)	Prec@1 33.333 (77.922)	Prec@5 68.254 (91.198)
Test on T test set - [9][20/45]	Time 0.297 (0.327)	Loss 0.4704 (1.0628)	Prec@1 87.302 (74.376)	Prec@5 100.000 (89.267)
Test on T test set - [9][30/45]	Time 0.335 (0.326)	Loss 1.1730 (1.1180)	Prec@1 71.429 (72.657)	Prec@5 95.238 (88.530)
Test on T test set - [9][40/45]	Time 0.291 (0.323)	Loss 2.1569 (1.3207)	Prec@1 41.270 (68.138)	Prec@5 77.778 (84.863)
 * Test on T test set - Prec@1 65.921, Prec@5 84.239
Epoch 9, K-means clustering 0, Average clustering time 0.028, Prec@1 72.737
Epoch 9, K-means clustering 1, Average clustering time 0.081, Prec@1 74.370
Epoch 9, K-means clustering 2, Average clustering time 0.083, Prec@1 74.405
Epoch 9, K-means clustering 3, Average clustering time 0.086, Prec@1 74.299
Epoch 9, K-means clustering 4, Average clustering time 0.087, Prec@1 73.837
Epoch 9, K-means clustering 0, Average clustering time 0.008, Prec@1 70.607
Epoch 9, K-means clustering 1, Average clustering time 0.043, Prec@1 71.033
Epoch 9, K-means clustering 2, Average clustering time 0.054, Prec@1 71.672
Epoch 9, K-means clustering 3, Average clustering time 0.060, Prec@1 71.778
Epoch 9, K-means clustering 4, Average clustering time 0.063, Prec@1 71.707
The penalty weight is 0.268271
Train - epoch [11/200]	BT 1.266 (1.266)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 4.6259 (4.6259)
Train - epoch [10/200]	BT 1.400 (1.400)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 4.2083 (4.2083)
The penalty weight is 0.244919
Train - epoch [10/200]	BT 1.254 (1.254)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 4.2380 (4.2380)
Train - epoch [11/200]	BT 1.272 (1.272)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 4.6795 (4.6795)
Train - epoch [10/200]	BT 1.290 (1.290)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 4.4464 (4.4464)
Train - epoch [11/200]	BT 1.261 (1.261)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 4.7608 (4.7608)
Train - epoch [10/200]	BT 1.310 (1.310)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 4.2455 (4.2455)
Train - epoch [11/200]	BT 1.744 (1.744)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 4.5052 (4.5052)
Train - epoch [11/200]	BT 1.230 (1.230)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 4.6715 (4.6715)
Train - epoch [10/200]	BT 1.286 (1.286)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 4.3454 (4.3454)
Train - epoch [11/200]	BT 1.254 (1.254)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 4.8410 (4.8410)
Train - epoch [10/200]	BT 1.439 (1.439)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 4.5547 (4.5547)
Train - epoch [11/200]	BT 1.304 (1.304)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 4.5843 (4.5843)
Train - epoch [10/200]	BT 1.732 (1.732)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 4.7090 (4.7090)
Test on T training set - [11][0/45]	T 0.458 (0.458)	D 0.337 (0.337)	T@1 85.714 (85.714)	T@5 96.825 (96.825)	L 0.7045 (0.7045)
Test on T training set - [11][10/45]	T 0.313 (0.318)	D 0.191 (0.197)	T@1 34.921 (77.922)	T@5 68.254 (91.486)	L 2.6688 (0.9134)
Test on T training set - [11][20/45]	T 0.306 (0.312)	D 0.183 (0.191)	T@1 85.714 (73.923)	T@5 100.000 (89.342)	L 0.5523 (1.0805)
Test on T training set - [11][30/45]	T 0.318 (0.321)	D 0.196 (0.200)	T@1 73.016 (72.555)	T@5 95.238 (88.786)	L 1.1130 (1.1227)
Test on T training set - [11][40/45]	T 0.295 (0.323)	D 0.183 (0.202)	T@1 39.683 (67.983)	T@5 77.778 (84.979)	L 2.1198 (1.3231)
 * Test on T training set - Prec@1 65.992, Prec@5 84.665
Test on T training set - [10][0/45]	T 0.444 (0.444)	D 0.320 (0.320)	T@1 84.127 (84.127)	T@5 96.825 (96.825)	L 0.6839 (0.6839)
Test on T training set - [10][10/45]	T 0.298 (0.318)	D 0.173 (0.197)	T@1 31.746 (78.355)	T@5 66.667 (91.775)	L 2.7562 (0.9027)
Test on T training set - [10][20/45]	T 0.583 (0.324)	D 0.460 (0.203)	T@1 84.127 (74.225)	T@5 100.000 (89.569)	L 0.5299 (1.0565)
Test on T training set - [10][30/45]	T 0.311 (0.318)	D 0.189 (0.198)	T@1 76.190 (72.504)	T@5 93.651 (88.633)	L 1.0778 (1.1100)
Test on T training set - [10][40/45]	T 0.304 (0.319)	D 0.191 (0.199)	T@1 41.270 (68.138)	T@5 76.190 (84.746)	L 2.1014 (1.3141)
 * Test on T training set - Prec@1 66.028, Prec@5 84.310
Test on T test set - [11][0/45]	Time 0.693 (0.693)	Loss 0.7045 (0.7045)	Prec@1 85.714 (85.714)	Prec@5 96.825 (96.825)
Test on T test set - [11][10/45]	Time 0.305 (0.345)	Loss 2.6688 (0.9134)	Prec@1 34.921 (77.922)	Prec@5 68.254 (91.486)
Test on T test set - [11][20/45]	Time 0.313 (0.346)	Loss 0.5523 (1.0805)	Prec@1 85.714 (73.923)	Prec@5 100.000 (89.342)
Test on T test set - [11][30/45]	Time 0.301 (0.342)	Loss 1.1130 (1.1227)	Prec@1 73.016 (72.555)	Prec@5 95.238 (88.786)
Test on T test set - [11][40/45]	Time 0.301 (0.334)	Loss 2.1198 (1.3231)	Prec@1 39.683 (67.983)	Prec@5 77.778 (84.979)
 * Test on T test set - Prec@1 65.992, Prec@5 84.665
Epoch 11, K-means clustering 0, Average clustering time 0.478, Prec@1 72.914
Epoch 11, K-means clustering 1, Average clustering time 0.287, Prec@1 74.831
Epoch 11, K-means clustering 2, Average clustering time 0.214, Prec@1 74.760
Epoch 11, K-means clustering 3, Average clustering time 0.177, Prec@1 74.547
Epoch 11, K-means clustering 4, Average clustering time 0.155, Prec@1 74.405
Epoch 11, K-means clustering 0, Average clustering time 0.003, Prec@1 70.749
Epoch 11, K-means clustering 1, Average clustering time 0.029, Prec@1 71.601
Epoch 11, K-means clustering 2, Average clustering time 0.039, Prec@1 71.743
Epoch 11, K-means clustering 3, Average clustering time 0.044, Prec@1 71.814
Epoch 11, K-means clustering 4, Average clustering time 0.046, Prec@1 71.707
Test on T test set - [10][0/45]	Time 0.450 (0.450)	Loss 0.6839 (0.6839)	Prec@1 84.127 (84.127)	Prec@5 96.825 (96.825)
Test on T test set - [10][10/45]	Time 0.304 (0.328)	Loss 2.7562 (0.9027)	Prec@1 31.746 (78.355)	Prec@5 66.667 (91.775)
Test on T test set - [10][20/45]	Time 0.301 (0.339)	Loss 0.5299 (1.0565)	Prec@1 84.127 (74.225)	Prec@5 100.000 (89.569)
Test on T test set - [10][30/45]	Time 0.300 (0.329)	Loss 1.0778 (1.1100)	Prec@1 76.190 (72.504)	Prec@5 93.651 (88.633)
Test on T test set - [10][40/45]	Time 0.306 (0.325)	Loss 2.1014 (1.3141)	Prec@1 41.270 (68.138)	Prec@5 76.190 (84.746)
 * Test on T test set - Prec@1 66.028, Prec@5 84.310
Epoch 10, K-means clustering 0, Average clustering time 0.028, Prec@1 72.772
Epoch 10, K-means clustering 1, Average clustering time 0.072, Prec@1 74.228
Epoch 10, K-means clustering 2, Average clustering time 0.077, Prec@1 74.583
Epoch 10, K-means clustering 3, Average clustering time 0.079, Prec@1 74.263
Epoch 10, K-means clustering 4, Average clustering time 0.080, Prec@1 74.334
Epoch 10, K-means clustering 0, Average clustering time 0.008, Prec@1 70.927
Epoch 10, K-means clustering 1, Average clustering time 0.042, Prec@1 72.133
Epoch 10, K-means clustering 2, Average clustering time 0.073, Prec@1 72.098
Epoch 10, K-means clustering 3, Average clustering time 0.072, Prec@1 72.133
Epoch 10, K-means clustering 4, Average clustering time 0.072, Prec@1 72.417
The penalty weight is 0.291313
Train - epoch [12/200]	BT 1.312 (1.312)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 5.1475 (5.1475)
The penalty weight is 0.268271
Train - epoch [11/200]	BT 1.320 (1.320)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 4.9964 (4.9964)
Train - epoch [12/200]	BT 1.285 (1.285)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 5.0886 (5.0886)
Train - epoch [11/200]	BT 1.281 (1.281)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 5.0582 (5.0582)
Train - epoch [12/200]	BT 1.278 (1.278)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 4.9992 (4.9992)
Train - epoch [11/200]	BT 1.191 (1.191)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 5.1360 (5.1360)
Train - epoch [12/200]	BT 1.283 (1.283)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 5.2457 (5.2457)
Train - epoch [11/200]	BT 1.778 (1.778)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 4.8800 (4.8800)
Train - epoch [11/200]	BT 1.238 (1.238)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 5.0461 (5.0461)
Train - epoch [12/200]	BT 1.293 (1.293)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 5.1298 (5.1298)
Train - epoch [11/200]	BT 1.300 (1.300)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 5.2147 (5.2147)
Train - epoch [12/200]	BT 1.260 (1.260)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 5.0774 (5.0774)
Train - epoch [11/200]	BT 1.287 (1.287)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 4.9562 (4.9562)
Train - epoch [12/200]	BT 1.688 (1.688)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 4.6272 (4.6272)
Test on T training set - [12][0/45]	T 0.480 (0.480)	D 0.354 (0.354)	T@1 85.714 (85.714)	T@5 96.825 (96.825)	L 0.6636 (0.6636)
Test on T training set - [12][10/45]	T 0.309 (0.319)	D 0.187 (0.197)	T@1 33.333 (77.922)	T@5 68.254 (91.053)	L 2.6861 (0.9098)
Test on T training set - [12][20/45]	T 0.299 (0.312)	D 0.179 (0.191)	T@1 82.540 (74.150)	T@5 100.000 (89.872)	L 0.6225 (1.0432)
Test on T training set - [12][30/45]	T 0.305 (0.311)	D 0.183 (0.190)	T@1 71.429 (72.657)	T@5 93.651 (89.094)	L 1.0827 (1.0922)
Test on T training set - [12][40/45]	T 0.315 (0.311)	D 0.194 (0.191)	T@1 42.857 (68.293)	T@5 79.365 (85.405)	L 2.0250 (1.2960)
 * Test on T training set - Prec@1 65.992, Prec@5 84.878
Test on T training set - [11][0/45]	T 0.439 (0.439)	D 0.325 (0.325)	T@1 85.714 (85.714)	T@5 96.825 (96.825)	L 0.7044 (0.7044)
Test on T training set - [11][10/45]	T 0.305 (0.314)	D 0.188 (0.198)	T@1 34.921 (77.922)	T@5 68.254 (91.486)	L 2.6662 (0.9137)
Test on T training set - [11][20/45]	T 0.318 (0.311)	D 0.193 (0.193)	T@1 85.714 (73.923)	T@5 100.000 (89.342)	L 0.5536 (1.0809)
Test on T training set - [11][30/45]	T 0.307 (0.310)	D 0.194 (0.192)	T@1 74.603 (72.657)	T@5 95.238 (88.735)	L 1.1113 (1.1227)
Test on T training set - [11][40/45]	T 0.302 (0.309)	D 0.187 (0.191)	T@1 39.683 (68.060)	T@5 77.778 (84.940)	L 2.1193 (1.3231)
 * Test on T training set - Prec@1 66.063, Prec@5 84.629
Test on T test set - [12][0/45]	Time 0.455 (0.455)	Loss 0.6636 (0.6636)	Prec@1 85.714 (85.714)	Prec@5 96.825 (96.825)
Test on T test set - [12][10/45]	Time 0.311 (0.324)	Loss 2.6861 (0.9098)	Prec@1 33.333 (77.922)	Prec@5 68.254 (91.053)
Test on T test set - [12][20/45]	Time 0.303 (0.314)	Loss 0.6225 (1.0432)	Prec@1 82.540 (74.150)	Prec@5 100.000 (89.872)
Test on T test set - [12][30/45]	Time 0.297 (0.311)	Loss 1.0827 (1.0922)	Prec@1 71.429 (72.657)	Prec@5 93.651 (89.094)
Test on T test set - [12][40/45]	Time 0.317 (0.319)	Loss 2.0250 (1.2960)	Prec@1 42.857 (68.293)	Prec@5 79.365 (85.405)
 * Test on T test set - Prec@1 65.992, Prec@5 84.878
Epoch 12, K-means clustering 0, Average clustering time 0.478, Prec@1 73.056
Epoch 12, K-means clustering 1, Average clustering time 0.295, Prec@1 74.334
Epoch 12, K-means clustering 2, Average clustering time 0.220, Prec@1 74.867
Epoch 12, K-means clustering 3, Average clustering time 0.186, Prec@1 74.583
Epoch 12, K-means clustering 4, Average clustering time 0.163, Prec@1 74.512
Epoch 12, K-means clustering 0, Average clustering time 0.003, Prec@1 70.572
Epoch 12, K-means clustering 1, Average clustering time 0.036, Prec@1 71.211
Epoch 12, K-means clustering 2, Average clustering time 0.043, Prec@1 71.424
Epoch 12, K-means clustering 3, Average clustering time 0.047, Prec@1 71.565
Epoch 12, K-means clustering 4, Average clustering time 0.049, Prec@1 71.424
Test on T test set - [11][0/45]	Time 0.443 (0.443)	Loss 0.7044 (0.7044)	Prec@1 85.714 (85.714)	Prec@5 96.825 (96.825)
Test on T test set - [11][10/45]	Time 0.321 (0.317)	Loss 2.6662 (0.9137)	Prec@1 34.921 (77.922)	Prec@5 68.254 (91.486)
Test on T test set - [11][20/45]	Time 0.296 (0.310)	Loss 0.5536 (1.0809)	Prec@1 85.714 (73.923)	Prec@5 100.000 (89.342)
Test on T test set - [11][30/45]	Time 0.384 (0.319)	Loss 1.1113 (1.1227)	Prec@1 74.603 (72.657)	Prec@5 95.238 (88.735)
Test on T test set - [11][40/45]	Time 0.299 (0.316)	Loss 2.1193 (1.3231)	Prec@1 39.683 (68.060)	Prec@5 77.778 (84.940)
 * Test on T test set - Prec@1 66.063, Prec@5 84.629
Epoch 11, K-means clustering 0, Average clustering time 0.028, Prec@1 72.950
Epoch 11, K-means clustering 1, Average clustering time 0.081, Prec@1 74.867
Epoch 11, K-means clustering 2, Average clustering time 0.085, Prec@1 74.725
Epoch 11, K-means clustering 3, Average clustering time 0.086, Prec@1 74.689
Epoch 11, K-means clustering 4, Average clustering time 0.087, Prec@1 74.583
Epoch 11, K-means clustering 0, Average clustering time 0.007, Prec@1 70.820
Epoch 11, K-means clustering 1, Average clustering time 0.041, Prec@1 71.743
Epoch 11, K-means clustering 2, Average clustering time 0.052, Prec@1 71.672
Epoch 11, K-means clustering 3, Average clustering time 0.058, Prec@1 71.778
Epoch 11, K-means clustering 4, Average clustering time 0.061, Prec@1 71.601
The penalty weight is 0.314021
Train - epoch [13/200]	BT 1.284 (1.284)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 5.0538 (5.0538)
The penalty weight is 0.291313
Train - epoch [12/200]	BT 1.265 (1.265)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 5.5570 (5.5570)
Train - epoch [13/200]	BT 1.288 (1.288)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 5.3705 (5.3705)
Train - epoch [12/200]	BT 1.280 (1.280)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 5.4832 (5.4832)
Train - epoch [13/200]	BT 1.256 (1.256)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 5.5245 (5.5245)
Train - epoch [12/200]	BT 1.315 (1.315)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 5.4005 (5.4005)
Train - epoch [13/200]	BT 1.253 (1.253)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 5.4959 (5.4959)
Train - epoch [12/200]	BT 1.288 (1.288)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 5.6524 (5.6524)
