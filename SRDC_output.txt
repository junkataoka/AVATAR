/data/home/jkataok1/alexnet_resnet_finetune/checkpoints/amazon_to_webcam_resnet50.pkl
Source pre-trained model has been loaded!
begin training
Test on T training set - [0][0/25]	T 0.372 (0.372)	D 0.303 (0.303)	T@1 93.750 (93.750)	T@5 96.875 (96.875)	L 0.5553 (0.5553)
Test on T training set - [0][1/25]	T 0.283 (0.327)	D 0.222 (0.262)	T@1 100.000 (96.875)	T@5 100.000 (98.438)	L 0.0867 (0.3210)
Test on T training set - [0][2/25]	T 0.252 (0.302)	D 0.188 (0.238)	T@1 75.000 (89.583)	T@5 100.000 (98.958)	L 0.9956 (0.5459)
Test on T training set - [0][3/25]	T 0.196 (0.276)	D 0.129 (0.210)	T@1 53.125 (80.469)	T@5 96.875 (98.438)	L 1.3240 (0.7404)
Test on T training set - [0][4/25]	T 0.266 (0.274)	D 0.202 (0.209)	T@1 81.250 (80.625)	T@5 96.875 (98.125)	L 0.5429 (0.7009)
Test on T training set - [0][5/25]	T 0.299 (0.278)	D 0.233 (0.213)	T@1 96.875 (83.333)	T@5 96.875 (97.917)	L 0.3213 (0.6376)
Test on T training set - [0][6/25]	T 0.301 (0.281)	D 0.236 (0.216)	T@1 84.375 (83.482)	T@5 96.875 (97.768)	L 1.0157 (0.6916)
Test on T training set - [0][7/25]	T 0.257 (0.278)	D 0.187 (0.212)	T@1 90.625 (84.375)	T@5 100.000 (98.047)	L 0.3269 (0.6461)
Test on T training set - [0][8/25]	T 0.308 (0.282)	D 0.245 (0.216)	T@1 96.875 (85.764)	T@5 100.000 (98.264)	L 0.2634 (0.6035)
Test on T training set - [0][9/25]	T 3.261 (0.579)	D 3.187 (0.513)	T@1 75.000 (84.688)	T@5 93.750 (97.812)	L 0.8599 (0.6292)
Test on T training set - [0][10/25]	T 0.345 (0.558)	D 0.274 (0.491)	T@1 34.375 (80.114)	T@5 71.875 (95.455)	L 2.2566 (0.7771)
Test on T training set - [0][11/25]	T 0.255 (0.533)	D 0.185 (0.466)	T@1 28.125 (75.781)	T@5 59.375 (92.448)	L 2.7232 (0.9393)
Test on T training set - [0][12/25]	T 0.270 (0.513)	D 0.207 (0.446)	T@1 50.000 (73.798)	T@5 84.375 (91.827)	L 1.8504 (1.0094)
Test on T training set - [0][13/25]	T 0.190 (0.490)	D 0.116 (0.422)	T@1 93.750 (75.223)	T@5 100.000 (92.411)	L 0.2294 (0.9537)
Test on T training set - [0][14/25]	T 0.214 (0.471)	D 0.145 (0.404)	T@1 84.375 (75.833)	T@5 100.000 (92.917)	L 0.5210 (0.9248)
Test on T training set - [0][15/25]	T 0.236 (0.456)	D 0.170 (0.389)	T@1 78.125 (75.977)	T@5 96.875 (93.164)	L 0.7038 (0.9110)
Test on T training set - [0][16/25]	T 0.239 (0.444)	D 0.167 (0.376)	T@1 96.875 (77.206)	T@5 96.875 (93.382)	L 0.2926 (0.8746)
Test on T training set - [0][17/25]	T 0.300 (0.436)	D 0.237 (0.368)	T@1 78.125 (77.257)	T@5 100.000 (93.750)	L 0.6362 (0.8614)
Test on T training set - [0][18/25]	T 0.264 (0.427)	D 0.198 (0.360)	T@1 78.125 (77.303)	T@5 96.875 (93.914)	L 1.0120 (0.8693)
Test on T training set - [0][19/25]	T 0.295 (0.420)	D 0.218 (0.352)	T@1 87.500 (77.812)	T@5 100.000 (94.219)	L 0.5108 (0.8514)
Test on T training set - [0][20/25]	T 0.300 (0.414)	D 0.227 (0.346)	T@1 75.000 (77.679)	T@5 100.000 (94.494)	L 0.8697 (0.8523)
Test on T training set - [0][21/25]	T 0.217 (0.405)	D 0.147 (0.337)	T@1 96.875 (78.551)	T@5 100.000 (94.744)	L 0.1960 (0.8224)
Test on T training set - [0][22/25]	T 0.247 (0.399)	D 0.184 (0.331)	T@1 62.500 (77.853)	T@5 96.875 (94.837)	L 1.2624 (0.8416)
Test on T training set - [0][23/25]	T 0.208 (0.391)	D 0.133 (0.323)	T@1 84.375 (78.125)	T@5 100.000 (95.052)	L 0.3646 (0.8217)
Test on T training set - [0][24/25]	T 0.210 (0.383)	D 0.153 (0.316)	T@1 74.074 (77.987)	T@5 92.593 (94.969)	L 1.0778 (0.8304)
 * Test on T training set - Prec@1 77.987, Prec@5 94.969
Test on T test set - [0][0/25]	Time 0.362 (0.362)	Loss 0.4898 (0.4898)	Prec@1 87.500 (87.500)	Prec@5 100.000 (100.000)
Test on T test set - [0][1/25]	Time 0.278 (0.320)	Loss 0.0736 (0.2817)	Prec@1 100.000 (93.750)	Prec@5 100.000 (100.000)
Test on T test set - [0][2/25]	Time 0.246 (0.295)	Loss 0.9770 (0.5135)	Prec@1 75.000 (87.500)	Prec@5 100.000 (100.000)
Test on T test set - [0][3/25]	Time 0.186 (0.268)	Loss 1.2895 (0.7075)	Prec@1 56.250 (79.688)	Prec@5 100.000 (100.000)
Test on T test set - [0][4/25]	Time 0.262 (0.267)	Loss 0.5276 (0.6715)	Prec@1 84.375 (80.625)	Prec@5 96.875 (99.375)
Test on T test set - [0][5/25]	Time 0.292 (0.271)	Loss 0.3240 (0.6136)	Prec@1 96.875 (83.333)	Prec@5 100.000 (99.479)
Test on T test set - [0][6/25]	Time 0.294 (0.274)	Loss 0.9280 (0.6585)	Prec@1 81.250 (83.036)	Prec@5 96.875 (99.107)
Test on T test set - [0][7/25]	Time 0.235 (0.269)	Loss 0.2781 (0.6110)	Prec@1 93.750 (84.375)	Prec@5 100.000 (99.219)
Test on T test set - [0][8/25]	Time 0.301 (0.273)	Loss 0.2216 (0.5677)	Prec@1 96.875 (85.764)	Prec@5 100.000 (99.306)
Test on T test set - [0][9/25]	Time 0.303 (0.276)	Loss 0.7815 (0.5891)	Prec@1 87.500 (85.938)	Prec@5 100.000 (99.375)
Test on T test set - [0][10/25]	Time 0.246 (0.273)	Loss 2.2079 (0.7362)	Prec@1 40.625 (81.818)	Prec@5 65.625 (96.307)
Test on T test set - [0][11/25]	Time 0.257 (0.272)	Loss 2.7659 (0.9054)	Prec@1 21.875 (76.823)	Prec@5 65.625 (93.750)
Test on T test set - [0][12/25]	Time 0.265 (0.271)	Loss 1.8671 (0.9794)	Prec@1 46.875 (74.519)	Prec@5 81.250 (92.788)
Test on T test set - [0][13/25]	Time 0.176 (0.265)	Loss 0.2161 (0.9248)	Prec@1 100.000 (76.339)	Prec@5 100.000 (93.304)
Test on T test set - [0][14/25]	Time 0.206 (0.261)	Loss 0.5393 (0.8991)	Prec@1 87.500 (77.083)	Prec@5 96.875 (93.542)
Test on T test set - [0][15/25]	Time 0.226 (0.258)	Loss 0.6783 (0.8853)	Prec@1 87.500 (77.734)	Prec@5 96.875 (93.750)
Test on T test set - [0][16/25]	Time 0.235 (0.257)	Loss 0.2861 (0.8501)	Prec@1 96.875 (78.860)	Prec@5 96.875 (93.934)
Test on T test set - [0][17/25]	Time 0.290 (0.259)	Loss 0.7001 (0.8418)	Prec@1 71.875 (78.472)	Prec@5 96.875 (94.097)
Test on T test set - [0][18/25]	Time 0.252 (0.259)	Loss 1.0491 (0.8527)	Prec@1 75.000 (78.289)	Prec@5 93.750 (94.079)
Test on T test set - [0][19/25]	Time 0.276 (0.259)	Loss 0.4700 (0.8335)	Prec@1 84.375 (78.594)	Prec@5 100.000 (94.375)
Test on T test set - [0][20/25]	Time 0.290 (0.261)	Loss 0.8185 (0.8328)	Prec@1 75.000 (78.423)	Prec@5 96.875 (94.494)
Test on T test set - [0][21/25]	Time 0.218 (0.259)	Loss 0.2251 (0.8052)	Prec@1 96.875 (79.261)	Prec@5 100.000 (94.744)
Test on T test set - [0][22/25]	Time 0.244 (0.258)	Loss 1.1627 (0.8207)	Prec@1 71.875 (78.940)	Prec@5 96.875 (94.837)
Test on T test set - [0][23/25]	Time 0.196 (0.256)	Loss 0.4164 (0.8039)	Prec@1 87.500 (79.297)	Prec@5 100.000 (95.052)
Test on T test set - [0][24/25]	Time 0.229 (0.255)	Loss 1.2110 (0.8177)	Prec@1 70.370 (78.994)	Prec@5 96.296 (95.094)
 * Test on T test set - Prec@1 78.994, Prec@5 95.094
Epoch 0 - Kernel K-means clustering 0: Clustering time 0.007, Prec@1 87.547
Epoch 0 - Kernel K-means clustering 1: Clustering time 0.007, Prec@1 89.434
Epoch 0 - Kernel K-means clustering 2: Clustering time 0.007, Prec@1 90.063
Epoch 0 - Kernel K-means clustering 3: Clustering time 0.007, Prec@1 90.566
Epoch 0 - Kernel K-means clustering 4: Clustering time 0.007, Prec@1 90.818
Epoch 0 - Kernel K-means clustering 5: Clustering time 0.007, Prec@1 91.069
Epoch 0 - Kernel K-means clustering 6: Clustering time 0.007, Prec@1 91.195
Epoch 0 - Kernel K-means clustering 7: Clustering time 0.007, Prec@1 91.195
Converged at iteration 8
Epoch 0 - Kernel K-means clustering 0: Clustering time 0.007, Prec@1 85.031
Epoch 0 - Kernel K-means clustering 1: Clustering time 0.007, Prec@1 86.918
Epoch 0 - Kernel K-means clustering 2: Clustering time 0.007, Prec@1 87.421
Epoch 0 - Kernel K-means clustering 3: Clustering time 0.007, Prec@1 87.296
Epoch 0 - Kernel K-means clustering 4: Clustering time 0.007, Prec@1 87.421
Epoch 0 - Kernel K-means clustering 5: Clustering time 0.007, Prec@1 87.547
Epoch 0 - Kernel K-means clustering 6: Clustering time 0.007, Prec@1 87.547
Converged at iteration 7
Train - epoch [0/200]	BT 1.191 (1.191)	DT 0.000 (0.000)	S@1 90.625 (90.625)	Loss 0.3998 (0.3998)
Train - epoch [0/200]	BT 0.781 (0.781)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 0.2239 (0.2239)
Train - epoch [0/200]	BT 0.806 (0.806)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.0577 (0.0577)
Train - epoch [0/200]	BT 0.803 (0.803)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.1656 (0.1656)
Train - epoch [0/200]	BT 0.798 (0.798)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 0.3514 (0.3514)
Train - epoch [0/200]	BT 0.799 (0.799)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 0.2612 (0.2612)
Train - epoch [0/200]	BT 0.811 (0.811)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 0.2485 (0.2485)
Train - epoch [0/200]	BT 0.786 (0.786)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.1747 (0.1747)
Train - epoch [0/200]	BT 0.785 (0.785)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.1296 (0.1296)
Train - epoch [0/200]	BT 0.808 (0.808)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 0.2349 (0.2349)
Train - epoch [0/200]	BT 0.759 (0.759)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 0.1783 (0.1783)
Train - epoch [0/200]	BT 0.798 (0.798)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 0.2358 (0.2358)
Train - epoch [0/200]	BT 0.767 (0.767)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.1242 (0.1242)
Train - epoch [0/200]	BT 0.791 (0.791)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 0.2949 (0.2949)
Train - epoch [0/200]	BT 0.787 (0.787)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.1553 (0.1553)
Train - epoch [0/200]	BT 0.790 (0.790)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.1475 (0.1475)
Train - epoch [0/200]	BT 0.778 (0.778)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 0.1756 (0.1756)
Train - epoch [0/200]	BT 0.784 (0.784)	DT 0.000 (0.000)	S@1 90.625 (90.625)	Loss 0.2168 (0.2168)
Train - epoch [0/200]	BT 0.799 (0.799)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 0.2422 (0.2422)
Train - epoch [0/200]	BT 0.790 (0.790)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 0.3337 (0.3337)
Train - epoch [0/200]	BT 0.795 (0.795)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 0.3731 (0.3731)
Train - epoch [0/200]	BT 0.778 (0.778)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 0.2530 (0.2530)
Train - epoch [0/200]	BT 0.755 (0.755)	DT 0.000 (0.000)	S@1 90.625 (90.625)	Loss 0.2997 (0.2997)
Train - epoch [0/200]	BT 0.750 (0.750)	DT 0.000 (0.000)	S@1 90.625 (90.625)	Loss 0.1835 (0.1835)
Train - epoch [0/200]	BT 1.009 (1.009)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 0.2177 (0.2177)
Train - epoch [0/200]	BT 0.806 (0.806)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 0.2543 (0.2543)
Train - epoch [0/200]	BT 0.794 (0.794)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 0.3479 (0.3479)
Train - epoch [0/200]	BT 0.802 (0.802)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 0.1527 (0.1527)
Train - epoch [0/200]	BT 0.796 (0.796)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 0.1939 (0.1939)
Train - epoch [0/200]	BT 0.793 (0.793)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 0.2979 (0.2979)
Train - epoch [0/200]	BT 0.794 (0.794)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.1088 (0.1088)
Train - epoch [0/200]	BT 0.781 (0.781)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 0.2929 (0.2929)
Train - epoch [0/200]	BT 0.792 (0.792)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.1831 (0.1831)
Train - epoch [0/200]	BT 0.804 (0.804)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.1650 (0.1650)
Train - epoch [0/200]	BT 0.797 (0.797)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 0.1569 (0.1569)
Train - epoch [0/200]	BT 0.737 (0.737)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 0.3031 (0.3031)
Train - epoch [0/200]	BT 0.794 (0.794)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 0.2666 (0.2666)
Train - epoch [0/200]	BT 0.787 (0.787)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.1804 (0.1804)
Train - epoch [0/200]	BT 0.804 (0.804)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 0.2728 (0.2728)
Train - epoch [0/200]	BT 0.806 (0.806)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 0.1881 (0.1881)
Train - epoch [0/200]	BT 0.792 (0.792)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 0.2161 (0.2161)
Train - epoch [0/200]	BT 0.800 (0.800)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.0714 (0.0714)
Train - epoch [0/200]	BT 0.789 (0.789)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 0.2116 (0.2116)
Train - epoch [0/200]	BT 0.801 (0.801)	DT 0.000 (0.000)	S@1 90.625 (90.625)	Loss 0.2962 (0.2962)
Train - epoch [0/200]	BT 0.809 (0.809)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 0.2402 (0.2402)
Train - epoch [0/200]	BT 0.788 (0.788)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.1346 (0.1346)
Train - epoch [0/200]	BT 0.758 (0.758)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.1313 (0.1313)
Train - epoch [0/200]	BT 0.762 (0.762)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 0.1141 (0.1141)
Train - epoch [0/200]	BT 1.051 (1.051)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.1483 (0.1483)
Train - epoch [0/200]	BT 0.860 (0.860)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 0.1512 (0.1512)
Train - epoch [0/200]	BT 0.770 (0.770)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 0.2540 (0.2540)
Train - epoch [0/200]	BT 0.803 (0.803)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 0.1997 (0.1997)
Train - epoch [0/200]	BT 0.789 (0.789)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 0.2694 (0.2694)
Train - epoch [0/200]	BT 0.794 (0.794)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.1304 (0.1304)
Train - epoch [0/200]	BT 0.775 (0.775)	DT 0.000 (0.000)	S@1 90.625 (90.625)	Loss 0.2511 (0.2511)
Train - epoch [0/200]	BT 0.778 (0.778)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.1141 (0.1141)
Train - epoch [0/200]	BT 0.740 (0.740)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 0.2558 (0.2558)
Train - epoch [0/200]	BT 0.809 (0.809)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 0.1891 (0.1891)
Train - epoch [0/200]	BT 0.779 (0.779)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 0.2206 (0.2206)
Train - epoch [0/200]	BT 0.797 (0.797)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 0.1110 (0.1110)
Train - epoch [0/200]	BT 0.772 (0.772)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 0.1492 (0.1492)
Train - epoch [0/200]	BT 0.774 (0.774)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 0.2611 (0.2611)
Train - epoch [0/200]	BT 0.770 (0.770)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.1045 (0.1045)
Train - epoch [0/200]	BT 0.800 (0.800)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 0.1717 (0.1717)
Train - epoch [0/200]	BT 0.806 (0.806)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 0.3863 (0.3863)
Train - epoch [0/200]	BT 0.795 (0.795)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.1583 (0.1583)
Train - epoch [0/200]	BT 0.809 (0.809)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 0.1848 (0.1848)
Train - epoch [0/200]	BT 0.798 (0.798)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.1240 (0.1240)
Train - epoch [0/200]	BT 0.795 (0.795)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 0.2034 (0.2034)
Train - epoch [0/200]	BT 0.802 (0.802)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.1364 (0.1364)
Train - epoch [0/200]	BT 0.763 (0.763)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 0.1839 (0.1839)
Train - epoch [0/200]	BT 0.769 (0.769)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 0.1597 (0.1597)
Train - epoch [0/200]	BT 1.020 (1.020)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 0.2242 (0.2242)
Train - epoch [0/200]	BT 0.793 (0.793)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.0968 (0.0968)
Train - epoch [0/200]	BT 0.775 (0.775)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 0.2505 (0.2505)
Train - epoch [0/200]	BT 0.789 (0.789)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 0.2980 (0.2980)
Train - epoch [0/200]	BT 0.789 (0.789)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.1269 (0.1269)
Train - epoch [0/200]	BT 0.727 (0.727)	DT 0.000 (0.000)	S@1 90.625 (90.625)	Loss 0.2899 (0.2899)
Train - epoch [0/200]	BT 0.770 (0.770)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.0586 (0.0586)
Train - epoch [0/200]	BT 0.779 (0.779)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 0.2000 (0.2000)
Train - epoch [0/200]	BT 0.789 (0.789)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 0.2337 (0.2337)
Train - epoch [0/200]	BT 0.782 (0.782)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 0.2039 (0.2039)
Train - epoch [0/200]	BT 0.797 (0.797)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.0712 (0.0712)
Train - epoch [0/200]	BT 0.794 (0.794)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.1356 (0.1356)
Train - epoch [0/200]	BT 0.800 (0.800)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 0.1525 (0.1525)
Train - epoch [0/200]	BT 0.789 (0.789)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 0.2852 (0.2852)
Train - epoch [0/200]	BT 0.779 (0.779)	DT 0.000 (0.000)	S@1 90.625 (90.625)	Loss 0.3193 (0.3193)
Train - epoch [0/200]	BT 0.755 (0.755)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.0959 (0.0959)
Test on T training set - [0][0/25]	T 0.374 (0.374)	D 0.305 (0.305)	T@1 100.000 (100.000)	T@5 100.000 (100.000)	L 0.2856 (0.2856)
Test on T training set - [0][1/25]	T 0.278 (0.326)	D 0.218 (0.262)	T@1 100.000 (100.000)	T@5 100.000 (100.000)	L 0.0498 (0.1677)
Test on T training set - [0][2/25]	T 0.252 (0.301)	D 0.187 (0.237)	T@1 81.250 (93.750)	T@5 100.000 (100.000)	L 0.6217 (0.3191)
Test on T training set - [0][3/25]	T 0.186 (0.273)	D 0.126 (0.209)	T@1 68.750 (87.500)	T@5 100.000 (100.000)	L 0.9967 (0.4885)
Test on T training set - [0][4/25]	T 0.277 (0.273)	D 0.205 (0.208)	T@1 81.250 (86.250)	T@5 96.875 (99.375)	L 0.3994 (0.4706)
Test on T training set - [0][5/25]	T 0.293 (0.277)	D 0.227 (0.211)	T@1 90.625 (86.979)	T@5 93.750 (98.438)	L 0.4922 (0.4742)
Test on T training set - [0][6/25]	T 0.301 (0.280)	D 0.237 (0.215)	T@1 78.125 (85.714)	T@5 96.875 (98.214)	L 0.7653 (0.5158)
Test on T training set - [0][7/25]	T 0.238 (0.275)	D 0.178 (0.210)	T@1 96.875 (87.109)	T@5 100.000 (98.438)	L 0.2427 (0.4817)
Test on T training set - [0][8/25]	T 0.311 (0.279)	D 0.248 (0.215)	T@1 96.875 (88.194)	T@5 100.000 (98.611)	L 0.1807 (0.4482)
Test on T training set - [0][9/25]	T 0.309 (0.282)	D 0.240 (0.217)	T@1 71.875 (86.562)	T@5 100.000 (98.750)	L 0.8878 (0.4922)
Test on T training set - [0][10/25]	T 0.248 (0.279)	D 0.185 (0.214)	T@1 37.500 (82.102)	T@5 81.250 (97.159)	L 1.8564 (0.6162)
Test on T training set - [0][11/25]	T 0.245 (0.276)	D 0.181 (0.211)	T@1 31.250 (77.865)	T@5 71.875 (95.052)	L 2.4070 (0.7654)
Test on T training set - [0][12/25]	T 0.275 (0.276)	D 0.212 (0.211)	T@1 50.000 (75.721)	T@5 87.500 (94.471)	L 1.5122 (0.8229)
Test on T training set - [0][13/25]	T 0.181 (0.269)	D 0.116 (0.205)	T@1 87.500 (76.562)	T@5 100.000 (94.866)	L 0.3594 (0.7898)
Test on T training set - [0][14/25]	T 0.214 (0.266)	D 0.147 (0.201)	T@1 87.500 (77.292)	T@5 93.750 (94.792)	L 0.5534 (0.7740)
Test on T training set - [0][15/25]	T 0.234 (0.264)	D 0.169 (0.199)	T@1 93.750 (78.320)	T@5 96.875 (94.922)	L 0.3774 (0.7492)
Test on T training set - [0][16/25]	T 0.235 (0.262)	D 0.163 (0.197)	T@1 96.875 (79.412)	T@5 100.000 (95.221)	L 0.1987 (0.7168)
Test on T training set - [0][17/25]	T 0.301 (0.264)	D 0.238 (0.199)	T@1 81.250 (79.514)	T@5 96.875 (95.312)	L 0.5836 (0.7094)
Test on T training set - [0][18/25]	T 0.263 (0.264)	D 0.197 (0.199)	T@1 81.250 (79.605)	T@5 100.000 (95.559)	L 0.7647 (0.7124)
Test on T training set - [0][19/25]	T 0.279 (0.265)	D 0.217 (0.200)	T@1 71.875 (79.219)	T@5 100.000 (95.781)	L 0.7458 (0.7140)
Test on T training set - [0][20/25]	T 0.302 (0.267)	D 0.230 (0.201)	T@1 59.375 (78.274)	T@5 100.000 (95.982)	L 1.0058 (0.7279)
Test on T training set - [0][21/25]	T 0.214 (0.264)	D 0.146 (0.199)	T@1 96.875 (79.119)	T@5 100.000 (96.165)	L 0.1240 (0.7005)
Test on T training set - [0][22/25]	T 0.237 (0.263)	D 0.175 (0.198)	T@1 75.000 (78.940)	T@5 93.750 (96.060)	L 1.1005 (0.7179)
Test on T training set - [0][23/25]	T 0.198 (0.260)	D 0.134 (0.195)	T@1 84.375 (79.167)	T@5 100.000 (96.224)	L 0.3841 (0.7040)
Test on T training set - [0][24/25]	T 0.216 (0.258)	D 0.159 (0.194)	T@1 62.963 (78.616)	T@5 96.296 (96.226)	L 1.0114 (0.7144)
 * Test on T training set - Prec@1 78.616, Prec@5 96.226
Test on T test set - [0][0/25]	Time 0.376 (0.376)	Loss 0.2467 (0.2467)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Test on T test set - [0][1/25]	Time 0.288 (0.332)	Loss 0.0579 (0.1523)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Test on T test set - [0][2/25]	Time 0.240 (0.301)	Loss 0.5958 (0.3002)	Prec@1 84.375 (94.792)	Prec@5 100.000 (100.000)
Test on T test set - [0][3/25]	Time 0.188 (0.273)	Loss 1.0505 (0.4877)	Prec@1 59.375 (85.938)	Prec@5 100.000 (100.000)
Test on T test set - [0][4/25]	Time 0.273 (0.273)	Loss 0.4021 (0.4706)	Prec@1 84.375 (85.625)	Prec@5 100.000 (100.000)
Test on T test set - [0][5/25]	Time 0.283 (0.274)	Loss 0.4549 (0.4680)	Prec@1 93.750 (86.979)	Prec@5 93.750 (98.958)
Test on T test set - [0][6/25]	Time 0.293 (0.277)	Loss 0.7352 (0.5062)	Prec@1 81.250 (86.161)	Prec@5 100.000 (99.107)
Test on T test set - [0][7/25]	Time 0.236 (0.272)	Loss 0.2544 (0.4747)	Prec@1 96.875 (87.500)	Prec@5 100.000 (99.219)
Test on T test set - [0][8/25]	Time 0.320 (0.277)	Loss 0.1436 (0.4379)	Prec@1 96.875 (88.542)	Prec@5 100.000 (99.306)
Test on T test set - [0][9/25]	Time 0.293 (0.279)	Loss 0.8333 (0.4774)	Prec@1 78.125 (87.500)	Prec@5 100.000 (99.375)
Test on T test set - [0][10/25]	Time 0.244 (0.276)	Loss 1.8853 (0.6054)	Prec@1 40.625 (83.239)	Prec@5 81.250 (97.727)
Test on T test set - [0][11/25]	Time 0.242 (0.273)	Loss 2.4061 (0.7555)	Prec@1 34.375 (79.167)	Prec@5 71.875 (95.573)
Test on T test set - [0][12/25]	Time 0.269 (0.273)	Loss 1.5408 (0.8159)	Prec@1 56.250 (77.404)	Prec@5 87.500 (94.952)
Test on T test set - [0][13/25]	Time 0.174 (0.266)	Loss 0.2975 (0.7789)	Prec@1 93.750 (78.571)	Prec@5 100.000 (95.312)
Test on T test set - [0][14/25]	Time 0.213 (0.262)	Loss 0.5726 (0.7651)	Prec@1 84.375 (78.958)	Prec@5 93.750 (95.208)
Test on T test set - [0][15/25]	Time 0.236 (0.260)	Loss 0.4028 (0.7425)	Prec@1 93.750 (79.883)	Prec@5 93.750 (95.117)
Test on T test set - [0][16/25]	Time 0.233 (0.259)	Loss 0.2053 (0.7109)	Prec@1 96.875 (80.882)	Prec@5 100.000 (95.404)
Test on T test set - [0][17/25]	Time 0.292 (0.261)	Loss 0.5835 (0.7038)	Prec@1 81.250 (80.903)	Prec@5 96.875 (95.486)
Test on T test set - [0][18/25]	Time 0.255 (0.260)	Loss 0.8080 (0.7093)	Prec@1 84.375 (81.086)	Prec@5 93.750 (95.395)
Test on T test set - [0][19/25]	Time 0.275 (0.261)	Loss 0.6175 (0.7047)	Prec@1 81.250 (81.094)	Prec@5 100.000 (95.625)
Test on T test set - [0][20/25]	Time 0.290 (0.262)	Loss 1.0342 (0.7204)	Prec@1 59.375 (80.060)	Prec@5 100.000 (95.833)
Test on T test set - [0][21/25]	Time 0.214 (0.260)	Loss 0.1185 (0.6930)	Prec@1 100.000 (80.966)	Prec@5 100.000 (96.023)
Test on T test set - [0][22/25]	Time 0.236 (0.259)	Loss 1.0646 (0.7092)	Prec@1 71.875 (80.571)	Prec@5 93.750 (95.924)
Test on T test set - [0][23/25]	Time 0.192 (0.256)	Loss 0.4215 (0.6972)	Prec@1 93.750 (81.120)	Prec@5 100.000 (96.094)
Test on T test set - [0][24/25]	Time 0.213 (0.255)	Loss 0.9911 (0.7072)	Prec@1 66.667 (80.629)	Prec@5 96.296 (96.101)
 * Test on T test set - Prec@1 80.629, Prec@5 96.101
Epoch 0 - Kernel K-means clustering 0: Clustering time 0.006, Prec@1 87.673
Epoch 0 - Kernel K-means clustering 1: Clustering time 0.007, Prec@1 88.931
Epoch 0 - Kernel K-means clustering 2: Clustering time 0.007, Prec@1 89.182
Epoch 0 - Kernel K-means clustering 3: Clustering time 0.006, Prec@1 89.560
Epoch 0 - Kernel K-means clustering 4: Clustering time 0.006, Prec@1 89.560
Epoch 0 - Kernel K-means clustering 5: Clustering time 0.007, Prec@1 89.560
Converged at iteration 6
Epoch 0 - Kernel K-means clustering 0: Clustering time 0.006, Prec@1 84.025
Epoch 0 - Kernel K-means clustering 1: Clustering time 0.007, Prec@1 85.031
Epoch 0 - Kernel K-means clustering 2: Clustering time 0.007, Prec@1 86.164
Epoch 0 - Kernel K-means clustering 3: Clustering time 0.007, Prec@1 86.918
Epoch 0 - Kernel K-means clustering 4: Clustering time 0.007, Prec@1 87.296
Epoch 0 - Kernel K-means clustering 5: Clustering time 0.006, Prec@1 87.547
Epoch 0 - Kernel K-means clustering 6: Clustering time 0.007, Prec@1 87.421
Epoch 0 - Kernel K-means clustering 7: Clustering time 0.006, Prec@1 87.421
Converged at iteration 8
Train - epoch [1/200]	BT 1.037 (1.037)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.4558 (1.4558)
Train - epoch [1/200]	BT 0.722 (0.722)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.3959 (1.3959)
Train - epoch [1/200]	BT 0.766 (0.766)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 1.5227 (1.5227)
Train - epoch [1/200]	BT 0.796 (0.796)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 1.4774 (1.4774)
Train - epoch [1/200]	BT 0.784 (0.784)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.4568 (1.4568)
Train - epoch [1/200]	BT 0.792 (0.792)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.4161 (1.4161)
Train - epoch [1/200]	BT 0.813 (0.813)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 1.5630 (1.5630)
Train - epoch [1/200]	BT 0.784 (0.784)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.4858 (1.4858)
Train - epoch [1/200]	BT 0.814 (0.814)	DT 0.000 (0.000)	S@1 90.625 (90.625)	Loss 1.6756 (1.6756)
Train - epoch [1/200]	BT 0.803 (0.803)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.4402 (1.4402)
Train - epoch [1/200]	BT 0.814 (0.814)	DT 0.000 (0.000)	S@1 90.625 (90.625)	Loss 1.5989 (1.5989)
Train - epoch [1/200]	BT 0.811 (0.811)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.4301 (1.4301)
Train - epoch [1/200]	BT 0.790 (0.790)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.4696 (1.4696)
Train - epoch [1/200]	BT 0.799 (0.799)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.4609 (1.4609)
Train - epoch [1/200]	BT 0.801 (0.801)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.4816 (1.4816)
Train - epoch [1/200]	BT 0.810 (0.810)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.4878 (1.4878)
Train - epoch [1/200]	BT 0.809 (0.809)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.5130 (1.5130)
Train - epoch [1/200]	BT 0.793 (0.793)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.4883 (1.4883)
Train - epoch [1/200]	BT 0.776 (0.776)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.5303 (1.5303)
Train - epoch [1/200]	BT 0.786 (0.786)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.5992 (1.5992)
Train - epoch [1/200]	BT 0.787 (0.787)	DT 0.000 (0.000)	S@1 90.625 (90.625)	Loss 1.6108 (1.6108)
Train - epoch [1/200]	BT 0.787 (0.787)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 1.5835 (1.5835)
Train - epoch [1/200]	BT 0.755 (0.755)	DT 0.000 (0.000)	S@1 90.625 (90.625)	Loss 1.5088 (1.5088)
Train - epoch [1/200]	BT 0.752 (0.752)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.4274 (1.4274)
Train - epoch [1/200]	BT 0.978 (0.978)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.4390 (1.4390)
Train - epoch [1/200]	BT 0.787 (0.787)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.4197 (1.4197)
Train - epoch [1/200]	BT 0.753 (0.753)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.4911 (1.4911)
Train - epoch [1/200]	BT 0.749 (0.749)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.4363 (1.4363)
Train - epoch [1/200]	BT 0.795 (0.795)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.4037 (1.4037)
Train - epoch [1/200]	BT 0.790 (0.790)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.4230 (1.4230)
Train - epoch [1/200]	BT 0.802 (0.802)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.5209 (1.5209)
Train - epoch [1/200]	BT 0.780 (0.780)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.4523 (1.4523)
Train - epoch [1/200]	BT 0.765 (0.765)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.4375 (1.4375)
Train - epoch [1/200]	BT 0.765 (0.765)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.4756 (1.4756)
Train - epoch [1/200]	BT 0.784 (0.784)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.5465 (1.5465)
Train - epoch [1/200]	BT 0.775 (0.775)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.4588 (1.4588)
Train - epoch [1/200]	BT 0.793 (0.793)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 1.5201 (1.5201)
Train - epoch [1/200]	BT 0.778 (0.778)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.4857 (1.4857)
Train - epoch [1/200]	BT 0.769 (0.769)	DT 0.000 (0.000)	S@1 90.625 (90.625)	Loss 1.5443 (1.5443)
Train - epoch [1/200]	BT 0.773 (0.773)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.4554 (1.4554)
Train - epoch [1/200]	BT 0.793 (0.793)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.4354 (1.4354)
Train - epoch [1/200]	BT 0.778 (0.778)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.4628 (1.4628)
Train - epoch [1/200]	BT 0.789 (0.789)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.4700 (1.4700)
Train - epoch [1/200]	BT 0.789 (0.789)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.4273 (1.4273)
Train - epoch [1/200]	BT 0.779 (0.779)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 1.5448 (1.5448)
Train - epoch [1/200]	BT 0.790 (0.790)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.5332 (1.5332)
Train - epoch [1/200]	BT 0.756 (0.756)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.3922 (1.3922)
Train - epoch [1/200]	BT 0.751 (0.751)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.4443 (1.4443)
Train - epoch [1/200]	BT 0.990 (0.990)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.4431 (1.4431)
Train - epoch [1/200]	BT 0.843 (0.843)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.4067 (1.4067)
Train - epoch [1/200]	BT 0.779 (0.779)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.4724 (1.4724)
Train - epoch [1/200]	BT 0.774 (0.774)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.4697 (1.4697)
Train - epoch [1/200]	BT 0.719 (0.719)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 1.5370 (1.5370)
Train - epoch [1/200]	BT 0.778 (0.778)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.4714 (1.4714)
Train - epoch [1/200]	BT 0.783 (0.783)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.4390 (1.4390)
Train - epoch [1/200]	BT 0.779 (0.779)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.5134 (1.5134)
Train - epoch [1/200]	BT 0.789 (0.789)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.4222 (1.4222)
Train - epoch [1/200]	BT 0.802 (0.802)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.4334 (1.4334)
Train - epoch [1/200]	BT 0.788 (0.788)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.4683 (1.4683)
Train - epoch [1/200]	BT 0.793 (0.793)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.4765 (1.4765)
Train - epoch [1/200]	BT 0.796 (0.796)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.5178 (1.5178)
Train - epoch [1/200]	BT 0.788 (0.788)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 1.5505 (1.5505)
Train - epoch [1/200]	BT 0.798 (0.798)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.3921 (1.3921)
Train - epoch [1/200]	BT 0.780 (0.780)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.4661 (1.4661)
Train - epoch [1/200]	BT 0.784 (0.784)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.5151 (1.5151)
Train - epoch [1/200]	BT 0.793 (0.793)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 1.5837 (1.5837)
Train - epoch [1/200]	BT 0.799 (0.799)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.4601 (1.4601)
Train - epoch [1/200]	BT 0.794 (0.794)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.4303 (1.4303)
Train - epoch [1/200]	BT 0.786 (0.786)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.5205 (1.5205)
Train - epoch [1/200]	BT 0.703 (0.703)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.4270 (1.4270)
Train - epoch [1/200]	BT 0.748 (0.748)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.4573 (1.4573)
Train - epoch [1/200]	BT 0.759 (0.759)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 1.4996 (1.4996)
Train - epoch [1/200]	BT 1.001 (1.001)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 1.5043 (1.5043)
Train - epoch [1/200]	BT 0.829 (0.829)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.4012 (1.4012)
Train - epoch [1/200]	BT 0.799 (0.799)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.5055 (1.5055)
Train - epoch [1/200]	BT 0.793 (0.793)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.3951 (1.3951)
Train - epoch [1/200]	BT 0.808 (0.808)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.4996 (1.4996)
Train - epoch [1/200]	BT 0.753 (0.753)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.4482 (1.4482)
Train - epoch [1/200]	BT 0.771 (0.771)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.4913 (1.4913)
Train - epoch [1/200]	BT 0.805 (0.805)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.4986 (1.4986)
Train - epoch [1/200]	BT 0.792 (0.792)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.4302 (1.4302)
Train - epoch [1/200]	BT 0.806 (0.806)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.4549 (1.4549)
Train - epoch [1/200]	BT 0.768 (0.768)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.5310 (1.5310)
Train - epoch [1/200]	BT 0.773 (0.773)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.4995 (1.4995)
Train - epoch [1/200]	BT 0.799 (0.799)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.4347 (1.4347)
Train - epoch [1/200]	BT 0.782 (0.782)	DT 0.000 (0.000)	S@1 84.375 (84.375)	Loss 1.5902 (1.5902)
Train - epoch [1/200]	BT 0.777 (0.777)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 1.4695 (1.4695)
Train - epoch [1/200]	BT 0.750 (0.750)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 1.4087 (1.4087)
Test on T training set - [1][0/25]	T 0.364 (0.364)	D 0.298 (0.298)	T@1 93.750 (93.750)	T@5 100.000 (100.000)	L 0.4113 (0.4113)
Test on T training set - [1][1/25]	T 0.275 (0.319)	D 0.214 (0.256)	T@1 100.000 (96.875)	T@5 100.000 (100.000)	L 0.0371 (0.2242)
Test on T training set - [1][2/25]	T 0.244 (0.294)	D 0.184 (0.232)	T@1 93.750 (95.833)	T@5 100.000 (100.000)	L 0.4681 (0.3055)
Test on T training set - [1][3/25]	T 0.198 (0.270)	D 0.127 (0.206)	T@1 59.375 (86.719)	T@5 100.000 (100.000)	L 1.0873 (0.5009)
Test on T training set - [1][4/25]	T 0.254 (0.267)	D 0.191 (0.203)	T@1 84.375 (86.250)	T@5 100.000 (100.000)	L 0.3614 (0.4730)
Test on T training set - [1][5/25]	T 0.286 (0.270)	D 0.226 (0.207)	T@1 93.750 (87.500)	T@5 100.000 (100.000)	L 0.3555 (0.4534)
Test on T training set - [1][6/25]	T 0.294 (0.273)	D 0.227 (0.210)	T@1 68.750 (84.821)	T@5 100.000 (100.000)	L 0.9645 (0.5265)
Test on T training set - [1][7/25]	T 0.227 (0.268)	D 0.167 (0.204)	T@1 100.000 (86.719)	T@5 100.000 (100.000)	L 0.1170 (0.4753)
Test on T training set - [1][8/25]	T 0.304 (0.272)	D 0.243 (0.209)	T@1 96.875 (87.847)	T@5 100.000 (100.000)	L 0.1208 (0.4359)
Test on T training set - [1][9/25]	T 0.305 (0.275)	D 0.236 (0.211)	T@1 71.875 (86.250)	T@5 96.875 (99.688)	L 0.9826 (0.4906)
Test on T training set - [1][10/25]	T 0.245 (0.272)	D 0.183 (0.209)	T@1 46.875 (82.670)	T@5 81.250 (98.011)	L 1.6978 (0.6003)
Test on T training set - [1][11/25]	T 0.242 (0.270)	D 0.177 (0.206)	T@1 37.500 (78.906)	T@5 75.000 (96.094)	L 2.1907 (0.7328)
Test on T training set - [1][12/25]	T 0.272 (0.270)	D 0.205 (0.206)	T@1 56.250 (77.163)	T@5 87.500 (95.433)	L 1.5698 (0.7972)
Test on T training set - [1][13/25]	T 0.168 (0.263)	D 0.108 (0.199)	T@1 93.750 (78.348)	T@5 100.000 (95.759)	L 0.3315 (0.7640)
Test on T training set - [1][14/25]	T 0.218 (0.260)	D 0.150 (0.196)	T@1 87.500 (78.958)	T@5 93.750 (95.625)	L 0.4429 (0.7426)
Test on T training set - [1][15/25]	T 0.227 (0.258)	D 0.167 (0.194)	T@1 93.750 (79.883)	T@5 100.000 (95.898)	L 0.3537 (0.7182)
Test on T training set - [1][16/25]	T 0.225 (0.256)	D 0.166 (0.192)	T@1 96.875 (80.882)	T@5 96.875 (95.956)	L 0.1944 (0.6874)
Test on T training set - [1][17/25]	T 0.312 (0.259)	D 0.244 (0.195)	T@1 75.000 (80.556)	T@5 90.625 (95.660)	L 0.7604 (0.6915)
Test on T training set - [1][18/25]	T 0.259 (0.259)	D 0.191 (0.195)	T@1 65.625 (79.770)	T@5 93.750 (95.559)	L 1.0396 (0.7098)
Test on T training set - [1][19/25]	T 0.281 (0.260)	D 0.218 (0.196)	T@1 84.375 (80.000)	T@5 100.000 (95.781)	L 0.5008 (0.6994)
Test on T training set - [1][20/25]	T 0.292 (0.262)	D 0.230 (0.198)	T@1 68.750 (79.464)	T@5 100.000 (95.982)	L 0.9834 (0.7129)
Test on T training set - [1][21/25]	T 0.212 (0.259)	D 0.152 (0.196)	T@1 96.875 (80.256)	T@5 100.000 (96.165)	L 0.1250 (0.6862)
Test on T training set - [1][22/25]	T 0.243 (0.259)	D 0.183 (0.195)	T@1 81.250 (80.299)	T@5 100.000 (96.332)	L 0.8958 (0.6953)
Test on T training set - [1][23/25]	T 0.207 (0.256)	D 0.131 (0.192)	T@1 90.625 (80.729)	T@5 100.000 (96.484)	L 0.3160 (0.6795)
Test on T training set - [1][24/25]	T 0.205 (0.254)	D 0.151 (0.191)	T@1 55.556 (79.874)	T@5 100.000 (96.604)	L 1.0274 (0.6913)
 * Test on T training set - Prec@1 79.874, Prec@5 96.604
Test on T test set - [1][0/25]	Time 0.367 (0.367)	Loss 0.3225 (0.3225)	Prec@1 93.750 (93.750)	Prec@5 100.000 (100.000)
Test on T test set - [1][1/25]	Time 0.284 (0.325)	Loss 0.0467 (0.1846)	Prec@1 100.000 (96.875)	Prec@5 100.000 (100.000)
Test on T test set - [1][2/25]	Time 0.238 (0.296)	Loss 0.4783 (0.2825)	Prec@1 90.625 (94.792)	Prec@5 100.000 (100.000)
Test on T test set - [1][3/25]	Time 0.184 (0.268)	Loss 1.1729 (0.5051)	Prec@1 56.250 (85.156)	Prec@5 100.000 (100.000)
Test on T test set - [1][4/25]	Time 0.263 (0.267)	Loss 0.3434 (0.4728)	Prec@1 87.500 (85.625)	Prec@5 100.000 (100.000)
Test on T test set - [1][5/25]	Time 0.294 (0.272)	Loss 0.3151 (0.4465)	Prec@1 93.750 (86.979)	Prec@5 96.875 (99.479)
Test on T test set - [1][6/25]	Time 0.294 (0.275)	Loss 0.9059 (0.5121)	Prec@1 71.875 (84.821)	Prec@5 100.000 (99.554)
Test on T test set - [1][7/25]	Time 0.242 (0.271)	Loss 0.1522 (0.4671)	Prec@1 100.000 (86.719)	Prec@5 100.000 (99.609)
Test on T test set - [1][8/25]	Time 0.305 (0.275)	Loss 0.1087 (0.4273)	Prec@1 100.000 (88.194)	Prec@5 100.000 (99.653)
Test on T test set - [1][9/25]	Time 0.297 (0.277)	Loss 0.9383 (0.4784)	Prec@1 75.000 (86.875)	Prec@5 96.875 (99.375)
Test on T test set - [1][10/25]	Time 0.250 (0.274)	Loss 1.7629 (0.5952)	Prec@1 46.875 (83.239)	Prec@5 81.250 (97.727)
Test on T test set - [1][11/25]	Time 0.240 (0.271)	Loss 2.3553 (0.7419)	Prec@1 40.625 (79.688)	Prec@5 68.750 (95.312)
Test on T test set - [1][12/25]	Time 0.266 (0.271)	Loss 1.6569 (0.8122)	Prec@1 53.125 (77.644)	Prec@5 81.250 (94.231)
Test on T test set - [1][13/25]	Time 0.175 (0.264)	Loss 0.2502 (0.7721)	Prec@1 96.875 (79.018)	Prec@5 100.000 (94.643)
Test on T test set - [1][14/25]	Time 0.212 (0.261)	Loss 0.4480 (0.7505)	Prec@1 87.500 (79.583)	Prec@5 93.750 (94.583)
Test on T test set - [1][15/25]	Time 0.224 (0.258)	Loss 0.3528 (0.7256)	Prec@1 93.750 (80.469)	Prec@5 93.750 (94.531)
Test on T test set - [1][16/25]	Time 0.227 (0.257)	Loss 0.1694 (0.6929)	Prec@1 96.875 (81.434)	Prec@5 96.875 (94.669)
Test on T test set - [1][17/25]	Time 0.301 (0.259)	Loss 0.7027 (0.6935)	Prec@1 81.250 (81.424)	Prec@5 93.750 (94.618)
Test on T test set - [1][18/25]	Time 0.255 (0.259)	Loss 0.9614 (0.7076)	Prec@1 68.750 (80.757)	Prec@5 96.875 (94.737)
Test on T test set - [1][19/25]	Time 0.282 (0.260)	Loss 0.4623 (0.6953)	Prec@1 87.500 (81.094)	Prec@5 100.000 (95.000)
Test on T test set - [1][20/25]	Time 0.291 (0.261)	Loss 0.9064 (0.7053)	Prec@1 68.750 (80.506)	Prec@5 100.000 (95.238)
Test on T test set - [1][21/25]	Time 0.220 (0.260)	Loss 0.1447 (0.6799)	Prec@1 100.000 (81.392)	Prec@5 100.000 (95.455)
Test on T test set - [1][22/25]	Time 0.232 (0.258)	Loss 0.9197 (0.6903)	Prec@1 78.125 (81.250)	Prec@5 100.000 (95.652)
Test on T test set - [1][23/25]	Time 0.204 (0.256)	Loss 0.3600 (0.6765)	Prec@1 90.625 (81.641)	Prec@5 100.000 (95.833)
Test on T test set - [1][24/25]	Time 0.204 (0.254)	Loss 0.8997 (0.6841)	Prec@1 70.370 (81.258)	Prec@5 96.296 (95.849)
 * Test on T test set - Prec@1 81.258, Prec@5 95.849
Epoch 1 - Kernel K-means clustering 0: Clustering time 0.006, Prec@1 87.673
Epoch 1 - Kernel K-means clustering 1: Clustering time 0.007, Prec@1 88.428
Epoch 1 - Kernel K-means clustering 2: Clustering time 0.006, Prec@1 89.686
Epoch 1 - Kernel K-means clustering 3: Clustering time 0.006, Prec@1 90.189
Epoch 1 - Kernel K-means clustering 4: Clustering time 0.006, Prec@1 90.818
Epoch 1 - Kernel K-means clustering 5: Clustering time 0.006, Prec@1 91.698
Epoch 1 - Kernel K-means clustering 6: Clustering time 0.006, Prec@1 92.075
Epoch 1 - Kernel K-means clustering 7: Clustering time 0.006, Prec@1 92.075
Converged at iteration 8
Epoch 1 - Kernel K-means clustering 0: Clustering time 0.006, Prec@1 85.535
Epoch 1 - Kernel K-means clustering 1: Clustering time 0.007, Prec@1 87.421
Epoch 1 - Kernel K-means clustering 2: Clustering time 0.006, Prec@1 89.434
Epoch 1 - Kernel K-means clustering 3: Clustering time 0.007, Prec@1 90.440
Epoch 1 - Kernel K-means clustering 4: Clustering time 0.006, Prec@1 90.692
Epoch 1 - Kernel K-means clustering 5: Clustering time 0.006, Prec@1 90.692
Converged at iteration 6
Train - epoch [2/200]	BT 1.033 (1.033)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 2.7458 (2.7458)
Train - epoch [2/200]	BT 0.741 (0.741)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.7472 (2.7472)
Train - epoch [2/200]	BT 0.777 (0.777)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.7148 (2.7148)
Train - epoch [2/200]	BT 0.785 (0.785)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 2.7741 (2.7741)
Train - epoch [2/200]	BT 0.812 (0.812)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6939 (2.6939)
Train - epoch [2/200]	BT 0.793 (0.793)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6848 (2.6848)
Train - epoch [2/200]	BT 0.784 (0.784)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6806 (2.6806)
Train - epoch [2/200]	BT 0.802 (0.802)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.7283 (2.7283)
Train - epoch [2/200]	BT 0.777 (0.777)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.7002 (2.7002)
Train - epoch [2/200]	BT 0.800 (0.800)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 2.7677 (2.7677)
Train - epoch [2/200]	BT 0.795 (0.795)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 2.6868 (2.6868)
Train - epoch [2/200]	BT 0.780 (0.780)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6936 (2.6936)
Train - epoch [2/200]	BT 0.795 (0.795)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6752 (2.6752)
Train - epoch [2/200]	BT 0.796 (0.796)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6601 (2.6601)
Train - epoch [2/200]	BT 0.804 (0.804)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6489 (2.6489)
Train - epoch [2/200]	BT 0.816 (0.816)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6612 (2.6612)
Train - epoch [2/200]	BT 0.785 (0.785)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 2.7682 (2.7682)
Train - epoch [2/200]	BT 0.808 (0.808)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 2.7204 (2.7204)
Train - epoch [2/200]	BT 0.815 (0.815)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 2.7131 (2.7131)
Train - epoch [2/200]	BT 0.803 (0.803)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6595 (2.6595)
Train - epoch [2/200]	BT 0.782 (0.782)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 2.8138 (2.8138)
Train - epoch [2/200]	BT 0.784 (0.784)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6673 (2.6673)
Train - epoch [2/200]	BT 0.762 (0.762)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6578 (2.6578)
Train - epoch [2/200]	BT 0.761 (0.761)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 2.8003 (2.8003)
Train - epoch [2/200]	BT 1.022 (1.022)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6861 (2.6861)
Train - epoch [2/200]	BT 0.765 (0.765)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 2.7621 (2.7621)
Train - epoch [2/200]	BT 0.832 (0.832)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6466 (2.6466)
Train - epoch [2/200]	BT 0.820 (0.820)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.7097 (2.7097)
Train - epoch [2/200]	BT 0.787 (0.787)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 2.7360 (2.7360)
Train - epoch [2/200]	BT 0.783 (0.783)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 2.6686 (2.6686)
Train - epoch [2/200]	BT 0.797 (0.797)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6622 (2.6622)
Train - epoch [2/200]	BT 0.792 (0.792)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6970 (2.6970)
Train - epoch [2/200]	BT 0.784 (0.784)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 2.7566 (2.7566)
Train - epoch [2/200]	BT 0.780 (0.780)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6569 (2.6569)
Train - epoch [2/200]	BT 0.792 (0.792)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6668 (2.6668)
Train - epoch [2/200]	BT 0.784 (0.784)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 2.6813 (2.6813)
Train - epoch [2/200]	BT 0.788 (0.788)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6854 (2.6854)
Train - epoch [2/200]	BT 0.796 (0.796)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 2.7265 (2.7265)
Train - epoch [2/200]	BT 0.763 (0.763)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6959 (2.6959)
Train - epoch [2/200]	BT 0.789 (0.789)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 2.6732 (2.6732)
Train - epoch [2/200]	BT 0.793 (0.793)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6875 (2.6875)
Train - epoch [2/200]	BT 0.792 (0.792)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6680 (2.6680)
Train - epoch [2/200]	BT 0.725 (0.725)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6789 (2.6789)
Train - epoch [2/200]	BT 0.779 (0.779)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6607 (2.6607)
Train - epoch [2/200]	BT 0.783 (0.783)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 2.6925 (2.6925)
Train - epoch [2/200]	BT 0.781 (0.781)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 2.7260 (2.7260)
Train - epoch [2/200]	BT 0.762 (0.762)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 2.7286 (2.7286)
Train - epoch [2/200]	BT 0.761 (0.761)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6276 (2.6276)
Train - epoch [2/200]	BT 1.016 (1.016)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 2.7522 (2.7522)
Train - epoch [2/200]	BT 0.811 (0.811)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6335 (2.6335)
Train - epoch [2/200]	BT 0.782 (0.782)	DT 0.000 (0.000)	S@1 87.500 (87.500)	Loss 2.8624 (2.8624)
Train - epoch [2/200]	BT 0.767 (0.767)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 2.6345 (2.6345)
Train - epoch [2/200]	BT 0.756 (0.756)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 2.7094 (2.7094)
Train - epoch [2/200]	BT 0.787 (0.787)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 2.7254 (2.7254)
Train - epoch [2/200]	BT 0.789 (0.789)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 2.6558 (2.6558)
Train - epoch [2/200]	BT 0.782 (0.782)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6508 (2.6508)
Train - epoch [2/200]	BT 0.786 (0.786)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6474 (2.6474)
Train - epoch [2/200]	BT 0.798 (0.798)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6505 (2.6505)
Train - epoch [2/200]	BT 0.797 (0.797)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6125 (2.6125)
Train - epoch [2/200]	BT 0.799 (0.799)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 2.6303 (2.6303)
Train - epoch [2/200]	BT 0.794 (0.794)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6234 (2.6234)
Train - epoch [2/200]	BT 0.781 (0.781)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 2.7192 (2.7192)
Train - epoch [2/200]	BT 0.791 (0.791)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 2.6324 (2.6324)
Train - epoch [2/200]	BT 0.776 (0.776)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6681 (2.6681)
Train - epoch [2/200]	BT 0.791 (0.791)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 2.6917 (2.6917)
Train - epoch [2/200]	BT 0.806 (0.806)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6419 (2.6419)
Train - epoch [2/200]	BT 0.785 (0.785)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6015 (2.6015)
Train - epoch [2/200]	BT 0.787 (0.787)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6768 (2.6768)
Train - epoch [2/200]	BT 0.784 (0.784)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6660 (2.6660)
Train - epoch [2/200]	BT 0.797 (0.797)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6051 (2.6051)
Train - epoch [2/200]	BT 0.761 (0.761)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 2.7578 (2.7578)
Train - epoch [2/200]	BT 0.755 (0.755)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 2.6825 (2.6825)
Train - epoch [2/200]	BT 0.989 (0.989)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6355 (2.6355)
Train - epoch [2/200]	BT 0.792 (0.792)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6279 (2.6279)
Train - epoch [2/200]	BT 0.788 (0.788)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 2.6699 (2.6699)
Train - epoch [2/200]	BT 0.779 (0.779)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6165 (2.6165)
Train - epoch [2/200]	BT 0.750 (0.750)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 2.7251 (2.7251)
Train - epoch [2/200]	BT 0.802 (0.802)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 2.7142 (2.7142)
Train - epoch [2/200]	BT 0.793 (0.793)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6175 (2.6175)
Train - epoch [2/200]	BT 0.775 (0.775)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6457 (2.6457)
Train - epoch [2/200]	BT 0.791 (0.791)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6161 (2.6161)
Train - epoch [2/200]	BT 0.798 (0.798)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.5597 (2.5597)
Train - epoch [2/200]	BT 0.777 (0.777)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.5916 (2.5916)
Train - epoch [2/200]	BT 0.778 (0.778)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6352 (2.6352)
Train - epoch [2/200]	BT 0.792 (0.792)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.5703 (2.5703)
Train - epoch [2/200]	BT 0.819 (0.819)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.6130 (2.6130)
Train - epoch [2/200]	BT 0.772 (0.772)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 2.5819 (2.5819)
Train - epoch [2/200]	BT 0.776 (0.776)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 2.6414 (2.6414)
Test on T training set - [2][0/25]	T 0.375 (0.375)	D 0.303 (0.303)	T@1 100.000 (100.000)	T@5 100.000 (100.000)	L 0.1640 (0.1640)
Test on T training set - [2][1/25]	T 0.276 (0.325)	D 0.216 (0.259)	T@1 100.000 (100.000)	T@5 100.000 (100.000)	L 0.0307 (0.0973)
Test on T training set - [2][2/25]	T 0.248 (0.299)	D 0.186 (0.235)	T@1 90.625 (96.875)	T@5 100.000 (100.000)	L 0.4600 (0.2182)
Test on T training set - [2][3/25]	T 0.186 (0.271)	D 0.126 (0.208)	T@1 65.625 (89.062)	T@5 100.000 (100.000)	L 0.9753 (0.4075)
Test on T training set - [2][4/25]	T 0.266 (0.270)	D 0.202 (0.206)	T@1 84.375 (88.125)	T@5 100.000 (100.000)	L 0.3890 (0.4038)
Test on T training set - [2][5/25]	T 0.290 (0.273)	D 0.229 (0.210)	T@1 93.750 (89.062)	T@5 96.875 (99.479)	L 0.4458 (0.4108)
Test on T training set - [2][6/25]	T 0.299 (0.277)	D 0.229 (0.213)	T@1 84.375 (88.393)	T@5 100.000 (99.554)	L 0.5295 (0.4277)
Test on T training set - [2][7/25]	T 0.224 (0.270)	D 0.160 (0.206)	T@1 100.000 (89.844)	T@5 100.000 (99.609)	L 0.1243 (0.3898)
Test on T training set - [2][8/25]	T 0.289 (0.272)	D 0.226 (0.209)	T@1 96.875 (90.625)	T@5 100.000 (99.653)	L 0.1742 (0.3658)
Test on T training set - [2][9/25]	T 0.288 (0.274)	D 0.226 (0.210)	T@1 81.250 (89.688)	T@5 100.000 (99.688)	L 0.7540 (0.4047)
Test on T training set - [2][10/25]	T 0.255 (0.272)	D 0.192 (0.209)	T@1 62.500 (87.216)	T@5 87.500 (98.580)	L 1.4296 (0.4978)
Test on T training set - [2][11/25]	T 0.239 (0.269)	D 0.179 (0.206)	T@1 53.125 (84.375)	T@5 81.250 (97.135)	L 1.7373 (0.6011)
Test on T training set - [2][12/25]	T 0.300 (0.272)	D 0.232 (0.208)	T@1 56.250 (82.212)	T@5 90.625 (96.635)	L 1.2902 (0.6541)
Test on T training set - [2][13/25]	T 0.177 (0.265)	D 0.108 (0.201)	T@1 96.875 (83.259)	T@5 100.000 (96.875)	L 0.2683 (0.6266)
Test on T training set - [2][14/25]	T 0.210 (0.261)	D 0.145 (0.197)	T@1 87.500 (83.542)	T@5 96.875 (96.875)	L 0.3923 (0.6110)
Test on T training set - [2][15/25]	T 0.244 (0.260)	D 0.185 (0.197)	T@1 93.750 (84.180)	T@5 100.000 (97.070)	L 0.3572 (0.5951)
Test on T training set - [2][16/25]	T 0.233 (0.259)	D 0.167 (0.195)	T@1 96.875 (84.926)	T@5 96.875 (97.059)	L 0.1807 (0.5707)
Test on T training set - [2][17/25]	T 0.304 (0.261)	D 0.237 (0.197)	T@1 75.000 (84.375)	T@5 96.875 (97.049)	L 0.7336 (0.5798)
Test on T training set - [2][18/25]	T 0.249 (0.261)	D 0.189 (0.197)	T@1 75.000 (83.882)	T@5 100.000 (97.204)	L 0.7398 (0.5882)
Test on T training set - [2][19/25]	T 0.289 (0.262)	D 0.224 (0.198)	T@1 81.250 (83.750)	T@5 100.000 (97.344)	L 0.5258 (0.5851)
Test on T training set - [2][20/25]	T 0.291 (0.263)	D 0.231 (0.200)	T@1 78.125 (83.482)	T@5 100.000 (97.470)	L 0.8274 (0.5966)
Test on T training set - [2][21/25]	T 0.223 (0.262)	D 0.155 (0.198)	T@1 96.875 (84.091)	T@5 100.000 (97.585)	L 0.1757 (0.5775)
Test on T training set - [2][22/25]	T 0.235 (0.260)	D 0.176 (0.197)	T@1 84.375 (84.103)	T@5 100.000 (97.690)	L 0.7314 (0.5842)
Test on T training set - [2][23/25]	T 0.194 (0.258)	D 0.131 (0.194)	T@1 87.500 (84.245)	T@5 100.000 (97.786)	L 0.3615 (0.5749)
Test on T training set - [2][24/25]	T 0.212 (0.256)	D 0.158 (0.192)	T@1 74.074 (83.899)	T@5 100.000 (97.862)	L 0.7251 (0.5800)
 * Test on T training set - Prec@1 83.899, Prec@5 97.862
Test on T test set - [2][0/25]	Time 0.370 (0.370)	Loss 0.1566 (0.1566)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Test on T test set - [2][1/25]	Time 0.275 (0.323)	Loss 0.0405 (0.0986)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Test on T test set - [2][2/25]	Time 0.244 (0.297)	Loss 0.4096 (0.2023)	Prec@1 93.750 (97.917)	Prec@5 100.000 (100.000)
Test on T test set - [2][3/25]	Time 0.183 (0.268)	Loss 1.0490 (0.4139)	Prec@1 59.375 (88.281)	Prec@5 100.000 (100.000)
Test on T test set - [2][4/25]	Time 0.260 (0.266)	Loss 0.3238 (0.3959)	Prec@1 84.375 (87.500)	Prec@5 100.000 (100.000)
Test on T test set - [2][5/25]	Time 0.300 (0.272)	Loss 0.4023 (0.3970)	Prec@1 93.750 (88.542)	Prec@5 96.875 (99.479)
Test on T test set - [2][6/25]	Time 0.285 (0.274)	Loss 0.4954 (0.4111)	Prec@1 84.375 (87.946)	Prec@5 100.000 (99.554)
Test on T test set - [2][7/25]	Time 0.235 (0.269)	Loss 0.1502 (0.3784)	Prec@1 100.000 (89.453)	Prec@5 100.000 (99.609)
Test on T test set - [2][8/25]	Time 0.299 (0.272)	Loss 0.1398 (0.3519)	Prec@1 96.875 (90.278)	Prec@5 100.000 (99.653)
Test on T test set - [2][9/25]	Time 0.302 (0.275)	Loss 0.6639 (0.3831)	Prec@1 84.375 (89.688)	Prec@5 100.000 (99.688)
Test on T test set - [2][10/25]	Time 0.243 (0.272)	Loss 1.3575 (0.4717)	Prec@1 59.375 (86.932)	Prec@5 87.500 (98.580)
Test on T test set - [2][11/25]	Time 0.238 (0.270)	Loss 1.7690 (0.5798)	Prec@1 46.875 (83.594)	Prec@5 81.250 (97.135)
Test on T test set - [2][12/25]	Time 0.267 (0.269)	Loss 1.2051 (0.6279)	Prec@1 56.250 (81.490)	Prec@5 90.625 (96.635)
Test on T test set - [2][13/25]	Time 0.181 (0.263)	Loss 0.2455 (0.6006)	Prec@1 96.875 (82.589)	Prec@5 100.000 (96.875)
Test on T test set - [2][14/25]	Time 0.208 (0.259)	Loss 0.4238 (0.5888)	Prec@1 87.500 (82.917)	Prec@5 96.875 (96.875)
Test on T test set - [2][15/25]	Time 0.226 (0.257)	Loss 0.3284 (0.5725)	Prec@1 93.750 (83.594)	Prec@5 96.875 (96.875)
Test on T test set - [2][16/25]	Time 0.218 (0.255)	Loss 0.1619 (0.5484)	Prec@1 96.875 (84.375)	Prec@5 100.000 (97.059)
Test on T test set - [2][17/25]	Time 0.299 (0.257)	Loss 0.7125 (0.5575)	Prec@1 75.000 (83.854)	Prec@5 96.875 (97.049)
Test on T test set - [2][18/25]	Time 0.253 (0.257)	Loss 0.7657 (0.5685)	Prec@1 78.125 (83.553)	Prec@5 100.000 (97.204)
Test on T test set - [2][19/25]	Time 0.275 (0.258)	Loss 0.5243 (0.5662)	Prec@1 84.375 (83.594)	Prec@5 100.000 (97.344)
Test on T test set - [2][20/25]	Time 0.294 (0.260)	Loss 0.7997 (0.5774)	Prec@1 78.125 (83.333)	Prec@5 100.000 (97.470)
Test on T test set - [2][21/25]	Time 0.207 (0.257)	Loss 0.1761 (0.5591)	Prec@1 100.000 (84.091)	Prec@5 100.000 (97.585)
Test on T test set - [2][22/25]	Time 1.744 (0.322)	Loss 0.6770 (0.5642)	Prec@1 81.250 (83.967)	Prec@5 100.000 (97.690)
Test on T test set - [2][23/25]	Time 0.231 (0.318)	Loss 0.3583 (0.5557)	Prec@1 93.750 (84.375)	Prec@5 100.000 (97.786)
Test on T test set - [2][24/25]	Time 0.210 (0.314)	Loss 0.7036 (0.5607)	Prec@1 77.778 (84.151)	Prec@5 96.296 (97.736)
 * Test on T test set - Prec@1 84.151, Prec@5 97.736
Epoch 2 - Kernel K-means clustering 0: Clustering time 0.006, Prec@1 88.679
Epoch 2 - Kernel K-means clustering 1: Clustering time 0.007, Prec@1 89.308
Epoch 2 - Kernel K-means clustering 2: Clustering time 0.006, Prec@1 89.308
Epoch 2 - Kernel K-means clustering 3: Clustering time 0.006, Prec@1 89.686
Epoch 2 - Kernel K-means clustering 4: Clustering time 0.006, Prec@1 89.560
Epoch 2 - Kernel K-means clustering 5: Clustering time 0.006, Prec@1 89.560
Converged at iteration 6
Epoch 2 - Kernel K-means clustering 0: Clustering time 0.006, Prec@1 87.044
Epoch 2 - Kernel K-means clustering 1: Clustering time 0.007, Prec@1 88.176
Epoch 2 - Kernel K-means clustering 2: Clustering time 0.013, Prec@1 88.553
Epoch 2 - Kernel K-means clustering 3: Clustering time 0.006, Prec@1 88.553
Epoch 2 - Kernel K-means clustering 4: Clustering time 0.006, Prec@1 88.679
Epoch 2 - Kernel K-means clustering 5: Clustering time 0.006, Prec@1 88.805
Epoch 2 - Kernel K-means clustering 6: Clustering time 0.007, Prec@1 88.805
Converged at iteration 7
Train - epoch [3/200]	BT 1.026 (1.026)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.8376 (3.8376)
Train - epoch [3/200]	BT 0.822 (0.822)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 3.8700 (3.8700)
Train - epoch [3/200]	BT 0.795 (0.795)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.8266 (3.8266)
Train - epoch [3/200]	BT 0.782 (0.782)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.8368 (3.8368)
Train - epoch [3/200]	BT 0.799 (0.799)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 3.8641 (3.8641)
Train - epoch [3/200]	BT 0.779 (0.779)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.8358 (3.8358)
Train - epoch [3/200]	BT 0.810 (0.810)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 3.8744 (3.8744)
Train - epoch [3/200]	BT 0.792 (0.792)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.8025 (3.8025)
Train - epoch [3/200]	BT 0.805 (0.805)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 3.8496 (3.8496)
Train - epoch [3/200]	BT 0.799 (0.799)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 3.8842 (3.8842)
Train - epoch [3/200]	BT 0.781 (0.781)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 3.8799 (3.8799)
Train - epoch [3/200]	BT 0.794 (0.794)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 3.8611 (3.8611)
Train - epoch [3/200]	BT 0.802 (0.802)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.8032 (3.8032)
Train - epoch [3/200]	BT 0.769 (0.769)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.8000 (3.8000)
Train - epoch [3/200]	BT 0.788 (0.788)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 3.8569 (3.8569)
Train - epoch [3/200]	BT 0.790 (0.790)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.7816 (3.7816)
Train - epoch [3/200]	BT 0.806 (0.806)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.8072 (3.8072)
Train - epoch [3/200]	BT 0.794 (0.794)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.7861 (3.7861)
Train - epoch [3/200]	BT 0.804 (0.804)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.8049 (3.8049)
Train - epoch [3/200]	BT 0.772 (0.772)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.8171 (3.8171)
Train - epoch [3/200]	BT 0.780 (0.780)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.8290 (3.8290)
Train - epoch [3/200]	BT 0.787 (0.787)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.7884 (3.7884)
Train - epoch [3/200]	BT 0.669 (0.669)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.7724 (3.7724)
Train - epoch [3/200]	BT 0.758 (0.758)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.8123 (3.8123)
Train - epoch [3/200]	BT 0.997 (0.997)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.7583 (3.7583)
Train - epoch [3/200]	BT 0.773 (0.773)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.7719 (3.7719)
Train - epoch [3/200]	BT 0.820 (0.820)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.7741 (3.7741)
Train - epoch [3/200]	BT 0.794 (0.794)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.7551 (3.7551)
Train - epoch [3/200]	BT 0.800 (0.800)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.7267 (3.7267)
Train - epoch [3/200]	BT 0.807 (0.807)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 3.8247 (3.8247)
Train - epoch [3/200]	BT 0.791 (0.791)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.7372 (3.7372)
Train - epoch [3/200]	BT 0.792 (0.792)	DT 0.000 (0.000)	S@1 93.750 (93.750)	Loss 3.9038 (3.9038)
Train - epoch [3/200]	BT 0.795 (0.795)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.7431 (3.7431)
Train - epoch [3/200]	BT 0.792 (0.792)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.7296 (3.7296)
Train - epoch [3/200]	BT 0.797 (0.797)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.7572 (3.7572)
Train - epoch [3/200]	BT 0.793 (0.793)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.7925 (3.7925)
Train - epoch [3/200]	BT 0.781 (0.781)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 3.8676 (3.8676)
Train - epoch [3/200]	BT 0.801 (0.801)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.7350 (3.7350)
Train - epoch [3/200]	BT 0.791 (0.791)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.7491 (3.7491)
Train - epoch [3/200]	BT 0.775 (0.775)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.7121 (3.7121)
Train - epoch [3/200]	BT 0.775 (0.775)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 3.7852 (3.7852)
Train - epoch [3/200]	BT 0.794 (0.794)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.7418 (3.7418)
Train - epoch [3/200]	BT 0.783 (0.783)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.7850 (3.7850)
Train - epoch [3/200]	BT 0.780 (0.780)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.7597 (3.7597)
Train - epoch [3/200]	BT 0.747 (0.747)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.7126 (3.7126)
Train - epoch [3/200]	BT 0.771 (0.771)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 3.7753 (3.7753)
Train - epoch [3/200]	BT 0.755 (0.755)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 3.6994 (3.6994)
Train - epoch [3/200]	BT 0.753 (0.753)	DT 0.000 (0.000)	S@1 96.875 (96.875)	Loss 3.7283 (3.7283)
