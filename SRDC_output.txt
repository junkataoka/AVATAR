==> creating model 'resnet50' 
/data/home/jkataok1/alexnet_resnet_finetune/checkpoints/amazon_to_webcam_resnet50.pkl
Source pre-trained model has been loaded!
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
==> creating model 'resnet50' 
/data/home/jkataok1/alexnet_resnet_finetune/checkpoints/amazon_to_webcam_resnet50.pkl
Source pre-trained model has been loaded!
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=7, stride=7, padding=0)
  (fc1): Sequential(
    (0): Linear(in_features=2048, out_features=512, bias=True)
    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (fc2): Linear(in_features=512, out_features=32, bias=True)
  (random_layer): RandomLayer()
  (domain_classifier): Sequential(
    (d_fc1): Linear(in_features=2048, out_features=512, bias=True)
    (d_bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (d_relu1): ReLU(inplace=True)
    (d_fc2): Linear(in_features=512, out_features=32, bias=True)
  )
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=7, stride=7, padding=0)
  (fc1): Sequential(
    (0): Linear(in_features=2048, out_features=512, bias=True)
    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (fc2): Linear(in_features=512, out_features=32, bias=True)
  (random_layer): RandomLayer()
  (domain_classifier): Sequential(
    (d_fc1): Linear(in_features=2048, out_features=512, bias=True)
    (d_bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (d_relu1): ReLU(inplace=True)
    (d_fc2): Linear(in_features=512, out_features=32, bias=True)
  )
)
https://app.neptune.ai/junkataoka/SRDC/e/SRDC-764
Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.
)
https://app.neptune.ai/junkataoka/SRDC/e/SRDC-763
Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.
begin training
begin training
Test on T training set - [0][0/13]	T 0.819 (0.819)	D 0.705 (0.705)	T@1 87.302 (87.302)	T@5 98.413 (98.413)	L 0.4808 (0.4808)
Test on T training set - [0][10/13]	T 0.549 (0.604)	D 0.430 (0.486)	T@1 74.603 (76.768)	T@5 100.000 (94.661)	L 0.7314 (0.8402)
 * Test on T training set - Prec@1 77.484, Prec@5 95.220
Test on T training set - [0][0/13]	T 0.802 (0.802)	D 0.686 (0.686)	T@1 87.302 (87.302)	T@5 98.413 (98.413)	L 0.4808 (0.4808)
Test on T training set - [0][10/13]	T 0.548 (0.601)	D 0.436 (0.484)	T@1 74.603 (76.768)	T@5 100.000 (94.661)	L 0.7314 (0.8402)
 * Test on T training set - Prec@1 77.484, Prec@5 95.220
Test on T test set - [0][0/13]	Time 0.715 (0.715)	Loss 0.4808 (0.4808)	Prec@1 87.302 (87.302)	Prec@5 98.413 (98.413)
Test on T test set - [0][10/13]	Time 0.516 (0.517)	Loss 0.7314 (0.8402)	Prec@1 74.603 (76.768)	Prec@5 100.000 (94.661)
 * Test on T test set - Prec@1 77.484, Prec@5 95.220
Epoch 0, K-means clustering 0, Average clustering time 0.021, Prec@1 83.019
Epoch 0, K-means clustering 1, Average clustering time 0.096, Prec@1 84.025
Epoch 0, K-means clustering 2, Average clustering time 0.096, Prec@1 84.780
Epoch 0, K-means clustering 3, Average clustering time 0.096, Prec@1 85.031
Epoch 0, K-means clustering 4, Average clustering time 0.096, Prec@1 85.786
Epoch 0, K-means clustering 5, Average clustering time 0.096, Prec@1 86.164
Epoch 0, K-means clustering 6, Average clustering time 0.096, Prec@1 86.541
Epoch 0, K-means clustering 7, Average clustering time 0.096, Prec@1 86.667
Epoch 0, K-means clustering 8, Average clustering time 0.096, Prec@1 86.792
Epoch 0, K-means clustering 9, Average clustering time 0.096, Prec@1 86.918
Epoch 0, K-means clustering 10, Average clustering time 0.095, Prec@1 87.044
Epoch 0, K-means clustering 11, Average clustering time 0.095, Prec@1 87.421
Epoch 0, K-means clustering 12, Average clustering time 0.095, Prec@1 87.799
Epoch 0, K-means clustering 13, Average clustering time 0.095, Prec@1 87.673
Epoch 0, K-means clustering 14, Average clustering time 0.095, Prec@1 87.673
Epoch 0, K-means clustering 15, Average clustering time 0.095, Prec@1 87.673
Epoch 0, K-means clustering 16, Average clustering time 0.095, Prec@1 87.673
Epoch 0, K-means clustering 17, Average clustering time 0.095, Prec@1 87.673
Epoch 0, K-means clustering 18, Average clustering time 0.095, Prec@1 87.673
Epoch 0, K-means clustering 19, Average clustering time 0.096, Prec@1 87.673
Epoch 0, K-means clustering 0, Average clustering time 0.003, Prec@1 84.151
Epoch 0, K-means clustering 1, Average clustering time 0.051, Prec@1 84.654
Epoch 0, K-means clustering 2, Average clustering time 0.067, Prec@1 85.157
Epoch 0, K-means clustering 3, Average clustering time 0.075, Prec@1 85.409
Epoch 0, K-means clustering 4, Average clustering time 0.080, Prec@1 85.409
Epoch 0, K-means clustering 5, Average clustering time 0.082, Prec@1 85.283
Epoch 0, K-means clustering 6, Average clustering time 0.084, Prec@1 85.283
Epoch 0, K-means clustering 7, Average clustering time 0.085, Prec@1 85.283
Epoch 0, K-means clustering 8, Average clustering time 0.086, Prec@1 85.283
Epoch 0, K-means clustering 9, Average clustering time 0.087, Prec@1 85.283
Epoch 0, K-means clustering 10, Average clustering time 0.087, Prec@1 85.283
Epoch 0, K-means clustering 11, Average clustering time 0.088, Prec@1 85.283
Epoch 0, K-means clustering 12, Average clustering time 0.089, Prec@1 85.283
Epoch 0, K-means clustering 13, Average clustering time 0.090, Prec@1 85.283
Epoch 0, K-means clustering 14, Average clustering time 0.090, Prec@1 85.283
Epoch 0, K-means clustering 15, Average clustering time 0.090, Prec@1 85.283
Epoch 0, K-means clustering 16, Average clustering time 0.091, Prec@1 85.283
Epoch 0, K-means clustering 17, Average clustering time 0.091, Prec@1 85.283
Epoch 0, K-means clustering 18, Average clustering time 0.091, Prec@1 85.283
Epoch 0, K-means clustering 19, Average clustering time 0.092, Prec@1 85.283
Test on T test set - [0][0/13]	Time 0.719 (0.719)	Loss 0.4808 (0.4808)	Prec@1 87.302 (87.302)	Prec@5 98.413 (98.413)
Test on T test set - [0][10/13]	Time 0.512 (0.517)	Loss 0.7314 (0.8402)	Prec@1 74.603 (76.768)	Prec@5 100.000 (94.661)
 * Test on T test set - Prec@1 77.484, Prec@5 95.220
Epoch 0, K-means clustering 0, Average clustering time 0.017, Prec@1 78.113
Epoch 0, K-means clustering 1, Average clustering time 0.098, Prec@1 82.138
Epoch 0, K-means clustering 2, Average clustering time 0.097, Prec@1 83.270
Epoch 0, K-means clustering 3, Average clustering time 0.096, Prec@1 84.025
Epoch 0, K-means clustering 4, Average clustering time 0.096, Prec@1 84.780
Epoch 0, K-means clustering 5, Average clustering time 0.095, Prec@1 85.283
Epoch 0, K-means clustering 6, Average clustering time 0.095, Prec@1 85.786
Epoch 0, K-means clustering 7, Average clustering time 0.095, Prec@1 85.786
Epoch 0, K-means clustering 8, Average clustering time 0.095, Prec@1 85.786
Epoch 0, K-means clustering 9, Average clustering time 0.095, Prec@1 85.786
Epoch 0, K-means clustering 10, Average clustering time 0.095, Prec@1 85.786
Epoch 0, K-means clustering 11, Average clustering time 0.095, Prec@1 85.786
Epoch 0, K-means clustering 12, Average clustering time 0.096, Prec@1 85.786
Epoch 0, K-means clustering 13, Average clustering time 0.096, Prec@1 85.786
Epoch 0, K-means clustering 14, Average clustering time 0.095, Prec@1 85.786
Epoch 0, K-means clustering 15, Average clustering time 0.095, Prec@1 85.786
Epoch 0, K-means clustering 16, Average clustering time 0.097, Prec@1 85.786
Epoch 0, K-means clustering 17, Average clustering time 0.097, Prec@1 85.786
Epoch 0, K-means clustering 18, Average clustering time 0.097, Prec@1 85.786
Epoch 0, K-means clustering 19, Average clustering time 0.097, Prec@1 85.786
Epoch 0, K-means clustering 0, Average clustering time 0.003, Prec@1 76.730
Epoch 0, K-means clustering 1, Average clustering time 0.051, Prec@1 81.887
Epoch 0, K-means clustering 2, Average clustering time 0.067, Prec@1 82.013
Epoch 0, K-means clustering 3, Average clustering time 0.076, Prec@1 82.138
Epoch 0, K-means clustering 4, Average clustering time 0.081, Prec@1 82.264
Epoch 0, K-means clustering 5, Average clustering time 0.086, Prec@1 82.264
Epoch 0, K-means clustering 6, Average clustering time 0.088, Prec@1 82.264
Epoch 0, K-means clustering 7, Average clustering time 0.089, Prec@1 82.264
Epoch 0, K-means clustering 8, Average clustering time 0.090, Prec@1 82.264
Epoch 0, K-means clustering 9, Average clustering time 0.091, Prec@1 82.264
Epoch 0, K-means clustering 10, Average clustering time 0.092, Prec@1 82.264
Epoch 0, K-means clustering 11, Average clustering time 0.093, Prec@1 82.264
Epoch 0, K-means clustering 12, Average clustering time 0.093, Prec@1 82.264
Epoch 0, K-means clustering 13, Average clustering time 0.093, Prec@1 82.264
Epoch 0, K-means clustering 14, Average clustering time 0.093, Prec@1 82.264
Epoch 0, K-means clustering 15, Average clustering time 0.094, Prec@1 82.264
Epoch 0, K-means clustering 16, Average clustering time 0.094, Prec@1 82.264
Epoch 0, K-means clustering 17, Average clustering time 0.094, Prec@1 82.264
Epoch 0, K-means clustering 18, Average clustering time 0.094, Prec@1 82.264
Epoch 0, K-means clustering 19, Average clustering time 0.094, Prec@1 82.264
Train - epoch [0/200]	BT 2.517 (2.517)	DT 0.000 (0.000)	S@1 97.619 (97.619)	Loss 0.1536 (0.1536)
The penalty weight is 0.000000
Train - epoch [0/200]	BT 1.031 (1.031)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.1226 (0.1226)
Train - epoch [0/200]	BT 2.542 (2.542)	DT 0.000 (0.000)	S@1 95.238 (95.238)	Loss 0.2384 (0.2384)
The penalty weight is 0.000000
Train - epoch [0/200]	BT 1.024 (1.024)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.1390 (0.1390)
Train - epoch [0/200]	BT 1.056 (1.056)	DT 0.000 (0.000)	S@1 95.238 (95.238)	Loss 0.2297 (0.2297)
Train - epoch [0/200]	BT 1.029 (1.029)	DT 0.000 (0.000)	S@1 90.476 (90.476)	Loss 0.3672 (0.3672)
Train - epoch [0/200]	BT 1.056 (1.056)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.2161 (0.2161)
Train - epoch [0/200]	BT 1.055 (1.055)	DT 0.000 (0.000)	S@1 97.619 (97.619)	Loss 0.2523 (0.2523)
Train - epoch [0/200]	BT 1.049 (1.049)	DT 0.000 (0.000)	S@1 97.619 (97.619)	Loss 0.2526 (0.2526)
Train - epoch [0/200]	BT 1.029 (1.029)	DT 0.000 (0.000)	S@1 95.238 (95.238)	Loss 0.2619 (0.2619)
Train - epoch [0/200]	BT 1.032 (1.032)	DT 0.000 (0.000)	S@1 92.857 (92.857)	Loss 0.2737 (0.2737)
Train - epoch [0/200]	BT 1.052 (1.052)	DT 0.000 (0.000)	S@1 95.238 (95.238)	Loss 0.3182 (0.3182)
    Train - epoch [0/200]	BT 1.025 (1.025)	DT 0.000 (0.000)	S@1 97.619 (97.619)	Loss 0.2172 (0.2172)
Train - epoch [0/200]	BT 1.019 (1.019)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.1998 (0.1998)
Test on T training set - [0][0/13]	T 0.702 (0.702)	D 0.582 (0.582)	T@1 98.413 (98.413)	T@5 100.000 (100.000)	L 0.2817 (0.2817)
Test on T training set - [0][10/13]	T 0.504 (0.511)	D 0.382 (0.392)	T@1 84.127 (78.644)	T@5 96.825 (94.228)	L 0.8622 (0.7997)
 * Test on T training set - Prec@1 78.239, Prec@5 95.094
Test on T training set - [0][0/13]	T 0.724 (0.724)	D 0.600 (0.600)	T@1 98.413 (98.413)	T@5 100.000 (100.000)	L 0.2184 (0.2184)
Test on T training set - [0][10/13]	T 0.512 (0.521)	D 0.386 (0.399)	T@1 77.778 (78.499)	T@5 96.825 (95.382)	L 0.8283 (0.7880)
 * Test on T training set - Prec@1 79.245, Prec@5 95.849
Test on T test set - [0][0/13]	Time 0.734 (0.734)	Loss 0.2184 (0.2184)	Prec@1 98.413 (98.413)	Prec@5 100.000 (100.000)
Test on T test set - [0][10/13]	Time 0.512 (0.520)	Loss 0.8283 (0.7880)	Prec@1 77.778 (78.499)	Prec@5 96.825 (95.382)
 * Test on T test set - Prec@1 79.245, Prec@5 95.849
Epoch 0 - Kernel K-means clustering 0: Clustering time 0.006, Prec@1 84.151
Epoch 0 - Kernel K-means clustering 1: Clustering time 0.008, Prec@1 86.289
Epoch 0 - Kernel K-means clustering 2: Clustering time 0.008, Prec@1 87.925
Epoch 0 - Kernel K-means clustering 3: Clustering time 0.008, Prec@1 88.931
Epoch 0 - Kernel K-means clustering 4: Clustering time 0.008, Prec@1 89.182
Epoch 0 - Kernel K-means clustering 5: Clustering time 0.011, Prec@1 89.182
Converged at iteration 6
Epoch 0 - Kernel K-means clustering 0: Clustering time 0.006, Prec@1 86.164
Epoch 0 - Kernel K-means clustering 1: Clustering time 0.008, Prec@1 88.553
Epoch 0 - Kernel K-means clustering 2: Clustering time 0.008, Prec@1 89.308
Epoch 0 - Kernel K-means clustering 3: Clustering time 0.008, Prec@1 89.560
Epoch 0 - Kernel K-means clustering 4: Clustering time 0.008, Prec@1 89.560
Converged at iteration 5
Test on T test set - [0][0/13]	Time 0.727 (0.727)	Loss 0.2150 (0.2150)	Prec@1 98.413 (98.413)	Prec@5 100.000 (100.000)
Test on T test set - [0][10/13]	Time 0.512 (0.519)	Loss 0.8622 (0.7997)	Prec@1 73.016 (77.345)	Prec@5 95.238 (94.372)
 * Test on T test set - Prec@1 78.239, Prec@5 95.094
Epoch 0 - Kernel K-means clustering 0: Clustering time 0.006, Prec@1 84.403
Epoch 0 - Kernel K-means clustering 1: Clustering time 0.009, Prec@1 85.157
Epoch 0 - Kernel K-means clustering 2: Clustering time 0.008, Prec@1 85.283
Epoch 0 - Kernel K-means clustering 3: Clustering time 0.008, Prec@1 85.660
Epoch 0 - Kernel K-means clustering 4: Clustering time 0.009, Prec@1 86.038
Epoch 0 - Kernel K-means clustering 5: Clustering time 0.010, Prec@1 86.415
Epoch 0 - Kernel K-means clustering 6: Clustering time 0.008, Prec@1 86.792
Epoch 0 - Kernel K-means clustering 7: Clustering time 0.008, Prec@1 87.296
Epoch 0 - Kernel K-means clustering 8: Clustering time 0.008, Prec@1 87.296
Epoch 0 - Kernel K-means clustering 9: Clustering time 0.009, Prec@1 87.421
Epoch 0 - Kernel K-means clustering 10: Clustering time 0.008, Prec@1 87.547
Epoch 0 - Kernel K-means clustering 11: Clustering time 0.008, Prec@1 87.547
Converged at iteration 12
Epoch 0 - Kernel K-means clustering 0: Clustering time 0.006, Prec@1 85.786
Epoch 0 - Kernel K-means clustering 1: Clustering time 0.009, Prec@1 85.786
Epoch 0 - Kernel K-means clustering 2: Clustering time 0.008, Prec@1 86.415
Epoch 0 - Kernel K-means clustering 3: Clustering time 0.008, Prec@1 87.044
Epoch 0 - Kernel K-means clustering 4: Clustering time 0.008, Prec@1 87.170
Epoch 0 - Kernel K-means clustering 5: Clustering time 0.008, Prec@1 87.170
Epoch 0 - Kernel K-means clustering 6: Clustering time 0.008, Prec@1 87.044
Epoch 0 - Kernel K-means clustering 7: Clustering time 0.008, Prec@1 87.044
Epoch 0 - Kernel K-means clustering 8: Clustering time 0.008, Prec@1 87.044
Epoch 0 - Kernel K-means clustering 9: Clustering time 0.009, Prec@1 87.044
Epoch 0 - Kernel K-means clustering 10: Clustering time 0.008, Prec@1 87.044
Converged at iteration 11
The penalty weight is 0.024995
Train - epoch [1/200]	BT 1.050 (1.050)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.4519 (0.4519)
Train - epoch [1/200]	BT 1.025 (1.025)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.4208 (0.4208)
The penalty weight is 0.024995
Train - epoch [1/200]	BT 1.033 (1.033)	DT 0.000 (0.000)	S@1 97.619 (97.619)	Loss 0.4353 (0.4353)
Train - epoch [1/200]	BT 1.079 (1.079)	DT 0.000 (0.000)	S@1 97.619 (97.619)	Loss 0.6408 (0.6408)
                                                                                                                                                                                                                                                                                                               Train - epoch [1/200]	BT 1.051 (1.051)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.5551 (0.5551)
Train - epoch [1/200]	BT 1.053 (1.053)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.4143 (0.4143)
Train - epoch [1/200]	BT 1.049 (1.049)	DT 0.000 (0.000)	S@1 95.238 (95.238)	Loss 0.5132 (0.5132)
Train - epoch [1/200]	BT 1.050 (1.050)	DT 0.000 (0.000)	S@1 97.619 (97.619)	Loss 0.5953 (0.5953)
Train - epoch [1/200]	BT 1.020 (1.020)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.4671 (0.4671)
Train - epoch [1/200]	BT 0.991 (0.991)	DT 0.000 (0.000)	S@1 95.238 (95.238)	Loss 0.5159 (0.5159)
Train - epoch [1/200]	BT 1.027 (1.027)	DT 0.000 (0.000)	S@1 100.000 (100.000)	Loss 0.6664 (0.6664)
Train - epoch [1/200]	BT 1.005 (1.005)	DT 0.000 (0.000)	S@1 97.619 (97.619)	Loss 0.4974 (0.4974)
Train - epoch [1/200]	BT 1.062 (1.062)	DT 0.000 (0.000)	S@1 95.238 (95.238)	Loss 0.4742 (0.4742)
Train - epoch [1/200]	BT 1.057 (1.057)	DT 0.000 (0.000)	S@1 97.619 (97.619)	Loss 0.5376 (0.5376)
Test on T training set - [1][0/13]	T 0.720 (0.720)	D 0.595 (0.595)	T@1 96.825 (96.825)	T@5 100.000 (100.000)	L 0.2232 (0.2232)
Test on T training set - [1][10/13]	T 0.504 (0.516)	D 0.383 (0.395)	T@1 68.254 (79.365)	T@5 95.238 (95.671)	L 0.8761 (0.7271)
 * Test on T training set - Prec@1 78.616, Prec@5 96.101
Test on T training set - [1][0/13]	T 0.711 (0.711)	D 0.587 (0.587)	T@1 98.413 (98.413)	T@5 100.000 (100.000)	L 0.2082 (0.2082)
Test on T training set - [1][10/13]	T 0.509 (0.517)	D 0.393 (0.396)	T@1 73.016 (78.932)	T@5 98.413 (95.238)	L 0.8090 (0.7474)
 * Test on T training set - Prec@1 79.245, Prec@5 95.723
Test on T test set - [1][0/13]	Time 0.724 (0.724)	Loss 0.2232 (0.2232)	Prec@1 96.825 (96.825)	Prec@5 100.000 (100.000)
Test on T test set - [1][10/13]	Time 0.493 (0.518)	Loss 0.8761 (0.7271)	Prec@1 68.254 (79.365)	Prec@5 95.238 (95.671)
 * Test on T test set - Prec@1 78.616, Prec@5 96.101
Epoch 1 - Kernel K-means clustering 0: Clustering time 0.007, Prec@1 83.270
Epoch 1 - Kernel K-means clustering 1: Clustering time 0.008, Prec@1 84.403
Epoch 1 - Kernel K-means clustering 2: Clustering time 0.008, Prec@1 85.157
Epoch 1 - Kernel K-means clustering 3: Clustering time 0.008, Prec@1 86.038
Epoch 1 - Kernel K-means clustering 4: Clustering time 0.008, Prec@1 86.415
Epoch 1 - Kernel K-means clustering 5: Clustering time 0.008, Prec@1 86.792
Epoch 1 - Kernel K-means clustering 6: Clustering time 0.008, Prec@1 86.918
Epoch 1 - Kernel K-means clustering 7: Clustering time 0.008, Prec@1 86.918
Epoch 1 - Kernel K-means clustering 8: Clustering time 0.008, Prec@1 86.918
Converged at iteration 9
Epoch 1 - Kernel K-means clustering 0: Clustering time 0.006, Prec@1 86.289
Epoch 1 - Kernel K-means clustering 1: Clustering time 0.008, Prec@1 87.799
Epoch 1 - Kernel K-means clustering 2: Clustering time 0.008, Prec@1 87.925
Epoch 1 - Kernel K-means clustering 3: Clustering time 0.008, Prec@1 87.925
Epoch 1 - Kernel K-means clustering 4: Clustering time 0.008, Prec@1 88.428
Epoch 1 - Kernel K-means clustering 5: Clustering time 0.007, Prec@1 88.805
Epoch 1 - Kernel K-means clustering 6: Clustering time 0.008, Prec@1 89.182
Epoch 1 - Kernel K-means clustering 7: Clustering time 0.008, Prec@1 89.182
Converged at iteration 8
Test on T test set - [1][0/13]	Time 0.715 (0.715)	Loss 0.2082 (0.2082)	Prec@1 98.413 (98.413)	Prec@5 100.000 (100.000)
Test on T test set - [1][10/13]	Time 0.508 (0.516)	Loss 0.8090 (0.7474)	Prec@1 73.016 (78.932)	Prec@5 98.413 (95.238)
 * Test on T test set - Prec@1 79.245, Prec@5 95.723
Epoch 1 - Kernel K-means clustering 0: Clustering time 0.007, Prec@1 83.019
Epoch 1 - Kernel K-means clustering 1: Clustering time 0.008, Prec@1 83.774
Epoch 1 - Kernel K-means clustering 2: Clustering time 0.008, Prec@1 84.528
Epoch 1 - Kernel K-means clustering 3: Clustering time 0.008, Prec@1 85.157
Epoch 1 - Kernel K-means clustering 4: Clustering time 0.008, Prec@1 85.660
Epoch 1 - Kernel K-means clustering 5: Clustering time 0.008, Prec@1 85.660
Converged at iteration 6
Epoch 1 - Kernel K-means clustering 0: Clustering time 0.007, Prec@1 85.535
Epoch 1 - Kernel K-means clustering 1: Clustering time 0.008, Prec@1 86.415
Epoch 1 - Kernel K-means clustering 2: Clustering time 0.008, Prec@1 86.667
Epoch 1 - Kernel K-means clustering 3: Clustering time 0.008, Prec@1 86.667
Epoch 1 - Kernel K-means clustering 4: Clustering time 0.008, Prec@1 86.541
Epoch 1 - Kernel K-means clustering 5: Clustering time 0.008, Prec@1 86.541
Converged at iteration 6
The penalty weight is 0.049958
Train - epoch [2/200]	BT 1.058 (1.058)	DT 0.000 (0.000)	S@1 85.714 (85.714)	Loss 0.7977 (0.7977)
Train - epoch [2/200]	BT 0.997 (0.997)	DT 0.000 (0.000)	S@1 80.952 (80.952)	Loss 0.7939 (0.7939)
The penalty weight is 0.049958
Train - epoch [2/200]	BT 1.044 (1.044)	DT 0.000 (0.000)	S@1 90.476 (90.476)	Loss 0.7632 (0.7632)
Train - epoch [2/200]	BT 0.998 (0.998)	DT 0.000 (0.000)	S@1 76.190 (76.190)	Loss 0.8550 (0.8550)
Train - epoch [2/200]	BT 1.048 (1.048)	DT 0.000 (0.000)	S@1 73.810 (73.810)	Loss 0.9427 (0.9427)
Train - epoch [2/200]	BT 1.028 (1.028)	DT 0.000 (0.000)	S@1 71.429 (71.429)	Loss 0.8209 (0.8209)
Train - epoch [2/200]	BT 1.244 (1.244)	DT 0.000 (0.000)	S@1 66.667 (66.667)	Loss 0.9293 (0.9293)
Train - epoch [2/200]	BT 1.046 (1.046)	DT 0.000 (0.000)	S@1 71.429 (71.429)	Loss 0.7791 (0.7791)
Train - epoch [2/200]	BT 1.225 (1.225)	DT 0.000 (0.000)	S@1 61.905 (61.905)	Loss 0.8410 (0.8410)
Train - epoch [2/200]	BT 1.041 (1.041)	DT 0.000 (0.000)	S@1 64.286 (64.286)	Loss 0.7830 (0.7830)
Train - epoch [2/200]	BT 1.028 (1.028)	DT 0.000 (0.000)	S@1 69.048 (69.048)	Loss 0.9021 (0.9021)
Train - epoch [2/200]	BT 1.028 (1.028)	DT 0.000 (0.000)	S@1 80.952 (80.952)	Loss 0.7436 (0.7436)
Train - epoch [2/200]	BT 1.057 (1.057)	DT 0.000 (0.000)	S@1 69.048 (69.048)	Loss 0.8398 (0.8398)
Train - epoch [2/200]	BT 1.031 (1.031)	DT 0.000 (0.000)	S@1 61.905 (61.905)	Loss 0.8757 (0.8757)
Test on T training set - [2][0/13]	T 0.707 (0.707)	D 0.592 (0.592)	T@1 100.000 (100.000)	T@5 100.000 (100.000)	L 0.0619 (0.0619)
Test on T training set - [2][10/13]	T 0.504 (0.518)	D 0.390 (0.401)	T@1 80.952 (83.117)	T@5 98.413 (95.815)	L 0.5983 (0.6100)
 * Test on T training set - Prec@1 83.522, Prec@5 96.352
Test on T training set - [2][0/13]	T 0.707 (0.707)	D 0.587 (0.587)	T@1 100.000 (100.000)	T@5 100.000 (100.000)	L 0.0356 (0.0356)
Test on T training set - [2][10/13]	T 0.506 (0.522)	D 0.379 (0.399)	T@1 73.016 (84.127)	T@5 100.000 (96.681)	L 0.7064 (0.5495)
 * Test on T training set - Prec@1 84.528, Prec@5 97.107
Test on T test set - [2][0/13]	Time 0.727 (0.727)	Loss 0.0619 (0.0619)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Test on T test set - [2][10/13]	Time 0.505 (0.518)	Loss 0.5983 (0.6100)	Prec@1 80.952 (83.117)	Prec@5 98.413 (95.815)
 * Test on T test set - Prec@1 83.522, Prec@5 96.352
Epoch 2 - Kernel K-means clustering 0: Clustering time 0.007, Prec@1 86.038
Epoch 2 - Kernel K-means clustering 1: Clustering time 0.008, Prec@1 86.667
Epoch 2 - Kernel K-means clustering 2: Clustering time 0.008, Prec@1 86.918
Epoch 2 - Kernel K-means clustering 3: Clustering time 0.008, Prec@1 87.421
Epoch 2 - Kernel K-means clustering 4: Clustering time 0.008, Prec@1 87.547
Epoch 2 - Kernel K-means clustering 5: Clustering time 0.009, Prec@1 87.421
Epoch 2 - Kernel K-means clustering 6: Clustering time 0.008, Prec@1 87.421
Epoch 2 - Kernel K-means clustering 7: Clustering time 0.008, Prec@1 87.547
Epoch 2 - Kernel K-means clustering 8: Clustering time 0.008, Prec@1 87.296
Epoch 2 - Kernel K-means clustering 9: Clustering time 0.008, Prec@1 87.170
Epoch 2 - Kernel K-means clustering 10: Clustering time 0.008, Prec@1 87.170
Converged at iteration 11
Epoch 2 - Kernel K-means clustering 0: Clustering time 0.006, Prec@1 88.050
Epoch 2 - Kernel K-means clustering 1: Clustering time 0.008, Prec@1 88.428
Epoch 2 - Kernel K-means clustering 2: Clustering time 0.008, Prec@1 88.931
Epoch 2 - Kernel K-means clustering 3: Clustering time 0.007, Prec@1 89.308
Epoch 2 - Kernel K-means clustering 4: Clustering time 0.008, Prec@1 89.434
Epoch 2 - Kernel K-means clustering 5: Clustering time 0.008, Prec@1 89.560
Epoch 2 - Kernel K-means clustering 6: Clustering time 0.008, Prec@1 90.063
Epoch 2 - Kernel K-means clustering 7: Clustering time 0.008, Prec@1 90.440
Epoch 2 - Kernel K-means clustering 8: Clustering time 0.007, Prec@1 90.566
Epoch 2 - Kernel K-means clustering 9: Clustering time 0.008, Prec@1 90.692
Epoch 2 - Kernel K-means clustering 10: Clustering time 0.021, Prec@1 90.692
Converged at iteration 11
Test on T test set - [2][0/13]	Time 0.742 (0.742)	Loss 0.0356 (0.0356)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Test on T test set - [2][10/13]	Time 0.516 (0.517)	Loss 0.7064 (0.5495)	Prec@1 73.016 (84.127)	Prec@5 100.000 (96.681)
 * Test on T test set - Prec@1 84.528, Prec@5 97.107
Epoch 2 - Kernel K-means clustering 0: Clustering time 0.007, Prec@1 87.170
Epoch 2 - Kernel K-means clustering 1: Clustering time 0.008, Prec@1 88.050
Epoch 2 - Kernel K-means clustering 2: Clustering time 0.020, Prec@1 88.931
Epoch 2 - Kernel K-means clustering 3: Clustering time 0.008, Prec@1 90.189
Epoch 2 - Kernel K-means clustering 4: Clustering time 0.008, Prec@1 90.566
Epoch 2 - Kernel K-means clustering 5: Clustering time 0.008, Prec@1 90.566
Epoch 2 - Kernel K-means clustering 6: Clustering time 0.008, Prec@1 90.566
Converged at iteration 7
Epoch 2 - Kernel K-means clustering 0: Clustering time 0.007, Prec@1 88.428
Epoch 2 - Kernel K-means clustering 1: Clustering time 0.008, Prec@1 89.434
Epoch 2 - Kernel K-means clustering 2: Clustering time 0.008, Prec@1 89.937
Epoch 2 - Kernel K-means clustering 3: Clustering time 0.007, Prec@1 90.314
Epoch 2 - Kernel K-means clustering 4: Clustering time 0.007, Prec@1 90.314
Converged at iteration 5
The penalty weight is 0.074860
Train - epoch [3/200]	BT 1.044 (1.044)	DT 0.000 (0.000)	S@1 80.952 (80.952)	Loss 1.1027 (1.1027)
The penalty weight is 0.074860
Train - epoch [3/200]	BT 1.064 (1.064)	DT 0.000 (0.000)	S@1 59.524 (59.524)	Loss 1.0633 (1.0633)
Train - epoch [3/200]	BT 1.062 (1.062)	DT 0.000 (0.000)	S@1 61.905 (61.905)	Loss 1.1408 (1.1408)
Train - epoch [3/200]	BT 1.033 (1.033)	DT 0.000 (0.000)	S@1 54.762 (54.762)	Loss 1.1709 (1.1709)
Train - epoch [3/200]	BT 1.088 (1.088)	DT 0.000 (0.000)	S@1 76.190 (76.190)	Loss 1.0573 (1.0573)
Train - epoch [3/200]	BT 1.037 (1.037)	DT 0.000 (0.000)	S@1 71.429 (71.429)	Loss 1.0305 (1.0305)
Train - epoch [3/200]	BT 1.033 (1.033)	DT 0.000 (0.000)	S@1 66.667 (66.667)	Loss 1.1628 (1.1628)
Train - epoch [3/200]	BT 1.033 (1.033)	DT 0.000 (0.000)	S@1 61.905 (61.905)	Loss 1.2041 (1.2041)
Train - epoch [3/200]	BT 1.036 (1.036)	DT 0.000 (0.000)	S@1 71.429 (71.429)	Loss 1.1296 (1.1296)
Train - epoch [3/200]	BT 1.036 (1.036)	DT 0.000 (0.000)	S@1 71.429 (71.429)	Loss 1.1334 (1.1334)
Train - epoch [3/200]	BT 1.021 (1.021)	DT 0.000 (0.000)	S@1 64.286 (64.286)	Loss 1.0709 (1.0709)
Train - epoch [3/200]	BT 1.030 (1.030)	DT 0.000 (0.000)	S@1 71.429 (71.429)	Loss 1.0429 (1.0429)
Test on T training set - [3][0/13]	T 0.710 (0.710)	D 0.591 (0.591)	T@1 100.000 (100.000)	T@5 100.000 (100.000)	L 0.0350 (0.0350)
Test on T training set - [3][10/13]	T 0.531 (0.522)	D 0.411 (0.402)	T@1 85.714 (84.271)	T@5 100.000 (97.258)	L 0.5405 (0.5556)
 * Test on T training set - Prec@1 84.906, Prec@5 97.610
Test on T training set - [3][0/13]	T 0.711 (0.711)	D 0.590 (0.590)	T@1 100.000 (100.000)	T@5 100.000 (100.000)	L 0.0347 (0.0347)
Test on T training set - [3][10/13]	T 0.513 (0.521)	D 0.400 (0.401)	T@1 71.429 (84.416)	T@5 100.000 (96.970)	L 0.8085 (0.5089)
 * Test on T training set - Prec@1 85.660, Prec@5 97.358
Test on T test set - [3][0/13]	Time 0.738 (0.738)	Loss 0.0350 (0.0350)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Test on T test set - [3][10/13]	Time 0.526 (0.535)	Loss 0.5405 (0.5556)	Prec@1 85.714 (84.271)	Prec@5 100.000 (97.258)
 * Test on T test set - Prec@1 84.906, Prec@5 97.610
Epoch 3 - Kernel K-means clustering 0: Clustering time 0.007, Prec@1 86.541
Epoch 3 - Kernel K-means clustering 1: Clustering time 0.009, Prec@1 87.547
Epoch 3 - Kernel K-means clustering 2: Clustering time 0.008, Prec@1 88.176
Epoch 3 - Kernel K-means clustering 3: Clustering time 0.009, Prec@1 88.302
Epoch 3 - Kernel K-means clustering 4: Clustering time 0.009, Prec@1 88.679
Epoch 3 - Kernel K-means clustering 5: Clustering time 0.009, Prec@1 88.679
Epoch 3 - Kernel K-means clustering 6: Clustering time 0.009, Prec@1 88.805
Epoch 3 - Kernel K-means clustering 7: Clustering time 0.009, Prec@1 88.553
Epoch 3 - Kernel K-means clustering 8: Clustering time 0.008, Prec@1 88.428
Epoch 3 - Kernel K-means clustering 9: Clustering time 0.009, Prec@1 88.428
Epoch 3 - Kernel K-means clustering 10: Clustering time 0.008, Prec@1 88.302
Epoch 3 - Kernel K-means clustering 11: Clustering time 0.008, Prec@1 88.553
Epoch 3 - Kernel K-means clustering 12: Clustering time 0.008, Prec@1 88.679
Epoch 3 - Kernel K-means clustering 13: Clustering time 0.008, Prec@1 88.679
Converged at iteration 14
Epoch 3 - Kernel K-means clustering 0: Clustering time 0.007, Prec@1 88.302
Epoch 3 - Kernel K-means clustering 1: Clustering time 0.009, Prec@1 88.931
Epoch 3 - Kernel K-means clustering 2: Clustering time 0.009, Prec@1 89.308
Epoch 3 - Kernel K-means clustering 3: Clustering time 0.009, Prec@1 89.811
Epoch 3 - Kernel K-means clustering 4: Clustering time 0.008, Prec@1 90.063
Epoch 3 - Kernel K-means clustering 5: Clustering time 0.009, Prec@1 90.692
Epoch 3 - Kernel K-means clustering 6: Clustering time 0.009, Prec@1 90.943
Epoch 3 - Kernel K-means clustering 7: Clustering time 0.009, Prec@1 91.069
Epoch 3 - Kernel K-means clustering 8: Clustering time 0.038, Prec@1 91.069
Converged at iteration 9
Test on T test set - [3][0/13]	Time 0.749 (0.749)	Loss 0.0347 (0.0347)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Test on T test set - [3][10/13]	Time 0.523 (0.541)	Loss 0.8085 (0.5089)	Prec@1 71.429 (84.416)	Prec@5 100.000 (96.970)
 * Test on T test set - Prec@1 85.660, Prec@5 97.358
Epoch 3 - Kernel K-means clustering 0: Clustering time 0.007, Prec@1 88.176
Epoch 3 - Kernel K-means clustering 1: Clustering time 0.009, Prec@1 88.805
Epoch 3 - Kernel K-means clustering 2: Clustering time 0.009, Prec@1 89.937
Epoch 3 - Kernel K-means clustering 3: Clustering time 0.009, Prec@1 90.440
Epoch 3 - Kernel K-means clustering 4: Clustering time 0.009, Prec@1 90.314
Epoch 3 - Kernel K-means clustering 5: Clustering time 0.008, Prec@1 90.063
Epoch 3 - Kernel K-means clustering 6: Clustering time 0.008, Prec@1 90.063
Converged at iteration 7
Epoch 3 - Kernel K-means clustering 0: Clustering time 0.007, Prec@1 89.560
Epoch 3 - Kernel K-means clustering 1: Clustering time 0.008, Prec@1 90.818
Epoch 3 - Kernel K-means clustering 2: Clustering time 0.008, Prec@1 91.321
Epoch 3 - Kernel K-means clustering 3: Clustering time 0.008, Prec@1 91.824
Epoch 3 - Kernel K-means clustering 4: Clustering time 0.008, Prec@1 92.201
Epoch 3 - Kernel K-means clustering 5: Clustering time 0.009, Prec@1 92.327
Epoch 3 - Kernel K-means clustering 6: Clustering time 0.008, Prec@1 92.579
Epoch 3 - Kernel K-means clustering 7: Clustering time 0.008, Prec@1 92.579
Converged at iteration 8
The penalty weight is 0.099668
Train - epoch [4/200]	BT 1.209 (1.209)	DT 0.000 (0.000)	S@1 66.667 (66.667)	Loss 1.5183 (1.5183)
Train - epoch [4/200]	BT 1.042 (1.042)	DT 0.000 (0.000)	S@1 66.667 (66.667)	Loss 1.4581 (1.4581)
The penalty weight is 0.099668
Train - epoch [4/200]	BT 1.039 (1.039)	DT 0.000 (0.000)	S@1 69.048 (69.048)	Loss 1.3692 (1.3692)
Train - epoch [4/200]	BT 1.028 (1.028)	DT 0.000 (0.000)	S@1 69.048 (69.048)	Loss 1.3896 (1.3896)
Train - epoch [4/200]	BT 1.047 (1.047)	DT 0.000 (0.000)	S@1 59.524 (59.524)	Loss 1.5350 (1.5350)
Train - epoch [4/200]	BT 1.054 (1.054)	DT 0.000 (0.000)	S@1 61.905 (61.905)	Loss 1.4601 (1.4601)
Train - epoch [4/200]	BT 1.047 (1.047)	DT 0.000 (0.000)	S@1 66.667 (66.667)	Loss 1.3838 (1.3838)
Train - epoch [4/200]	BT 1.025 (1.025)	DT 0.000 (0.000)	S@1 73.810 (73.810)	Loss 1.3094 (1.3094)
Train - epoch [4/200]	BT 1.070 (1.070)	DT 0.000 (0.000)	S@1 69.048 (69.048)	Loss 1.4225 (1.4225)
Train - epoch [4/200]	BT 1.008 (1.008)	DT 0.000 (0.000)	S@1 57.143 (57.143)	Loss 1.5539 (1.5539)
Train - epoch [4/200]	BT 1.040 (1.040)	DT 0.000 (0.000)	S@1 78.571 (78.571)	Loss 1.4007 (1.4007)
Train - epoch [4/200]	BT 1.005 (1.005)	DT 0.000 (0.000)	S@1 64.286 (64.286)	Loss 1.3808 (1.3808)
Train - epoch [4/200]	BT 1.045 (1.045)	DT 0.000 (0.000)	S@1 64.286 (64.286)	Loss 1.3856 (1.3856)
Train - epoch [4/200]	BT 1.015 (1.015)	DT 0.000 (0.000)	S@1 66.667 (66.667)	Loss 1.3960 (1.3960)
Train - epoch [4/200]	BT 1.035 (1.035)	DT 0.000 (0.000)	S@1 59.524 (59.524)	Loss 1.5200 (1.5200)
Train - epoch [4/200]	BT 1.032 (1.032)	DT 0.000 (0.000)	S@1 69.048 (69.048)	Loss 1.4410 (1.4410)
Test on T training set - [4][0/13]	T 0.746 (0.746)	D 0.620 (0.620)	T@1 100.000 (100.000)	T@5 100.000 (100.000)	L 0.0260 (0.0260)
Test on T training set - [4][10/13]	T 0.527 (0.538)	D 0.413 (0.419)	T@1 77.778 (84.704)	T@5 100.000 (97.691)	L 0.6481 (0.5519)
 * Test on T training set - Prec@1 85.660, Prec@5 97.987
Test on T training set - [4][0/13]	T 0.733 (0.733)	D 0.609 (0.609)	T@1 100.000 (100.000)	T@5 100.000 (100.000)	L 0.0197 (0.0197)
Test on T training set - [4][10/13]	T 0.540 (0.537)	D 0.423 (0.419)	T@1 73.016 (86.869)	T@5 98.413 (97.403)	L 0.8531 (0.4936)
 * Test on T training set - Prec@1 88.302, Prec@5 97.736
Test on T test set - [4][0/13]	Time 0.726 (0.726)	Loss 0.0260 (0.0260)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Test on T test set - [4][10/13]	Time 0.510 (0.518)	Loss 0.6481 (0.5519)	Prec@1 77.778 (84.704)	Prec@5 100.000 (97.691)
 * Test on T test set - Prec@1 85.660, Prec@5 97.987
Epoch 4 - Kernel K-means clustering 0: Clustering time 0.006, Prec@1 87.044
Epoch 4 - Kernel K-means clustering 1: Clustering time 0.008, Prec@1 88.050
Epoch 4 - Kernel K-means clustering 2: Clustering time 0.008, Prec@1 88.553
Epoch 4 - Kernel K-means clustering 3: Clustering time 0.008, Prec@1 89.182
Epoch 4 - Kernel K-means clustering 4: Clustering time 0.008, Prec@1 89.308
Epoch 4 - Kernel K-means clustering 5: Clustering time 0.008, Prec@1 89.434
Epoch 4 - Kernel K-means clustering 6: Clustering time 0.016, Prec@1 89.434
Converged at iteration 7
Epoch 4 - Kernel K-means clustering 0: Clustering time 0.007, Prec@1 88.428
Epoch 4 - Kernel K-means clustering 1: Clustering time 0.009, Prec@1 89.560
Epoch 4 - Kernel K-means clustering 2: Clustering time 0.008, Prec@1 89.686
Epoch 4 - Kernel K-means clustering 3: Clustering time 0.008, Prec@1 90.314
Epoch 4 - Kernel K-means clustering 4: Clustering time 0.008, Prec@1 90.566
Epoch 4 - Kernel K-means clustering 5: Clustering time 0.008, Prec@1 90.566
Epoch 4 - Kernel K-means clustering 6: Clustering time 0.008, Prec@1 90.566
Epoch 4 - Kernel K-means clustering 7: Clustering time 0.008, Prec@1 90.440
Epoch 4 - Kernel K-means clustering 8: Clustering time 0.007, Prec@1 90.440
Converged at iteration 9
Test on T test set - [4][0/13]	Time 0.734 (0.734)	Loss 0.0197 (0.0197)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Test on T test set - [4][10/13]	Time 0.518 (0.516)	Loss 0.8531 (0.4936)	Prec@1 73.016 (86.869)	Prec@5 98.413 (97.403)
 * Test on T test set - Prec@1 88.302, Prec@5 97.736
Epoch 4 - Kernel K-means clustering 0: Clustering time 0.006, Prec@1 90.063
Epoch 4 - Kernel K-means clustering 1: Clustering time 0.018, Prec@1 90.943
Epoch 4 - Kernel K-means clustering 2: Clustering time 0.008, Prec@1 91.321
Epoch 4 - Kernel K-means clustering 3: Clustering time 0.008, Prec@1 91.572
Epoch 4 - Kernel K-means clustering 4: Clustering time 0.008, Prec@1 91.447
Epoch 4 - Kernel K-means clustering 5: Clustering time 0.008, Prec@1 91.195
Epoch 4 - Kernel K-means clustering 6: Clustering time 0.009, Prec@1 91.069
Epoch 4 - Kernel K-means clustering 7: Clustering time 0.008, Prec@1 91.069
Converged at iteration 8
Epoch 4 - Kernel K-means clustering 0: Clustering time 0.006, Prec@1 90.943
Epoch 4 - Kernel K-means clustering 1: Clustering time 0.008, Prec@1 91.572
Epoch 4 - Kernel K-means clustering 2: Clustering time 0.008Prec@1 91.447
Epoch 4 - Kernel K-means clustering 3: Clustering time 0.009, Prec@1 91.447
Epoch 4 - Kernel K-means clustering 4: Clustering time 0.008, Prec@1 91.572
Epoch 4 - Kernel K-means clustering 5: Clustering time 0.008, Prec@1 91.572
Converged at iteration 6
The penalty weight is 0.124353
Train - epoch [5/200]	BT 1.029 (1.029)	DT 0.000 (0.000)	S@1 59.524 (59.524)	Loss 1.8289 (1.8289)
Train - epoch [5/200]	BT 1.023 (1.023)	DT 0.000 (0.000)	S@1 61.905 (61.905)	Loss 1.7551 (1.7551)
The penalty weight is 0.124353
Train - epoch [5/200]	BT 1.033 (1.033)	DT 0.000 (0.000)	S@1 47.619 (47.619)	Loss 1.8394 (1.8394)
Train - epoch [5/200]	BT 1.035 (1.035)	DT 0.000 (0.000)	S@1 69.048 (69.048)	Loss 1.7966 (1.7966)
Train - epoch [5/200]	BT 1.035 (1.035)	DT 0.000 (0.000)	S@1 69.048 (69.048)	Loss 1.7540 (1.7540)
Train - epoch [5/200]	BT 0.996 (0.996)	DT 0.000 (0.000)	S@1 64.286 (64.286)	Loss 1.7057 (1.7057)
Train - epoch [5/200]	BT 1.048 (1.048)	DT 0.000 (0.000)	S@1 71.429 (71.429)	Loss 1.7366 (1.7366)
Train - epoch [5/200]	BT 0.995 (0.995)	DT 0.000 (0.000)	S@1 59.524 (59.524)	Loss 1.7594 (1.7594)
Train - epoch [5/200]	BT 1.036 (1.036)	DT 0.000 (0.000)	S@1 64.286 (64.286)	Loss 1.7410 (1.7410)
Train - epoch [5/200]	BT 1.037 (1.037)	DT 0.000 (0.000)	S@1 57.143 (57.143)	Loss 1.5826 (1.5826)
Train - epoch [5/200]	BT 1.081 (1.081)	DT 0.000 (0.000)	S@1 73.810 (73.810)	Loss 1.6283 (1.6283)
Train - epoch [5/200]	BT 1.017 (1.017)	DT 0.000 (0.000)	S@1 76.190 (76.190)	Loss 1.8603 (1.8603)
Train - epoch [5/200]	BT 1.066 (1.066)	DT 0.000 (0.000)	S@1 64.286 (64.286)	Loss 1.6016 (1.6016)
Train - epoch [5/200]	BT 1.017 (1.017)	DT 0.000 (0.000)	S@1 57.143 (57.143)	Loss 1.6764 (1.6764)
